{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29783,"status":"ok","timestamp":1689153539814,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"},"user_tz":-360},"id":"BHXE4HPuSFEr","outputId":"d7a47dcc-fa24-4e2a-d355-3a486cc112bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01If0rTMSz9j"},"outputs":[],"source":["root_path = '/gdrive/My Drive/Colab Data/CRISPR Off Target/'\n","data_dir = root_path + '2018_DeepCRISPR/'\n","data_path = data_dir + 'all_off_target.csv'\n","resource_dir = data_dir + \"Resources/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5--9M4Oq9czH"},"outputs":[],"source":["import os\n","import random\n","import torch\n","import numpy as np\n","import copy\n","\n","seed = 12345\n","\n","os.environ['PYTHONHASHSEED']=str(seed)\n","random.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1689153547122,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"},"user_tz":-360},"id":"QNep67GoNdka","outputId":"eed8dce5-8ae1-49b4-a5ce-21435190b655"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1830,"status":"ok","timestamp":1689153753970,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"},"user_tz":-360},"id":"5IxgtNXVXeDt","outputId":"334cb09c-e633-41a7-bef4-0285744f33be"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/.shortcut-targets-by-id/1-CPBoDSc88CelqHVwU-GHHjHASJq0kkO/CRISPR Off Target/2018_DeepCRISPR/Resources\n"]}],"source":["cd $resource_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1689153757975,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"},"user_tz":-360},"id":"7a8BKh1kUPTC","outputId":"29f0e8fe-043e-4ad5-8203-faa702e72b35"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 1LSTM_1_1_OHE4.weights        BiLSTM_1_1_OHE4.weights\n"," 1LSTM_1_1_TME256.weights      BiLSTM_1_1_TME256.weights\n"," 1LSTM_2_1_OHE4.weights        BiLSTM_2_1_OHE4.weights\n"," 1LSTM_2_1_OHE4.weights.zip    BiLSTM_2_1_TME256.weights\n"," 1LSTM_2_1_TME256.weights      BiLSTM_255_1_OHE4.weights\n"," 1LSTM_255_1_OHE4.weights      BiLSTM_255_1_TME256.weights\n"," 1LSTM_255_1_TME256.weights   'Copy of Results and Analysis CRISPR.gdoc'\n"," 2LSTM_1_1_OHE4.weights        ega_stat\n"," 2LSTM_1_1_TME256.weights      Figures\n"," 2LSTM_2_1_OHE4.weights        ga_stat\n"," 2LSTM_2_1_TME256.weights      rnn_torchviz\n"," 2LSTM_255_1_OHE4.weights      rnn_torchviz.png\n"," 2LSTM_255_1_TME256.weights    test_bpe.csv\n"," basic_test.csv\t\t       test.csv\n"," basic_train.csv\t       train_bpe.csv\n"," best_lstm_model_ohe_23_8      train.csv\n"," best_model_attribution.pkl    w2v_d_model.bin\n"," best_model_attributions.pkl   w2v_model.bin\n"]}],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"GkopnuflmA-K"},"source":["#Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NeTlGGOoa8j"},"outputs":[],"source":["import torch.nn as nn\n","\n","class LSTM_Model_Generic(nn.Module):\n","    def __init__(self, config):\n","        super(LSTM_Model_Generic,self).__init__()\n","        # emb_size=256, hidden_size=128, hidden_layers=3, output=2\n","\n","        self.vocab_size = config[\"vocab_size\"]\n","        self.emb_size = config[\"emb_size\"]\n","        self.hidden_size = config[\"hidden_size\"]\n","        self.lstm_layers = config[\"lstm_layers\"]\n","        self.bi_lstm = config[\"bi_lstm\"]\n","        self.reshape = config[\"reshape\"]\n","\n","        self.number_hidden_layers = config[\"number_hidder_layers\"]\n","        self.dropout_prob = config[\"dropout_prob\"]\n","        self.hidden_layers = []\n","\n","        self.hidden_shape = self.hidden_size*2 if self.bi_lstm else self.hidden_size\n","\n","        self.embedding = None\n","        if self.vocab_size > 0:\n","            self.embedding = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0)\n","\n","        self.lstm= nn.LSTM(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n","                            batch_first=True, bidirectional=self.bi_lstm)\n","#         self.lstm= nn.GRU(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n","#                             batch_first=True, bidirectional=self.bi_lstm)\n","\n","        start_size = self.hidden_shape\n","\n","        self.relu = nn.ReLU\n","        # self.dropout = nn.Dropout(self.dropout_prob)\n","\n","        for i in range(self.number_hidden_layers):\n","            self.hidden_layers.append(nn.Sequential(\n","                nn.Linear(start_size, start_size // 2),\n","                nn.ReLU(),\n","                nn.Dropout(self.dropout_prob)))\n","\n","            start_size = start_size // 2\n","\n","        self.hidden_layers = nn.ModuleList(self.hidden_layers)\n","        self.output = nn.Linear(start_size,2)\n","\n","\n","    def forward(self,x):\n","        dir = 2 if self.bi_lstm else 1\n","        h = torch.zeros((self.lstm_layers*dir, x.size(0), self.hidden_size)).to(device)\n","        c = torch.zeros((self.lstm_layers*dir, x.size(0), self.hidden_size)).to(device)\n","\n","        if self.embedding is not None:\n","            x = x.type(torch.LongTensor).to(device)\n","            x = self.embedding(x)\n","        elif self.reshape:\n","            x = x.view(x.shape[0],x.shape[1],1)\n","\n","        x, (hidden, cell) = self.lstm(x, (h,c))\n","\n","        x = x[:, -1, :]\n","\n","        # print(x.shape)\n","        for i, layer in enumerate(self.hidden_layers):\n","            x = layer(x)\n","            # print(x.shape)\n","        x = self.output(x)\n","        # print(x.shape)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMSWgBwLF64o"},"outputs":[],"source":["import torch.nn as nn\n","\n","class RNN_Model_Generic(nn.Module):\n","    def __init__(self, config, model_type):\n","        super(RNN_Model_Generic,self).__init__()\n","        # emb_size=256, hidden_size=128, hidden_layers=3, output=2\n","\n","        self.model_type = model_type\n","        self.vocab_size = config[\"vocab_size\"]\n","        self.emb_size = config[\"emb_size\"]\n","        self.hidden_size = config[\"hidden_size\"]\n","        self.lstm_layers = config[\"lstm_layers\"]\n","        self.bi_lstm = config[\"bi_lstm\"]\n","        self.reshape = config[\"reshape\"]\n","\n","        self.number_hidden_layers = config[\"number_hidder_layers\"]\n","        self.dropout_prob = config[\"dropout_prob\"]\n","        self.hidden_layers = []\n","\n","        self.hidden_shape = self.hidden_size*2 if self.bi_lstm else self.hidden_size\n","\n","        self.embedding = None\n","        if self.vocab_size > 0:\n","            self.embedding = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0)\n","\n","\n","        if model_type == \"LSTM\":\n","            self.lstm = nn.LSTM(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n","                            batch_first=True, bidirectional=self.bi_lstm)\n","        elif model_type == \"GRU\":\n","            self.lstm= nn.GRU(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n","                           batch_first=True, bidirectional=self.bi_lstm)\n","        else:\n","            self.lstm= nn.RNN(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n","                           batch_first=True, bidirectional=self.bi_lstm)\n","\n","        start_size = self.hidden_shape\n","\n","        self.relu = nn.ReLU\n","        # self.dropout = nn.Dropout(self.dropout_prob)\n","\n","        for i in range(self.number_hidden_layers):\n","            self.hidden_layers.append(nn.Sequential(\n","                nn.Linear(start_size, start_size // 2),\n","                nn.ReLU(),\n","                nn.Dropout(self.dropout_prob)))\n","\n","            start_size = start_size // 2\n","\n","        self.hidden_layers = nn.ModuleList(self.hidden_layers)\n","        self.output = nn.Linear(start_size,2)\n","\n","\n","    def forward(self,x):\n","        dir = 2 if self.bi_lstm else 1\n","        h = torch.zeros((self.lstm_layers*dir, x.size(0), self.hidden_size)).to(device)\n","        c = torch.zeros((self.lstm_layers*dir, x.size(0), self.hidden_size)).to(device)\n","\n","        if self.embedding is not None:\n","            x = x.type(torch.LongTensor).to(device)\n","            x = self.embedding(x)\n","        elif self.reshape:\n","            x = x.view(x.shape[0],x.shape[1],1)\n","\n","        if self.model_type == \"LSTM\":\n","            x, (hidden, cell) = self.lstm(x, (h,c))\n","        else:\n","            x, hidden = self.lstm(x, h)\n","\n","        x = x[:, -1, :]\n","\n","        # print(x.shape)\n","        for i, layer in enumerate(self.hidden_layers):\n","            x = layer(x)\n","            # print(x.shape)\n","        x = self.output(x)\n","        # print(x.shape)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"VUcwPlCfFaGc"},"source":["#Training and Evaluation Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NETrJHBSCmOy"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class TrainerDataset(Dataset):\n","    def __init__(self, inputs, targets):\n","        self.inputs= inputs\n","        self.targets = torch.from_numpy(targets)\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","    def __getitem__(self, idx):\n","        return torch.Tensor(self.inputs[idx]), self.targets[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNp_GjpLF6x4"},"outputs":[],"source":["def trainer(config, train_x, train_y, num_epochs=100, batch_size=32, debug=False, lr=0.0001,model_type=\"LSTM\"):\n","    train_pos_idx = np.where(train_y==1)\n","    train_neg_idx = np.where(train_y==0)\n","\n","    train_xp = train_x[train_pos_idx]\n","    train_xn = train_x[train_neg_idx]\n","\n","    train_yp = train_y[train_pos_idx]\n","    train_yn = train_y[train_neg_idx]\n","\n","    train_dataset_pos = TrainerDataset(train_xp, train_yp)\n","    train_dataloader_pos = DataLoader(train_dataset_pos, batch_size=batch_size//2, shuffle=True)\n","    train_dataset_neg = TrainerDataset(train_xn, train_yn)\n","    train_dataloader_neg = DataLoader(train_dataset_neg, batch_size=batch_size//2, shuffle=True)\n","\n","    seed = 12345\n","    os.environ['PYTHONHASHSEED']=str(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    model = RNN_Model_Generic(config, model_type).to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n","    n_total_steps = len(train_dataloader_neg)\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        for i, (train_features_neg, train_labels_neg) in enumerate(train_dataloader_neg):\n","            train_features_pos, train_labels_pos = next(iter(train_dataloader_pos))\n","            train_features = torch.cat((train_features_pos, train_features_neg),0)\n","            train_labels = torch.cat((train_labels_pos, train_labels_neg),0)\n","\n","#             print(train_features.shape, train_labels.shape)\n","\n","            outputs = model(train_features.to(device))\n","            loss = criterion(outputs, train_labels.to(device))\n","\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # if (i+1) % 2000 == 0 and epoch % 10 == 0:\n","            if (i+1) % 200 == 0:\n","                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n","                if debug:\n","                    return model\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKq4TBTIWORv"},"outputs":[],"source":["def tester(model, test_x, test_y):\n","    test_dataset = TrainerDataset(test_x, test_y)\n","    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n","    model.eval()\n","    results = []\n","    true_labels = []\n","    with torch.no_grad():\n","        for test_features, test_labels in test_dataloader:\n","            outputs = model(test_features.to(device)).detach().to(\"cpu\")\n","            results.extend(outputs)\n","            true_labels.extend(test_labels)\n","    return true_labels, results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIEfwpqvUZwt"},"outputs":[],"source":["class Stats:\n","    def __init__(self):\n","        self.acc = 0\n","        self.pre = 0\n","        self.re = 0\n","        self.f1 = 0\n","        self.roc = 0\n","        self.prc = 0\n","        self.tn = 0\n","        self.fp = 0\n","        self.fn = 0\n","        self.tp = 0\n","    def print(self):\n","        print('Accuracy: %.4f' %self.acc)\n","        print('Precision: %.4f' %self.pre)\n","        print('Recall: %.4f' %self.re)\n","        print('F1 Score: %.4f' %self.f1)\n","        print('ROC: %.4f' %self.roc)\n","        print('PR AUC: %.4f' %self.prc)\n","        print(\"Confusion Matrix\")\n","        print(self.tn, \"\\t\", self.fp)\n","        print(self.fn, \"\\t\", self.tp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NhnOw-xXDPK"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import auc\n","from sklearn.metrics import accuracy_score\n","\n","def eval_matrices(model, test_x, test_y, debug = True):\n","    true_y, results = tester(model, test_x, test_y)\n","    predictions = [torch.nn.functional.softmax(r) for r in results]\n","    pred_y = np.array([y[1].item() for y in predictions])\n","    pred_y_list = []\n","    test_y = np.array([y.item() for y in true_y])\n","\n","    for x in pred_y:\n","        if(x>0.5):\n","            pred_y_list.append(1)\n","        else:\n","            pred_y_list.append(0)\n","\n","    pred_y_list = np.array(pred_y_list)\n","\n","    tn, fp, fn, tp = confusion_matrix(test_y, pred_y_list).ravel()\n","    precision, recall, _ = precision_recall_curve(test_y, pred_y)\n","    auc_score = auc(recall, precision)\n","    acc = accuracy_score(test_y, pred_y_list)\n","\n","    pr = -1\n","    re = -1\n","    f1 = -1\n","    try:\n","        pr = tp / (tp+fp)\n","        re = tp / (tp+fn)\n","        f1 = 2*pr*re / (pr+re)\n","    except:\n","        f1 = -1\n","\n","    stats = Stats()\n","    stats.acc = acc\n","    stats.pre = pr\n","    stats.re = re\n","    stats.f1 = f1\n","    stats.roc = roc_auc_score(test_y, pred_y)\n","    stats.prc = auc_score\n","    stats.tn = tn\n","    stats.fp = fp\n","    stats.fn = fn\n","    stats.tp = tp\n","\n","    if debug:\n","        print('Accuracy: %.4f' %acc)\n","        print('Precision: %.4f' %pr)\n","        print('Recall: %.4f' %re)\n","        print('F1 Score: %.4f' %f1)\n","        print('ROC:',roc_auc_score(test_y, pred_y))\n","        print('PR AUC: %.4f' % auc_score)\n","\n","        print(classification_report(test_y, pred_y_list, digits=4))\n","        print(\"Confusion Matrix\")\n","        print(confusion_matrix(test_y, pred_y_list))\n","\n","    return stats"]},{"cell_type":"markdown","metadata":{"id":"TUglVmcsG2BB"},"source":["# Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9380,"status":"ok","timestamp":1689153776346,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"},"user_tz":-360},"id":"ACEjI3LYGnWf","outputId":"cd72695e-aa53-4c86-e9bd-0905c424fd4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pickle5\n","  Downloading pickle5-0.0.11.tar.gz (132 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/132.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pickle5\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=256405 sha256=c690f0a23fd3578e98a1a74813eafb419f5a2b95b03312a1f74568075809983f\n","  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n","Successfully built pickle5\n","Installing collected packages: pickle5\n","Successfully installed pickle5-0.0.11\n"]}],"source":["!pip install pickle5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_UC6RTKF643","outputId":"d589b966-72c1-4749-928e-d57f713a016b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689153926169,"user_tz":-360,"elapsed":3575,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(153233, 23, 4)\n","(153233,)\n","(122586, 23, 4) (122586,)\n","(30647, 23, 4) (30647,)\n"]}],"source":["import pickle5 as pkl\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import RandomOverSampler\n","\n","in_file = data_dir + \"Encoded Data/all_encoded_data.pkl\"\n","enc_dict = {}\n","with open(in_file, \"rb\") as f:\n","    enc_dict = pkl.load(f)\n","\n","data_x = enc_dict['enc_superposed']\n","data_y = enc_dict['labels']\n","\n","data_x = np.array(data_x)\n","data_y = np.array(data_y)\n","\n","print(data_x.shape)\n","print(data_y.shape)\n","\n","\n","train_x, test_x, train_y, test_y = train_test_split(data_x, data_y,\n","                                                    stratify=data_y,\n","                                                    test_size=0.20,\n","                                                    random_state=5)\n","print(train_x.shape, train_y.shape)\n","print(test_x.shape, test_y.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"BkAq_CuszLoW"},"source":["# Best RNN Model"]},{"cell_type":"code","source":["best_config = {\n","    'vocab_size': 0,\n","    'emb_size': 4,\n","    'hidden_size': 256,\n","    'lstm_layers': 2,\n","    'bi_lstm': True,\n","    'number_hidder_layers': 0,\n","    'dropout_prob': 0.4,\n","    'reshape': False,\n","    'batch_size': 256,\n","    'epochs': 60,\n","    'learning_rate': 0.00100\n","}"],"metadata":{"id":"q7ajKJK9a63D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = best_config\n","debug=False\n","model_type=\"RNN\"\n","\n","model = trainer(config, train_x, train_y, num_epochs=config[\"epochs\"], lr=config[\"learning_rate\"], batch_size = config[\"batch_size\"], model_type = model_type)\n","stats = eval_matrices(model, test_x, test_y)\n","stats.print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypnzKYgNa8Mj","executionInfo":{"status":"ok","timestamp":1689154888012,"user_tz":-360,"elapsed":783534,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"}},"outputId":"04c5e531-5672-41ef-e363-617053c5d80d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/60], Step [200/954], Loss: 0.1600\n","Epoch [1/60], Step [400/954], Loss: 0.2942\n","Epoch [1/60], Step [600/954], Loss: 0.0585\n","Epoch [1/60], Step [800/954], Loss: 0.1347\n","Epoch [2/60], Step [200/954], Loss: 0.0197\n","Epoch [2/60], Step [400/954], Loss: 0.0395\n","Epoch [2/60], Step [600/954], Loss: 0.0436\n","Epoch [2/60], Step [800/954], Loss: 0.0199\n","Epoch [3/60], Step [200/954], Loss: 0.0214\n","Epoch [3/60], Step [400/954], Loss: 0.0526\n","Epoch [3/60], Step [600/954], Loss: 0.1540\n","Epoch [3/60], Step [800/954], Loss: 0.2036\n","Epoch [4/60], Step [200/954], Loss: 0.1292\n","Epoch [4/60], Step [400/954], Loss: 0.0871\n","Epoch [4/60], Step [600/954], Loss: 0.0296\n","Epoch [4/60], Step [800/954], Loss: 0.0187\n","Epoch [5/60], Step [200/954], Loss: 0.0092\n","Epoch [5/60], Step [400/954], Loss: 0.0050\n","Epoch [5/60], Step [600/954], Loss: 0.0066\n","Epoch [5/60], Step [800/954], Loss: 0.0083\n","Epoch [6/60], Step [200/954], Loss: 0.0276\n","Epoch [6/60], Step [400/954], Loss: 0.0071\n","Epoch [6/60], Step [600/954], Loss: 0.0088\n","Epoch [6/60], Step [800/954], Loss: 0.0192\n","Epoch [7/60], Step [200/954], Loss: 0.0119\n","Epoch [7/60], Step [400/954], Loss: 0.0037\n","Epoch [7/60], Step [600/954], Loss: 0.0322\n","Epoch [7/60], Step [800/954], Loss: 0.0027\n","Epoch [8/60], Step [200/954], Loss: 0.0124\n","Epoch [8/60], Step [400/954], Loss: 0.0026\n","Epoch [8/60], Step [600/954], Loss: 0.0324\n","Epoch [8/60], Step [800/954], Loss: 0.0067\n","Epoch [9/60], Step [200/954], Loss: 0.0214\n","Epoch [9/60], Step [400/954], Loss: 0.0240\n","Epoch [9/60], Step [600/954], Loss: 0.0172\n","Epoch [9/60], Step [800/954], Loss: 0.0035\n","Epoch [10/60], Step [200/954], Loss: 0.0057\n","Epoch [10/60], Step [400/954], Loss: 0.0069\n","Epoch [10/60], Step [600/954], Loss: 0.0088\n","Epoch [10/60], Step [800/954], Loss: 0.0116\n","Epoch [11/60], Step [200/954], Loss: 0.0190\n","Epoch [11/60], Step [400/954], Loss: 0.0011\n","Epoch [11/60], Step [600/954], Loss: 0.0008\n","Epoch [11/60], Step [800/954], Loss: 0.0035\n","Epoch [12/60], Step [200/954], Loss: 0.0012\n","Epoch [12/60], Step [400/954], Loss: 0.0139\n","Epoch [12/60], Step [600/954], Loss: 0.1017\n","Epoch [12/60], Step [800/954], Loss: 0.0437\n","Epoch [13/60], Step [200/954], Loss: 0.0069\n","Epoch [13/60], Step [400/954], Loss: 0.0017\n","Epoch [13/60], Step [600/954], Loss: 0.0016\n","Epoch [13/60], Step [800/954], Loss: 0.0318\n","Epoch [14/60], Step [200/954], Loss: 0.0425\n","Epoch [14/60], Step [400/954], Loss: 0.0052\n","Epoch [14/60], Step [600/954], Loss: 0.0294\n","Epoch [14/60], Step [800/954], Loss: 0.0083\n","Epoch [15/60], Step [200/954], Loss: 0.0109\n","Epoch [15/60], Step [400/954], Loss: 0.0013\n","Epoch [15/60], Step [600/954], Loss: 0.0599\n","Epoch [15/60], Step [800/954], Loss: 0.0226\n","Epoch [16/60], Step [200/954], Loss: 0.0010\n","Epoch [16/60], Step [400/954], Loss: 0.0030\n","Epoch [16/60], Step [600/954], Loss: 0.0058\n","Epoch [16/60], Step [800/954], Loss: 0.0030\n","Epoch [17/60], Step [200/954], Loss: 0.1006\n","Epoch [17/60], Step [400/954], Loss: 0.1853\n","Epoch [17/60], Step [600/954], Loss: 0.2616\n","Epoch [17/60], Step [800/954], Loss: 0.1103\n","Epoch [18/60], Step [200/954], Loss: 0.0806\n","Epoch [18/60], Step [400/954], Loss: 0.2404\n","Epoch [18/60], Step [600/954], Loss: 0.0430\n","Epoch [18/60], Step [800/954], Loss: 0.0856\n","Epoch [19/60], Step [200/954], Loss: 0.0854\n","Epoch [19/60], Step [400/954], Loss: 0.1236\n","Epoch [19/60], Step [600/954], Loss: 0.0707\n","Epoch [19/60], Step [800/954], Loss: 0.0847\n","Epoch [20/60], Step [200/954], Loss: 0.2252\n","Epoch [20/60], Step [400/954], Loss: 0.0931\n","Epoch [20/60], Step [600/954], Loss: 0.1151\n","Epoch [20/60], Step [800/954], Loss: 0.0901\n","Epoch [21/60], Step [200/954], Loss: 0.0762\n","Epoch [21/60], Step [400/954], Loss: 0.0465\n","Epoch [21/60], Step [600/954], Loss: 0.0682\n","Epoch [21/60], Step [800/954], Loss: 0.0732\n","Epoch [22/60], Step [200/954], Loss: 0.0486\n","Epoch [22/60], Step [400/954], Loss: 0.0391\n","Epoch [22/60], Step [600/954], Loss: 0.0363\n","Epoch [22/60], Step [800/954], Loss: 0.0439\n","Epoch [23/60], Step [200/954], Loss: 0.0752\n","Epoch [23/60], Step [400/954], Loss: 0.0398\n","Epoch [23/60], Step [600/954], Loss: 0.0482\n","Epoch [23/60], Step [800/954], Loss: 0.0380\n","Epoch [24/60], Step [200/954], Loss: 0.0574\n","Epoch [24/60], Step [400/954], Loss: 0.0473\n","Epoch [24/60], Step [600/954], Loss: 0.0365\n","Epoch [24/60], Step [800/954], Loss: 0.0708\n","Epoch [25/60], Step [200/954], Loss: 0.0674\n","Epoch [25/60], Step [400/954], Loss: 0.0409\n","Epoch [25/60], Step [600/954], Loss: 0.0191\n","Epoch [25/60], Step [800/954], Loss: 0.0757\n","Epoch [26/60], Step [200/954], Loss: 0.0058\n","Epoch [26/60], Step [400/954], Loss: 0.0523\n","Epoch [26/60], Step [600/954], Loss: 0.0765\n","Epoch [26/60], Step [800/954], Loss: 0.0230\n","Epoch [27/60], Step [200/954], Loss: 0.0566\n","Epoch [27/60], Step [400/954], Loss: 0.0365\n","Epoch [27/60], Step [600/954], Loss: 0.0119\n","Epoch [27/60], Step [800/954], Loss: 0.0378\n","Epoch [28/60], Step [200/954], Loss: 0.0243\n","Epoch [28/60], Step [400/954], Loss: 0.0404\n","Epoch [28/60], Step [600/954], Loss: 0.0858\n","Epoch [28/60], Step [800/954], Loss: 0.0272\n","Epoch [29/60], Step [200/954], Loss: 0.0069\n","Epoch [29/60], Step [400/954], Loss: 0.0046\n","Epoch [29/60], Step [600/954], Loss: 0.0198\n","Epoch [29/60], Step [800/954], Loss: 0.0634\n","Epoch [30/60], Step [200/954], Loss: 0.0567\n","Epoch [30/60], Step [400/954], Loss: 0.0478\n","Epoch [30/60], Step [600/954], Loss: 0.0211\n","Epoch [30/60], Step [800/954], Loss: 0.0340\n","Epoch [31/60], Step [200/954], Loss: 0.0268\n","Epoch [31/60], Step [400/954], Loss: 0.0274\n","Epoch [31/60], Step [600/954], Loss: 0.0158\n","Epoch [31/60], Step [800/954], Loss: 0.0347\n","Epoch [32/60], Step [200/954], Loss: 0.0303\n","Epoch [32/60], Step [400/954], Loss: 0.0050\n","Epoch [32/60], Step [600/954], Loss: 0.0119\n","Epoch [32/60], Step [800/954], Loss: 0.0171\n","Epoch [33/60], Step [200/954], Loss: 0.0094\n","Epoch [33/60], Step [400/954], Loss: 0.0547\n","Epoch [33/60], Step [600/954], Loss: 0.0631\n","Epoch [33/60], Step [800/954], Loss: 0.0407\n","Epoch [34/60], Step [200/954], Loss: 0.0501\n","Epoch [34/60], Step [400/954], Loss: 0.0411\n","Epoch [34/60], Step [600/954], Loss: 0.0130\n","Epoch [34/60], Step [800/954], Loss: 0.0140\n","Epoch [35/60], Step [200/954], Loss: 0.0120\n","Epoch [35/60], Step [400/954], Loss: 0.0223\n","Epoch [35/60], Step [600/954], Loss: 0.0575\n","Epoch [35/60], Step [800/954], Loss: 0.0385\n","Epoch [36/60], Step [200/954], Loss: 0.0529\n","Epoch [36/60], Step [400/954], Loss: 0.0460\n","Epoch [36/60], Step [600/954], Loss: 0.0828\n","Epoch [36/60], Step [800/954], Loss: 0.0267\n","Epoch [37/60], Step [200/954], Loss: 0.0676\n","Epoch [37/60], Step [400/954], Loss: 0.0530\n","Epoch [37/60], Step [600/954], Loss: 0.1073\n","Epoch [37/60], Step [800/954], Loss: 0.0102\n","Epoch [38/60], Step [200/954], Loss: 0.0344\n","Epoch [38/60], Step [400/954], Loss: 0.0603\n","Epoch [38/60], Step [600/954], Loss: 0.0694\n","Epoch [38/60], Step [800/954], Loss: 0.0115\n","Epoch [39/60], Step [200/954], Loss: 0.0277\n","Epoch [39/60], Step [400/954], Loss: 0.0613\n","Epoch [39/60], Step [600/954], Loss: 0.0760\n","Epoch [39/60], Step [800/954], Loss: 0.0185\n","Epoch [40/60], Step [200/954], Loss: 0.0149\n","Epoch [40/60], Step [400/954], Loss: 0.0359\n","Epoch [40/60], Step [600/954], Loss: 0.0585\n","Epoch [40/60], Step [800/954], Loss: 0.0063\n","Epoch [41/60], Step [200/954], Loss: 0.0088\n","Epoch [41/60], Step [400/954], Loss: 0.0064\n","Epoch [41/60], Step [600/954], Loss: 0.0457\n","Epoch [41/60], Step [800/954], Loss: 0.0200\n","Epoch [42/60], Step [200/954], Loss: 0.0213\n","Epoch [42/60], Step [400/954], Loss: 0.0170\n","Epoch [42/60], Step [600/954], Loss: 0.0078\n","Epoch [42/60], Step [800/954], Loss: 0.0199\n","Epoch [43/60], Step [200/954], Loss: 0.0077\n","Epoch [43/60], Step [400/954], Loss: 0.0101\n","Epoch [43/60], Step [600/954], Loss: 0.0253\n","Epoch [43/60], Step [800/954], Loss: 0.0153\n","Epoch [44/60], Step [200/954], Loss: 0.0236\n","Epoch [44/60], Step [400/954], Loss: 0.0090\n","Epoch [44/60], Step [600/954], Loss: 0.0139\n","Epoch [44/60], Step [800/954], Loss: 0.0226\n","Epoch [45/60], Step [200/954], Loss: 0.0258\n","Epoch [45/60], Step [400/954], Loss: 0.0111\n","Epoch [45/60], Step [600/954], Loss: 0.0064\n","Epoch [45/60], Step [800/954], Loss: 0.0505\n","Epoch [46/60], Step [200/954], Loss: 0.0542\n","Epoch [46/60], Step [400/954], Loss: 0.0022\n","Epoch [46/60], Step [600/954], Loss: 0.0153\n","Epoch [46/60], Step [800/954], Loss: 0.0219\n","Epoch [47/60], Step [200/954], Loss: 0.0267\n","Epoch [47/60], Step [400/954], Loss: 0.0067\n","Epoch [47/60], Step [600/954], Loss: 0.0057\n","Epoch [47/60], Step [800/954], Loss: 0.0486\n","Epoch [48/60], Step [200/954], Loss: 0.0197\n","Epoch [48/60], Step [400/954], Loss: 0.0260\n","Epoch [48/60], Step [600/954], Loss: 0.0208\n","Epoch [48/60], Step [800/954], Loss: 0.0310\n","Epoch [49/60], Step [200/954], Loss: 0.0147\n","Epoch [49/60], Step [400/954], Loss: 0.0036\n","Epoch [49/60], Step [600/954], Loss: 0.0046\n","Epoch [49/60], Step [800/954], Loss: 0.0368\n","Epoch [50/60], Step [200/954], Loss: 0.0554\n","Epoch [50/60], Step [400/954], Loss: 0.0393\n","Epoch [50/60], Step [600/954], Loss: 0.0083\n","Epoch [50/60], Step [800/954], Loss: 0.0152\n","Epoch [51/60], Step [200/954], Loss: 0.0100\n","Epoch [51/60], Step [400/954], Loss: 0.0095\n","Epoch [51/60], Step [600/954], Loss: 0.0516\n","Epoch [51/60], Step [800/954], Loss: 0.0112\n","Epoch [52/60], Step [200/954], Loss: 0.0258\n","Epoch [52/60], Step [400/954], Loss: 0.0485\n","Epoch [52/60], Step [600/954], Loss: 0.0043\n","Epoch [52/60], Step [800/954], Loss: 0.0030\n","Epoch [53/60], Step [200/954], Loss: 0.0226\n","Epoch [53/60], Step [400/954], Loss: 0.0410\n","Epoch [53/60], Step [600/954], Loss: 0.0230\n","Epoch [53/60], Step [800/954], Loss: 0.0145\n","Epoch [54/60], Step [200/954], Loss: 0.0596\n","Epoch [54/60], Step [400/954], Loss: 0.0040\n","Epoch [54/60], Step [600/954], Loss: 0.0304\n","Epoch [54/60], Step [800/954], Loss: 0.0086\n","Epoch [55/60], Step [200/954], Loss: 0.0088\n","Epoch [55/60], Step [400/954], Loss: 0.0626\n","Epoch [55/60], Step [600/954], Loss: 0.0070\n","Epoch [55/60], Step [800/954], Loss: 0.0605\n","Epoch [56/60], Step [200/954], Loss: 0.0346\n","Epoch [56/60], Step [400/954], Loss: 0.0090\n","Epoch [56/60], Step [600/954], Loss: 0.0239\n","Epoch [56/60], Step [800/954], Loss: 0.0180\n","Epoch [57/60], Step [200/954], Loss: 0.0145\n","Epoch [57/60], Step [400/954], Loss: 0.0044\n","Epoch [57/60], Step [600/954], Loss: 0.0055\n","Epoch [57/60], Step [800/954], Loss: 0.0630\n","Epoch [58/60], Step [200/954], Loss: 0.0261\n","Epoch [58/60], Step [400/954], Loss: 0.0310\n","Epoch [58/60], Step [600/954], Loss: 0.0053\n","Epoch [58/60], Step [800/954], Loss: 0.0166\n","Epoch [59/60], Step [200/954], Loss: 0.0085\n","Epoch [59/60], Step [400/954], Loss: 0.0335\n","Epoch [59/60], Step [600/954], Loss: 0.0346\n","Epoch [59/60], Step [800/954], Loss: 0.0435\n","Epoch [60/60], Step [200/954], Loss: 0.0022\n","Epoch [60/60], Step [400/954], Loss: 0.0083\n","Epoch [60/60], Step [600/954], Loss: 0.0178\n","Epoch [60/60], Step [800/954], Loss: 0.0186\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-19-f2e417b33a9e>:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  predictions = [torch.nn.functional.softmax(r) for r in results]\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9892\n","Precision: 0.2679\n","Recall: 0.8855\n","F1 Score: 0.4113\n","ROC: 0.9873685334886266\n","PR AUC: 0.5711\n","              precision    recall  f1-score   support\n","\n","           0     0.9995    0.9896    0.9945     30516\n","           1     0.2679    0.8855    0.4113       131\n","\n","    accuracy                         0.9892     30647\n","   macro avg     0.6337    0.9376    0.7029     30647\n","weighted avg     0.9964    0.9892    0.9920     30647\n","\n","Confusion Matrix\n","[[30199   317]\n"," [   15   116]]\n","Accuracy: 0.9892\n","Precision: 0.2679\n","Recall: 0.8855\n","F1 Score: 0.4113\n","ROC: 0.9874\n","PR AUC: 0.5711\n","Confusion Matrix\n","30199 \t 317\n","15 \t 116\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"best_rnn_model.pth\")"],"metadata":{"id":"eZvLUlfObJEk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Best LSTM Model"],"metadata":{"id":"YsfT13nabkNT"}},{"cell_type":"code","source":["best_config = {\n","    'vocab_size': 0,\n","    'emb_size': 4,\n","    'hidden_size': 512,\n","    'lstm_layers': 1,\n","    'bi_lstm': True,\n","    'number_hidder_layers': 2,\n","    'dropout_prob': 0.4,\n","    'reshape': False,\n","    'batch_size': 64,\n","    'epochs': 50,\n","    'learning_rate': 0.00010\n","}"],"metadata":{"id":"vFZlX8frbmeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = best_config\n","debug=False\n","model_type=\"LSTM\"\n","\n","model = trainer(config, train_x, train_y, num_epochs=config[\"epochs\"], lr=config[\"learning_rate\"], batch_size = config[\"batch_size\"], model_type = model_type)\n","stats = eval_matrices(model, test_x, test_y)\n","stats.print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNXnNcwBbpWq","executionInfo":{"status":"ok","timestamp":1689156685919,"user_tz":-360,"elapsed":1797909,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"}},"outputId":"0dbb4de8-9132-4a5b-9795-c1f77b1d16ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Step [200/3815], Loss: 0.4032\n","Epoch [1/50], Step [400/3815], Loss: 0.3179\n","Epoch [1/50], Step [600/3815], Loss: 0.2288\n","Epoch [1/50], Step [800/3815], Loss: 0.1329\n","Epoch [1/50], Step [1000/3815], Loss: 0.1317\n","Epoch [1/50], Step [1200/3815], Loss: 0.1568\n","Epoch [1/50], Step [1400/3815], Loss: 0.1962\n","Epoch [1/50], Step [1600/3815], Loss: 0.1526\n","Epoch [1/50], Step [1800/3815], Loss: 0.1736\n","Epoch [1/50], Step [2000/3815], Loss: 0.1103\n","Epoch [1/50], Step [2200/3815], Loss: 0.1300\n","Epoch [1/50], Step [2400/3815], Loss: 0.1748\n","Epoch [1/50], Step [2600/3815], Loss: 0.1881\n","Epoch [1/50], Step [2800/3815], Loss: 0.1425\n","Epoch [1/50], Step [3000/3815], Loss: 0.1693\n","Epoch [1/50], Step [3200/3815], Loss: 0.1734\n","Epoch [1/50], Step [3400/3815], Loss: 0.1572\n","Epoch [1/50], Step [3600/3815], Loss: 0.2549\n","Epoch [1/50], Step [3800/3815], Loss: 0.1542\n","Epoch [2/50], Step [200/3815], Loss: 0.1485\n","Epoch [2/50], Step [400/3815], Loss: 0.0825\n","Epoch [2/50], Step [600/3815], Loss: 0.0781\n","Epoch [2/50], Step [800/3815], Loss: 0.0951\n","Epoch [2/50], Step [1000/3815], Loss: 0.1060\n","Epoch [2/50], Step [1200/3815], Loss: 0.1149\n","Epoch [2/50], Step [1400/3815], Loss: 0.1292\n","Epoch [2/50], Step [1600/3815], Loss: 0.0506\n","Epoch [2/50], Step [1800/3815], Loss: 0.0614\n","Epoch [2/50], Step [2000/3815], Loss: 0.1079\n","Epoch [2/50], Step [2200/3815], Loss: 0.1128\n","Epoch [2/50], Step [2400/3815], Loss: 0.1546\n","Epoch [2/50], Step [2600/3815], Loss: 0.0875\n","Epoch [2/50], Step [2800/3815], Loss: 0.1390\n","Epoch [2/50], Step [3000/3815], Loss: 0.1442\n","Epoch [2/50], Step [3200/3815], Loss: 0.0894\n","Epoch [2/50], Step [3400/3815], Loss: 0.0523\n","Epoch [2/50], Step [3600/3815], Loss: 0.1020\n","Epoch [2/50], Step [3800/3815], Loss: 0.0258\n","Epoch [3/50], Step [200/3815], Loss: 0.0597\n","Epoch [3/50], Step [400/3815], Loss: 0.1223\n","Epoch [3/50], Step [600/3815], Loss: 0.1474\n","Epoch [3/50], Step [800/3815], Loss: 0.1412\n","Epoch [3/50], Step [1000/3815], Loss: 0.0499\n","Epoch [3/50], Step [1200/3815], Loss: 0.0367\n","Epoch [3/50], Step [1400/3815], Loss: 0.0150\n","Epoch [3/50], Step [1600/3815], Loss: 0.1031\n","Epoch [3/50], Step [1800/3815], Loss: 0.0488\n","Epoch [3/50], Step [2000/3815], Loss: 0.1265\n","Epoch [3/50], Step [2200/3815], Loss: 0.0584\n","Epoch [3/50], Step [2400/3815], Loss: 0.1999\n","Epoch [3/50], Step [2600/3815], Loss: 0.1288\n","Epoch [3/50], Step [2800/3815], Loss: 0.1223\n","Epoch [3/50], Step [3000/3815], Loss: 0.0386\n","Epoch [3/50], Step [3200/3815], Loss: 0.1839\n","Epoch [3/50], Step [3400/3815], Loss: 0.1567\n","Epoch [3/50], Step [3600/3815], Loss: 0.0185\n","Epoch [3/50], Step [3800/3815], Loss: 0.0204\n","Epoch [4/50], Step [200/3815], Loss: 0.0869\n","Epoch [4/50], Step [400/3815], Loss: 0.0111\n","Epoch [4/50], Step [600/3815], Loss: 0.0938\n","Epoch [4/50], Step [800/3815], Loss: 0.0628\n","Epoch [4/50], Step [1000/3815], Loss: 0.0525\n","Epoch [4/50], Step [1200/3815], Loss: 0.0089\n","Epoch [4/50], Step [1400/3815], Loss: 0.0477\n","Epoch [4/50], Step [1600/3815], Loss: 0.0795\n","Epoch [4/50], Step [1800/3815], Loss: 0.1337\n","Epoch [4/50], Step [2000/3815], Loss: 0.1013\n","Epoch [4/50], Step [2200/3815], Loss: 0.0417\n","Epoch [4/50], Step [2400/3815], Loss: 0.0497\n","Epoch [4/50], Step [2600/3815], Loss: 0.0506\n","Epoch [4/50], Step [2800/3815], Loss: 0.0421\n","Epoch [4/50], Step [3000/3815], Loss: 0.0501\n","Epoch [4/50], Step [3200/3815], Loss: 0.0343\n","Epoch [4/50], Step [3400/3815], Loss: 0.1493\n","Epoch [4/50], Step [3600/3815], Loss: 0.0426\n","Epoch [4/50], Step [3800/3815], Loss: 0.0646\n","Epoch [5/50], Step [200/3815], Loss: 0.0065\n","Epoch [5/50], Step [400/3815], Loss: 0.0958\n","Epoch [5/50], Step [600/3815], Loss: 0.0032\n","Epoch [5/50], Step [800/3815], Loss: 0.1497\n","Epoch [5/50], Step [1000/3815], Loss: 0.0507\n","Epoch [5/50], Step [1200/3815], Loss: 0.1130\n","Epoch [5/50], Step [1400/3815], Loss: 0.0143\n","Epoch [5/50], Step [1600/3815], Loss: 0.0040\n","Epoch [5/50], Step [1800/3815], Loss: 0.0033\n","Epoch [5/50], Step [2000/3815], Loss: 0.0066\n","Epoch [5/50], Step [2200/3815], Loss: 0.0171\n","Epoch [5/50], Step [2400/3815], Loss: 0.0308\n","Epoch [5/50], Step [2600/3815], Loss: 0.0550\n","Epoch [5/50], Step [2800/3815], Loss: 0.0073\n","Epoch [5/50], Step [3000/3815], Loss: 0.0898\n","Epoch [5/50], Step [3200/3815], Loss: 0.0505\n","Epoch [5/50], Step [3400/3815], Loss: 0.0713\n","Epoch [5/50], Step [3600/3815], Loss: 0.0075\n","Epoch [5/50], Step [3800/3815], Loss: 0.0105\n","Epoch [6/50], Step [200/3815], Loss: 0.0385\n","Epoch [6/50], Step [400/3815], Loss: 0.0948\n","Epoch [6/50], Step [600/3815], Loss: 0.0181\n","Epoch [6/50], Step [800/3815], Loss: 0.0608\n","Epoch [6/50], Step [1000/3815], Loss: 0.0114\n","Epoch [6/50], Step [1200/3815], Loss: 0.0357\n","Epoch [6/50], Step [1400/3815], Loss: 0.0081\n","Epoch [6/50], Step [1600/3815], Loss: 0.0725\n","Epoch [6/50], Step [1800/3815], Loss: 0.0055\n","Epoch [6/50], Step [2000/3815], Loss: 0.0022\n","Epoch [6/50], Step [2200/3815], Loss: 0.0035\n","Epoch [6/50], Step [2400/3815], Loss: 0.0070\n","Epoch [6/50], Step [2600/3815], Loss: 0.0049\n","Epoch [6/50], Step [2800/3815], Loss: 0.0050\n","Epoch [6/50], Step [3000/3815], Loss: 0.0153\n","Epoch [6/50], Step [3200/3815], Loss: 0.0771\n","Epoch [6/50], Step [3400/3815], Loss: 0.0161\n","Epoch [6/50], Step [3600/3815], Loss: 0.0730\n","Epoch [6/50], Step [3800/3815], Loss: 0.0749\n","Epoch [7/50], Step [200/3815], Loss: 0.0064\n","Epoch [7/50], Step [400/3815], Loss: 0.1000\n","Epoch [7/50], Step [600/3815], Loss: 0.0037\n","Epoch [7/50], Step [800/3815], Loss: 0.0644\n","Epoch [7/50], Step [1000/3815], Loss: 0.0111\n","Epoch [7/50], Step [1200/3815], Loss: 0.0163\n","Epoch [7/50], Step [1400/3815], Loss: 0.0322\n","Epoch [7/50], Step [1600/3815], Loss: 0.0032\n","Epoch [7/50], Step [1800/3815], Loss: 0.0732\n","Epoch [7/50], Step [2000/3815], Loss: 0.0040\n","Epoch [7/50], Step [2200/3815], Loss: 0.1274\n","Epoch [7/50], Step [2400/3815], Loss: 0.1446\n","Epoch [7/50], Step [2600/3815], Loss: 0.0105\n","Epoch [7/50], Step [2800/3815], Loss: 0.0467\n","Epoch [7/50], Step [3000/3815], Loss: 0.0629\n","Epoch [7/50], Step [3200/3815], Loss: 0.0120\n","Epoch [7/50], Step [3400/3815], Loss: 0.0086\n","Epoch [7/50], Step [3600/3815], Loss: 0.0047\n","Epoch [7/50], Step [3800/3815], Loss: 0.0106\n","Epoch [8/50], Step [200/3815], Loss: 0.0107\n","Epoch [8/50], Step [400/3815], Loss: 0.0897\n","Epoch [8/50], Step [600/3815], Loss: 0.0097\n","Epoch [8/50], Step [800/3815], Loss: 0.0431\n","Epoch [8/50], Step [1000/3815], Loss: 0.0195\n","Epoch [8/50], Step [1200/3815], Loss: 0.0198\n","Epoch [8/50], Step [1400/3815], Loss: 0.1652\n","Epoch [8/50], Step [1600/3815], Loss: 0.1019\n","Epoch [8/50], Step [1800/3815], Loss: 0.0051\n","Epoch [8/50], Step [2000/3815], Loss: 0.0605\n","Epoch [8/50], Step [2200/3815], Loss: 0.0371\n","Epoch [8/50], Step [2400/3815], Loss: 0.0487\n","Epoch [8/50], Step [2600/3815], Loss: 0.1166\n","Epoch [8/50], Step [2800/3815], Loss: 0.0040\n","Epoch [8/50], Step [3000/3815], Loss: 0.0032\n","Epoch [8/50], Step [3200/3815], Loss: 0.0022\n","Epoch [8/50], Step [3400/3815], Loss: 0.0024\n","Epoch [8/50], Step [3600/3815], Loss: 0.0027\n","Epoch [8/50], Step [3800/3815], Loss: 0.0099\n","Epoch [9/50], Step [200/3815], Loss: 0.1239\n","Epoch [9/50], Step [400/3815], Loss: 0.0021\n","Epoch [9/50], Step [600/3815], Loss: 0.0139\n","Epoch [9/50], Step [800/3815], Loss: 0.0262\n","Epoch [9/50], Step [1000/3815], Loss: 0.0156\n","Epoch [9/50], Step [1200/3815], Loss: 0.0199\n","Epoch [9/50], Step [1400/3815], Loss: 0.0081\n","Epoch [9/50], Step [1600/3815], Loss: 0.0633\n","Epoch [9/50], Step [1800/3815], Loss: 0.0135\n","Epoch [9/50], Step [2000/3815], Loss: 0.0083\n","Epoch [9/50], Step [2200/3815], Loss: 0.0020\n","Epoch [9/50], Step [2400/3815], Loss: 0.0015\n","Epoch [9/50], Step [2600/3815], Loss: 0.0470\n","Epoch [9/50], Step [2800/3815], Loss: 0.0851\n","Epoch [9/50], Step [3000/3815], Loss: 0.0170\n","Epoch [9/50], Step [3200/3815], Loss: 0.0215\n","Epoch [9/50], Step [3400/3815], Loss: 0.0050\n","Epoch [9/50], Step [3600/3815], Loss: 0.0009\n","Epoch [9/50], Step [3800/3815], Loss: 0.0049\n","Epoch [10/50], Step [200/3815], Loss: 0.0073\n","Epoch [10/50], Step [400/3815], Loss: 0.0015\n","Epoch [10/50], Step [600/3815], Loss: 0.0082\n","Epoch [10/50], Step [800/3815], Loss: 0.0059\n","Epoch [10/50], Step [1000/3815], Loss: 0.0410\n","Epoch [10/50], Step [1200/3815], Loss: 0.0015\n","Epoch [10/50], Step [1400/3815], Loss: 0.0064\n","Epoch [10/50], Step [1600/3815], Loss: 0.0998\n","Epoch [10/50], Step [1800/3815], Loss: 0.0035\n","Epoch [10/50], Step [2000/3815], Loss: 0.0516\n","Epoch [10/50], Step [2200/3815], Loss: 0.0025\n","Epoch [10/50], Step [2400/3815], Loss: 0.0012\n","Epoch [10/50], Step [2600/3815], Loss: 0.0089\n","Epoch [10/50], Step [2800/3815], Loss: 0.0010\n","Epoch [10/50], Step [3000/3815], Loss: 0.0149\n","Epoch [10/50], Step [3200/3815], Loss: 0.0037\n","Epoch [10/50], Step [3400/3815], Loss: 0.0024\n","Epoch [10/50], Step [3600/3815], Loss: 0.0036\n","Epoch [10/50], Step [3800/3815], Loss: 0.0648\n","Epoch [11/50], Step [200/3815], Loss: 0.0067\n","Epoch [11/50], Step [400/3815], Loss: 0.0007\n","Epoch [11/50], Step [600/3815], Loss: 0.0295\n","Epoch [11/50], Step [800/3815], Loss: 0.0314\n","Epoch [11/50], Step [1000/3815], Loss: 0.0047\n","Epoch [11/50], Step [1200/3815], Loss: 0.0089\n","Epoch [11/50], Step [1400/3815], Loss: 0.0043\n","Epoch [11/50], Step [1600/3815], Loss: 0.0016\n","Epoch [11/50], Step [1800/3815], Loss: 0.0098\n","Epoch [11/50], Step [2000/3815], Loss: 0.0193\n","Epoch [11/50], Step [2200/3815], Loss: 0.0013\n","Epoch [11/50], Step [2400/3815], Loss: 0.0155\n","Epoch [11/50], Step [2600/3815], Loss: 0.0020\n","Epoch [11/50], Step [2800/3815], Loss: 0.0024\n","Epoch [11/50], Step [3000/3815], Loss: 0.0776\n","Epoch [11/50], Step [3200/3815], Loss: 0.0076\n","Epoch [11/50], Step [3400/3815], Loss: 0.0062\n","Epoch [11/50], Step [3600/3815], Loss: 0.0487\n","Epoch [11/50], Step [3800/3815], Loss: 0.0018\n","Epoch [12/50], Step [200/3815], Loss: 0.0485\n","Epoch [12/50], Step [400/3815], Loss: 0.0030\n","Epoch [12/50], Step [600/3815], Loss: 0.0036\n","Epoch [12/50], Step [800/3815], Loss: 0.0010\n","Epoch [12/50], Step [1000/3815], Loss: 0.0087\n","Epoch [12/50], Step [1200/3815], Loss: 0.0017\n","Epoch [12/50], Step [1400/3815], Loss: 0.0022\n","Epoch [12/50], Step [1600/3815], Loss: 0.0011\n","Epoch [12/50], Step [1800/3815], Loss: 0.0138\n","Epoch [12/50], Step [2000/3815], Loss: 0.0013\n","Epoch [12/50], Step [2200/3815], Loss: 0.0025\n","Epoch [12/50], Step [2400/3815], Loss: 0.0019\n","Epoch [12/50], Step [2600/3815], Loss: 0.0044\n","Epoch [12/50], Step [2800/3815], Loss: 0.0044\n","Epoch [12/50], Step [3000/3815], Loss: 0.0015\n","Epoch [12/50], Step [3200/3815], Loss: 0.0023\n","Epoch [12/50], Step [3400/3815], Loss: 0.0024\n","Epoch [12/50], Step [3600/3815], Loss: 0.0108\n","Epoch [12/50], Step [3800/3815], Loss: 0.0008\n","Epoch [13/50], Step [200/3815], Loss: 0.0023\n","Epoch [13/50], Step [400/3815], Loss: 0.0020\n","Epoch [13/50], Step [600/3815], Loss: 0.0009\n","Epoch [13/50], Step [800/3815], Loss: 0.0133\n","Epoch [13/50], Step [1000/3815], Loss: 0.0030\n","Epoch [13/50], Step [1200/3815], Loss: 0.0013\n","Epoch [13/50], Step [1400/3815], Loss: 0.0024\n","Epoch [13/50], Step [1600/3815], Loss: 0.0041\n","Epoch [13/50], Step [1800/3815], Loss: 0.0012\n","Epoch [13/50], Step [2000/3815], Loss: 0.0022\n","Epoch [13/50], Step [2200/3815], Loss: 0.0019\n","Epoch [13/50], Step [2400/3815], Loss: 0.0674\n","Epoch [13/50], Step [2600/3815], Loss: 0.0044\n","Epoch [13/50], Step [2800/3815], Loss: 0.0005\n","Epoch [13/50], Step [3000/3815], Loss: 0.0081\n","Epoch [13/50], Step [3200/3815], Loss: 0.0020\n","Epoch [13/50], Step [3400/3815], Loss: 0.0097\n","Epoch [13/50], Step [3600/3815], Loss: 0.0019\n","Epoch [13/50], Step [3800/3815], Loss: 0.0155\n","Epoch [14/50], Step [200/3815], Loss: 0.0018\n","Epoch [14/50], Step [400/3815], Loss: 0.0911\n","Epoch [14/50], Step [600/3815], Loss: 0.0011\n","Epoch [14/50], Step [800/3815], Loss: 0.0486\n","Epoch [14/50], Step [1000/3815], Loss: 0.0038\n","Epoch [14/50], Step [1200/3815], Loss: 0.0077\n","Epoch [14/50], Step [1400/3815], Loss: 0.0014\n","Epoch [14/50], Step [1600/3815], Loss: 0.0006\n","Epoch [14/50], Step [1800/3815], Loss: 0.0017\n","Epoch [14/50], Step [2000/3815], Loss: 0.0031\n","Epoch [14/50], Step [2200/3815], Loss: 0.0023\n","Epoch [14/50], Step [2400/3815], Loss: 0.0066\n","Epoch [14/50], Step [2600/3815], Loss: 0.0021\n","Epoch [14/50], Step [2800/3815], Loss: 0.0031\n","Epoch [14/50], Step [3000/3815], Loss: 0.1458\n","Epoch [14/50], Step [3200/3815], Loss: 0.0010\n","Epoch [14/50], Step [3400/3815], Loss: 0.0026\n","Epoch [14/50], Step [3600/3815], Loss: 0.0007\n","Epoch [14/50], Step [3800/3815], Loss: 0.0151\n","Epoch [15/50], Step [200/3815], Loss: 0.0536\n","Epoch [15/50], Step [400/3815], Loss: 0.0034\n","Epoch [15/50], Step [600/3815], Loss: 0.0066\n","Epoch [15/50], Step [800/3815], Loss: 0.0608\n","Epoch [15/50], Step [1000/3815], Loss: 0.0018\n","Epoch [15/50], Step [1200/3815], Loss: 0.0021\n","Epoch [15/50], Step [1400/3815], Loss: 0.0482\n","Epoch [15/50], Step [1600/3815], Loss: 0.0009\n","Epoch [15/50], Step [1800/3815], Loss: 0.0035\n","Epoch [15/50], Step [2000/3815], Loss: 0.0029\n","Epoch [15/50], Step [2200/3815], Loss: 0.0009\n","Epoch [15/50], Step [2400/3815], Loss: 0.0093\n","Epoch [15/50], Step [2600/3815], Loss: 0.0004\n","Epoch [15/50], Step [2800/3815], Loss: 0.0005\n","Epoch [15/50], Step [3000/3815], Loss: 0.0231\n","Epoch [15/50], Step [3200/3815], Loss: 0.0015\n","Epoch [15/50], Step [3400/3815], Loss: 0.0024\n","Epoch [15/50], Step [3600/3815], Loss: 0.0011\n","Epoch [15/50], Step [3800/3815], Loss: 0.0007\n","Epoch [16/50], Step [200/3815], Loss: 0.0007\n","Epoch [16/50], Step [400/3815], Loss: 0.0017\n","Epoch [16/50], Step [600/3815], Loss: 0.0017\n","Epoch [16/50], Step [800/3815], Loss: 0.0571\n","Epoch [16/50], Step [1000/3815], Loss: 0.0008\n","Epoch [16/50], Step [1200/3815], Loss: 0.0036\n","Epoch [16/50], Step [1400/3815], Loss: 0.0013\n","Epoch [16/50], Step [1600/3815], Loss: 0.0634\n","Epoch [16/50], Step [1800/3815], Loss: 0.0035\n","Epoch [16/50], Step [2000/3815], Loss: 0.0039\n","Epoch [16/50], Step [2200/3815], Loss: 0.0005\n","Epoch [16/50], Step [2400/3815], Loss: 0.0008\n","Epoch [16/50], Step [2600/3815], Loss: 0.0006\n","Epoch [16/50], Step [2800/3815], Loss: 0.0047\n","Epoch [16/50], Step [3000/3815], Loss: 0.0577\n","Epoch [16/50], Step [3200/3815], Loss: 0.0008\n","Epoch [16/50], Step [3400/3815], Loss: 0.0025\n","Epoch [16/50], Step [3600/3815], Loss: 0.0017\n","Epoch [16/50], Step [3800/3815], Loss: 0.0019\n","Epoch [17/50], Step [200/3815], Loss: 0.0070\n","Epoch [17/50], Step [400/3815], Loss: 0.0004\n","Epoch [17/50], Step [600/3815], Loss: 0.0004\n","Epoch [17/50], Step [800/3815], Loss: 0.0006\n","Epoch [17/50], Step [1000/3815], Loss: 0.0176\n","Epoch [17/50], Step [1200/3815], Loss: 0.0004\n","Epoch [17/50], Step [1400/3815], Loss: 0.0043\n","Epoch [17/50], Step [1600/3815], Loss: 0.0041\n","Epoch [17/50], Step [1800/3815], Loss: 0.0002\n","Epoch [17/50], Step [2000/3815], Loss: 0.0006\n","Epoch [17/50], Step [2200/3815], Loss: 0.0002\n","Epoch [17/50], Step [2400/3815], Loss: 0.1209\n","Epoch [17/50], Step [2600/3815], Loss: 0.0420\n","Epoch [17/50], Step [2800/3815], Loss: 0.0017\n","Epoch [17/50], Step [3000/3815], Loss: 0.0047\n","Epoch [17/50], Step [3200/3815], Loss: 0.0004\n","Epoch [17/50], Step [3400/3815], Loss: 0.0006\n","Epoch [17/50], Step [3600/3815], Loss: 0.0021\n","Epoch [17/50], Step [3800/3815], Loss: 0.0005\n","Epoch [18/50], Step [200/3815], Loss: 0.0657\n","Epoch [18/50], Step [400/3815], Loss: 0.0035\n","Epoch [18/50], Step [600/3815], Loss: 0.0008\n","Epoch [18/50], Step [800/3815], Loss: 0.0159\n","Epoch [18/50], Step [1000/3815], Loss: 0.0643\n","Epoch [18/50], Step [1200/3815], Loss: 0.0927\n","Epoch [18/50], Step [1400/3815], Loss: 0.0159\n","Epoch [18/50], Step [1600/3815], Loss: 0.0004\n","Epoch [18/50], Step [1800/3815], Loss: 0.0068\n","Epoch [18/50], Step [2000/3815], Loss: 0.1511\n","Epoch [18/50], Step [2200/3815], Loss: 0.0011\n","Epoch [18/50], Step [2400/3815], Loss: 0.0012\n","Epoch [18/50], Step [2600/3815], Loss: 0.0023\n","Epoch [18/50], Step [2800/3815], Loss: 0.0013\n","Epoch [18/50], Step [3000/3815], Loss: 0.0028\n","Epoch [18/50], Step [3200/3815], Loss: 0.0017\n","Epoch [18/50], Step [3400/3815], Loss: 0.0007\n","Epoch [18/50], Step [3600/3815], Loss: 0.0005\n","Epoch [18/50], Step [3800/3815], Loss: 0.0030\n","Epoch [19/50], Step [200/3815], Loss: 0.1022\n","Epoch [19/50], Step [400/3815], Loss: 0.0007\n","Epoch [19/50], Step [600/3815], Loss: 0.0046\n","Epoch [19/50], Step [800/3815], Loss: 0.0052\n","Epoch [19/50], Step [1000/3815], Loss: 0.0026\n","Epoch [19/50], Step [1200/3815], Loss: 0.0022\n","Epoch [19/50], Step [1400/3815], Loss: 0.0016\n","Epoch [19/50], Step [1600/3815], Loss: 0.0024\n","Epoch [19/50], Step [1800/3815], Loss: 0.0005\n","Epoch [19/50], Step [2000/3815], Loss: 0.0411\n","Epoch [19/50], Step [2200/3815], Loss: 0.0010\n","Epoch [19/50], Step [2400/3815], Loss: 0.0005\n","Epoch [19/50], Step [2600/3815], Loss: 0.0018\n","Epoch [19/50], Step [2800/3815], Loss: 0.0012\n","Epoch [19/50], Step [3000/3815], Loss: 0.0016\n","Epoch [19/50], Step [3200/3815], Loss: 0.0012\n","Epoch [19/50], Step [3400/3815], Loss: 0.0476\n","Epoch [19/50], Step [3600/3815], Loss: 0.0069\n","Epoch [19/50], Step [3800/3815], Loss: 0.0004\n","Epoch [20/50], Step [200/3815], Loss: 0.0009\n","Epoch [20/50], Step [400/3815], Loss: 0.0021\n","Epoch [20/50], Step [600/3815], Loss: 0.1095\n","Epoch [20/50], Step [800/3815], Loss: 0.0010\n","Epoch [20/50], Step [1000/3815], Loss: 0.0009\n","Epoch [20/50], Step [1200/3815], Loss: 0.0008\n","Epoch [20/50], Step [1400/3815], Loss: 0.0097\n","Epoch [20/50], Step [1600/3815], Loss: 0.0008\n","Epoch [20/50], Step [1800/3815], Loss: 0.0007\n","Epoch [20/50], Step [2000/3815], Loss: 0.0008\n","Epoch [20/50], Step [2200/3815], Loss: 0.0004\n","Epoch [20/50], Step [2400/3815], Loss: 0.0027\n","Epoch [20/50], Step [2600/3815], Loss: 0.0011\n","Epoch [20/50], Step [2800/3815], Loss: 0.0006\n","Epoch [20/50], Step [3000/3815], Loss: 0.0024\n","Epoch [20/50], Step [3200/3815], Loss: 0.0003\n","Epoch [20/50], Step [3400/3815], Loss: 0.0008\n","Epoch [20/50], Step [3600/3815], Loss: 0.0002\n","Epoch [20/50], Step [3800/3815], Loss: 0.0002\n","Epoch [21/50], Step [200/3815], Loss: 0.0010\n","Epoch [21/50], Step [400/3815], Loss: 0.0015\n","Epoch [21/50], Step [600/3815], Loss: 0.0011\n","Epoch [21/50], Step [800/3815], Loss: 0.0003\n","Epoch [21/50], Step [1000/3815], Loss: 0.0003\n","Epoch [21/50], Step [1200/3815], Loss: 0.0460\n","Epoch [21/50], Step [1400/3815], Loss: 0.0031\n","Epoch [21/50], Step [1600/3815], Loss: 0.0656\n","Epoch [21/50], Step [1800/3815], Loss: 0.0176\n","Epoch [21/50], Step [2000/3815], Loss: 0.0015\n","Epoch [21/50], Step [2200/3815], Loss: 0.0004\n","Epoch [21/50], Step [2400/3815], Loss: 0.0013\n","Epoch [21/50], Step [2600/3815], Loss: 0.0004\n","Epoch [21/50], Step [2800/3815], Loss: 0.0037\n","Epoch [21/50], Step [3000/3815], Loss: 0.0007\n","Epoch [21/50], Step [3200/3815], Loss: 0.0004\n","Epoch [21/50], Step [3400/3815], Loss: 0.0003\n","Epoch [21/50], Step [3600/3815], Loss: 0.0004\n","Epoch [21/50], Step [3800/3815], Loss: 0.0060\n","Epoch [22/50], Step [200/3815], Loss: 0.0009\n","Epoch [22/50], Step [400/3815], Loss: 0.0004\n","Epoch [22/50], Step [600/3815], Loss: 0.0006\n","Epoch [22/50], Step [800/3815], Loss: 0.0003\n","Epoch [22/50], Step [1000/3815], Loss: 0.0885\n","Epoch [22/50], Step [1200/3815], Loss: 0.0004\n","Epoch [22/50], Step [1400/3815], Loss: 0.0244\n","Epoch [22/50], Step [1600/3815], Loss: 0.0015\n","Epoch [22/50], Step [1800/3815], Loss: 0.0052\n","Epoch [22/50], Step [2000/3815], Loss: 0.0034\n","Epoch [22/50], Step [2200/3815], Loss: 0.0026\n","Epoch [22/50], Step [2400/3815], Loss: 0.0047\n","Epoch [22/50], Step [2600/3815], Loss: 0.0001\n","Epoch [22/50], Step [2800/3815], Loss: 0.0163\n","Epoch [22/50], Step [3000/3815], Loss: 0.0025\n","Epoch [22/50], Step [3200/3815], Loss: 0.0026\n","Epoch [22/50], Step [3400/3815], Loss: 0.0004\n","Epoch [22/50], Step [3600/3815], Loss: 0.0014\n","Epoch [22/50], Step [3800/3815], Loss: 0.0097\n","Epoch [23/50], Step [200/3815], Loss: 0.0008\n","Epoch [23/50], Step [400/3815], Loss: 0.0003\n","Epoch [23/50], Step [600/3815], Loss: 0.0003\n","Epoch [23/50], Step [800/3815], Loss: 0.0003\n","Epoch [23/50], Step [1000/3815], Loss: 0.0007\n","Epoch [23/50], Step [1200/3815], Loss: 0.0090\n","Epoch [23/50], Step [1400/3815], Loss: 0.0004\n","Epoch [23/50], Step [1600/3815], Loss: 0.0258\n","Epoch [23/50], Step [1800/3815], Loss: 0.0008\n","Epoch [23/50], Step [2000/3815], Loss: 0.0010\n","Epoch [23/50], Step [2200/3815], Loss: 0.0004\n","Epoch [23/50], Step [2400/3815], Loss: 0.0073\n","Epoch [23/50], Step [2600/3815], Loss: 0.0012\n","Epoch [23/50], Step [2800/3815], Loss: 0.0009\n","Epoch [23/50], Step [3000/3815], Loss: 0.1092\n","Epoch [23/50], Step [3200/3815], Loss: 0.0002\n","Epoch [23/50], Step [3400/3815], Loss: 0.0062\n","Epoch [23/50], Step [3600/3815], Loss: 0.0340\n","Epoch [23/50], Step [3800/3815], Loss: 0.0015\n","Epoch [24/50], Step [200/3815], Loss: 0.0013\n","Epoch [24/50], Step [400/3815], Loss: 0.0004\n","Epoch [24/50], Step [600/3815], Loss: 0.0008\n","Epoch [24/50], Step [800/3815], Loss: 0.0005\n","Epoch [24/50], Step [1000/3815], Loss: 0.0004\n","Epoch [24/50], Step [1200/3815], Loss: 0.0009\n","Epoch [24/50], Step [1400/3815], Loss: 0.0002\n","Epoch [24/50], Step [1600/3815], Loss: 0.0290\n","Epoch [24/50], Step [1800/3815], Loss: 0.0001\n","Epoch [24/50], Step [2000/3815], Loss: 0.0005\n","Epoch [24/50], Step [2200/3815], Loss: 0.0003\n","Epoch [24/50], Step [2400/3815], Loss: 0.0005\n","Epoch [24/50], Step [2600/3815], Loss: 0.0002\n","Epoch [24/50], Step [2800/3815], Loss: 0.0041\n","Epoch [24/50], Step [3000/3815], Loss: 0.0011\n","Epoch [24/50], Step [3200/3815], Loss: 0.0010\n","Epoch [24/50], Step [3400/3815], Loss: 0.0002\n","Epoch [24/50], Step [3600/3815], Loss: 0.0031\n","Epoch [24/50], Step [3800/3815], Loss: 0.0003\n","Epoch [25/50], Step [200/3815], Loss: 0.0021\n","Epoch [25/50], Step [400/3815], Loss: 0.0005\n","Epoch [25/50], Step [600/3815], Loss: 0.0346\n","Epoch [25/50], Step [800/3815], Loss: 0.0004\n","Epoch [25/50], Step [1000/3815], Loss: 0.1135\n","Epoch [25/50], Step [1200/3815], Loss: 0.0004\n","Epoch [25/50], Step [1400/3815], Loss: 0.0006\n","Epoch [25/50], Step [1600/3815], Loss: 0.0014\n","Epoch [25/50], Step [1800/3815], Loss: 0.0003\n","Epoch [25/50], Step [2000/3815], Loss: 0.0068\n","Epoch [25/50], Step [2200/3815], Loss: 0.0038\n","Epoch [25/50], Step [2400/3815], Loss: 0.0002\n","Epoch [25/50], Step [2600/3815], Loss: 0.0005\n","Epoch [25/50], Step [2800/3815], Loss: 0.0007\n","Epoch [25/50], Step [3000/3815], Loss: 0.0001\n","Epoch [25/50], Step [3200/3815], Loss: 0.0004\n","Epoch [25/50], Step [3400/3815], Loss: 0.0035\n","Epoch [25/50], Step [3600/3815], Loss: 0.0007\n","Epoch [25/50], Step [3800/3815], Loss: 0.0063\n","Epoch [26/50], Step [200/3815], Loss: 0.0003\n","Epoch [26/50], Step [400/3815], Loss: 0.0029\n","Epoch [26/50], Step [600/3815], Loss: 0.0001\n","Epoch [26/50], Step [800/3815], Loss: 0.0020\n","Epoch [26/50], Step [1000/3815], Loss: 0.0291\n","Epoch [26/50], Step [1200/3815], Loss: 0.0027\n","Epoch [26/50], Step [1400/3815], Loss: 0.0002\n","Epoch [26/50], Step [1600/3815], Loss: 0.0005\n","Epoch [26/50], Step [1800/3815], Loss: 0.0010\n","Epoch [26/50], Step [2000/3815], Loss: 0.0003\n","Epoch [26/50], Step [2200/3815], Loss: 0.0009\n","Epoch [26/50], Step [2400/3815], Loss: 0.0001\n","Epoch [26/50], Step [2600/3815], Loss: 0.0004\n","Epoch [26/50], Step [2800/3815], Loss: 0.0003\n","Epoch [26/50], Step [3000/3815], Loss: 0.0003\n","Epoch [26/50], Step [3200/3815], Loss: 0.0024\n","Epoch [26/50], Step [3400/3815], Loss: 0.0002\n","Epoch [26/50], Step [3600/3815], Loss: 0.0028\n","Epoch [26/50], Step [3800/3815], Loss: 0.0006\n","Epoch [27/50], Step [200/3815], Loss: 0.0006\n","Epoch [27/50], Step [400/3815], Loss: 0.1086\n","Epoch [27/50], Step [600/3815], Loss: 0.0006\n","Epoch [27/50], Step [800/3815], Loss: 0.0011\n","Epoch [27/50], Step [1000/3815], Loss: 0.0007\n","Epoch [27/50], Step [1200/3815], Loss: 0.0002\n","Epoch [27/50], Step [1400/3815], Loss: 0.0002\n","Epoch [27/50], Step [1600/3815], Loss: 0.0007\n","Epoch [27/50], Step [1800/3815], Loss: 0.0005\n","Epoch [27/50], Step [2000/3815], Loss: 0.0730\n","Epoch [27/50], Step [2200/3815], Loss: 0.0003\n","Epoch [27/50], Step [2400/3815], Loss: 0.0005\n","Epoch [27/50], Step [2600/3815], Loss: 0.0010\n","Epoch [27/50], Step [2800/3815], Loss: 0.0005\n","Epoch [27/50], Step [3000/3815], Loss: 0.0039\n","Epoch [27/50], Step [3200/3815], Loss: 0.0279\n","Epoch [27/50], Step [3400/3815], Loss: 0.0005\n","Epoch [27/50], Step [3600/3815], Loss: 0.0004\n","Epoch [27/50], Step [3800/3815], Loss: 0.0011\n","Epoch [28/50], Step [200/3815], Loss: 0.0004\n","Epoch [28/50], Step [400/3815], Loss: 0.0005\n","Epoch [28/50], Step [600/3815], Loss: 0.0007\n","Epoch [28/50], Step [800/3815], Loss: 0.0681\n","Epoch [28/50], Step [1000/3815], Loss: 0.0002\n","Epoch [28/50], Step [1200/3815], Loss: 0.0004\n","Epoch [28/50], Step [1400/3815], Loss: 0.0005\n","Epoch [28/50], Step [1600/3815], Loss: 0.0001\n","Epoch [28/50], Step [1800/3815], Loss: 0.0001\n","Epoch [28/50], Step [2000/3815], Loss: 0.0037\n","Epoch [28/50], Step [2200/3815], Loss: 0.0003\n","Epoch [28/50], Step [2400/3815], Loss: 0.0005\n","Epoch [28/50], Step [2600/3815], Loss: 0.0003\n","Epoch [28/50], Step [2800/3815], Loss: 0.0010\n","Epoch [28/50], Step [3000/3815], Loss: 0.0004\n","Epoch [28/50], Step [3200/3815], Loss: 0.0002\n","Epoch [28/50], Step [3400/3815], Loss: 0.0284\n","Epoch [28/50], Step [3600/3815], Loss: 0.0001\n","Epoch [28/50], Step [3800/3815], Loss: 0.0266\n","Epoch [29/50], Step [200/3815], Loss: 0.0005\n","Epoch [29/50], Step [400/3815], Loss: 0.0006\n","Epoch [29/50], Step [600/3815], Loss: 0.0011\n","Epoch [29/50], Step [800/3815], Loss: 0.0002\n","Epoch [29/50], Step [1000/3815], Loss: 0.0002\n","Epoch [29/50], Step [1200/3815], Loss: 0.0013\n","Epoch [29/50], Step [1400/3815], Loss: 0.0062\n","Epoch [29/50], Step [1600/3815], Loss: 0.0010\n","Epoch [29/50], Step [1800/3815], Loss: 0.0009\n","Epoch [29/50], Step [2000/3815], Loss: 0.0001\n","Epoch [29/50], Step [2200/3815], Loss: 0.0021\n","Epoch [29/50], Step [2400/3815], Loss: 0.0006\n","Epoch [29/50], Step [2600/3815], Loss: 0.0005\n","Epoch [29/50], Step [2800/3815], Loss: 0.0005\n","Epoch [29/50], Step [3000/3815], Loss: 0.0006\n","Epoch [29/50], Step [3200/3815], Loss: 0.0001\n","Epoch [29/50], Step [3400/3815], Loss: 0.0013\n","Epoch [29/50], Step [3600/3815], Loss: 0.0003\n","Epoch [29/50], Step [3800/3815], Loss: 0.0005\n","Epoch [30/50], Step [200/3815], Loss: 0.0002\n","Epoch [30/50], Step [400/3815], Loss: 0.0038\n","Epoch [30/50], Step [600/3815], Loss: 0.0255\n","Epoch [30/50], Step [800/3815], Loss: 0.0002\n","Epoch [30/50], Step [1000/3815], Loss: 0.0007\n","Epoch [30/50], Step [1200/3815], Loss: 0.0004\n","Epoch [30/50], Step [1400/3815], Loss: 0.0046\n","Epoch [30/50], Step [1600/3815], Loss: 0.1121\n","Epoch [30/50], Step [1800/3815], Loss: 0.0005\n","Epoch [30/50], Step [2000/3815], Loss: 0.0003\n","Epoch [30/50], Step [2200/3815], Loss: 0.0013\n","Epoch [30/50], Step [2400/3815], Loss: 0.0007\n","Epoch [30/50], Step [2600/3815], Loss: 0.0005\n","Epoch [30/50], Step [2800/3815], Loss: 0.0003\n","Epoch [30/50], Step [3000/3815], Loss: 0.0002\n","Epoch [30/50], Step [3200/3815], Loss: 0.0004\n","Epoch [30/50], Step [3400/3815], Loss: 0.0002\n","Epoch [30/50], Step [3600/3815], Loss: 0.0002\n","Epoch [30/50], Step [3800/3815], Loss: 0.0021\n","Epoch [31/50], Step [200/3815], Loss: 0.0123\n","Epoch [31/50], Step [400/3815], Loss: 0.0204\n","Epoch [31/50], Step [600/3815], Loss: 0.0009\n","Epoch [31/50], Step [800/3815], Loss: 0.0010\n","Epoch [31/50], Step [1000/3815], Loss: 0.0002\n","Epoch [31/50], Step [1200/3815], Loss: 0.0004\n","Epoch [31/50], Step [1400/3815], Loss: 0.0006\n","Epoch [31/50], Step [1600/3815], Loss: 0.0002\n","Epoch [31/50], Step [1800/3815], Loss: 0.0006\n","Epoch [31/50], Step [2000/3815], Loss: 0.0001\n","Epoch [31/50], Step [2200/3815], Loss: 0.0010\n","Epoch [31/50], Step [2400/3815], Loss: 0.0002\n","Epoch [31/50], Step [2600/3815], Loss: 0.0012\n","Epoch [31/50], Step [2800/3815], Loss: 0.0020\n","Epoch [31/50], Step [3000/3815], Loss: 0.0008\n","Epoch [31/50], Step [3200/3815], Loss: 0.0001\n","Epoch [31/50], Step [3400/3815], Loss: 0.0000\n","Epoch [31/50], Step [3600/3815], Loss: 0.0017\n","Epoch [31/50], Step [3800/3815], Loss: 0.0056\n","Epoch [32/50], Step [200/3815], Loss: 0.0003\n","Epoch [32/50], Step [400/3815], Loss: 0.0003\n","Epoch [32/50], Step [600/3815], Loss: 0.0004\n","Epoch [32/50], Step [800/3815], Loss: 0.0002\n","Epoch [32/50], Step [1000/3815], Loss: 0.0009\n","Epoch [32/50], Step [1200/3815], Loss: 0.0002\n","Epoch [32/50], Step [1400/3815], Loss: 0.0002\n","Epoch [32/50], Step [1600/3815], Loss: 0.0007\n","Epoch [32/50], Step [1800/3815], Loss: 0.0001\n","Epoch [32/50], Step [2000/3815], Loss: 0.0003\n","Epoch [32/50], Step [2200/3815], Loss: 0.0010\n","Epoch [32/50], Step [2400/3815], Loss: 0.0003\n","Epoch [32/50], Step [2600/3815], Loss: 0.0005\n","Epoch [32/50], Step [2800/3815], Loss: 0.0013\n","Epoch [32/50], Step [3000/3815], Loss: 0.0025\n","Epoch [32/50], Step [3200/3815], Loss: 0.0015\n","Epoch [32/50], Step [3400/3815], Loss: 0.0011\n","Epoch [32/50], Step [3600/3815], Loss: 0.0002\n","Epoch [32/50], Step [3800/3815], Loss: 0.1019\n","Epoch [33/50], Step [200/3815], Loss: 0.0004\n","Epoch [33/50], Step [400/3815], Loss: 0.0003\n","Epoch [33/50], Step [600/3815], Loss: 0.0004\n","Epoch [33/50], Step [800/3815], Loss: 0.0001\n","Epoch [33/50], Step [1000/3815], Loss: 0.0002\n","Epoch [33/50], Step [1200/3815], Loss: 0.0006\n","Epoch [33/50], Step [1400/3815], Loss: 0.0004\n","Epoch [33/50], Step [1600/3815], Loss: 0.0041\n","Epoch [33/50], Step [1800/3815], Loss: 0.0005\n","Epoch [33/50], Step [2000/3815], Loss: 0.0004\n","Epoch [33/50], Step [2200/3815], Loss: 0.0003\n","Epoch [33/50], Step [2400/3815], Loss: 0.0001\n","Epoch [33/50], Step [2600/3815], Loss: 0.0001\n","Epoch [33/50], Step [2800/3815], Loss: 0.0002\n","Epoch [33/50], Step [3000/3815], Loss: 0.0001\n","Epoch [33/50], Step [3200/3815], Loss: 0.0002\n","Epoch [33/50], Step [3400/3815], Loss: 0.0002\n","Epoch [33/50], Step [3600/3815], Loss: 0.1335\n","Epoch [33/50], Step [3800/3815], Loss: 0.0007\n","Epoch [34/50], Step [200/3815], Loss: 0.0002\n","Epoch [34/50], Step [400/3815], Loss: 0.0006\n","Epoch [34/50], Step [600/3815], Loss: 0.0008\n","Epoch [34/50], Step [800/3815], Loss: 0.0007\n","Epoch [34/50], Step [1000/3815], Loss: 0.0003\n","Epoch [34/50], Step [1200/3815], Loss: 0.0003\n","Epoch [34/50], Step [1400/3815], Loss: 0.0007\n","Epoch [34/50], Step [1600/3815], Loss: 0.0017\n","Epoch [34/50], Step [1800/3815], Loss: 0.0002\n","Epoch [34/50], Step [2000/3815], Loss: 0.0001\n","Epoch [34/50], Step [2200/3815], Loss: 0.0001\n","Epoch [34/50], Step [2400/3815], Loss: 0.0006\n","Epoch [34/50], Step [2600/3815], Loss: 0.0003\n","Epoch [34/50], Step [2800/3815], Loss: 0.0001\n","Epoch [34/50], Step [3000/3815], Loss: 0.0013\n","Epoch [34/50], Step [3200/3815], Loss: 0.0002\n","Epoch [34/50], Step [3400/3815], Loss: 0.0002\n","Epoch [34/50], Step [3600/3815], Loss: 0.0003\n","Epoch [34/50], Step [3800/3815], Loss: 0.0001\n","Epoch [35/50], Step [200/3815], Loss: 0.0012\n","Epoch [35/50], Step [400/3815], Loss: 0.0003\n","Epoch [35/50], Step [600/3815], Loss: 0.0001\n","Epoch [35/50], Step [800/3815], Loss: 0.0002\n","Epoch [35/50], Step [1000/3815], Loss: 0.0001\n","Epoch [35/50], Step [1200/3815], Loss: 0.0001\n","Epoch [35/50], Step [1400/3815], Loss: 0.0003\n","Epoch [35/50], Step [1600/3815], Loss: 0.0001\n","Epoch [35/50], Step [1800/3815], Loss: 0.0038\n","Epoch [35/50], Step [2000/3815], Loss: 0.0005\n","Epoch [35/50], Step [2200/3815], Loss: 0.0010\n","Epoch [35/50], Step [2400/3815], Loss: 0.0004\n","Epoch [35/50], Step [2600/3815], Loss: 0.0002\n","Epoch [35/50], Step [2800/3815], Loss: 0.0001\n","Epoch [35/50], Step [3000/3815], Loss: 0.0002\n","Epoch [35/50], Step [3200/3815], Loss: 0.0007\n","Epoch [35/50], Step [3400/3815], Loss: 0.0014\n","Epoch [35/50], Step [3600/3815], Loss: 0.0132\n","Epoch [35/50], Step [3800/3815], Loss: 0.0015\n","Epoch [36/50], Step [200/3815], Loss: 0.0008\n","Epoch [36/50], Step [400/3815], Loss: 0.0001\n","Epoch [36/50], Step [600/3815], Loss: 0.0003\n","Epoch [36/50], Step [800/3815], Loss: 0.0012\n","Epoch [36/50], Step [1000/3815], Loss: 0.0000\n","Epoch [36/50], Step [1200/3815], Loss: 0.0002\n","Epoch [36/50], Step [1400/3815], Loss: 0.0761\n","Epoch [36/50], Step [1600/3815], Loss: 0.0001\n","Epoch [36/50], Step [1800/3815], Loss: 0.0002\n","Epoch [36/50], Step [2000/3815], Loss: 0.0001\n","Epoch [36/50], Step [2200/3815], Loss: 0.0004\n","Epoch [36/50], Step [2400/3815], Loss: 0.0002\n","Epoch [36/50], Step [2600/3815], Loss: 0.0002\n","Epoch [36/50], Step [2800/3815], Loss: 0.0001\n","Epoch [36/50], Step [3000/3815], Loss: 0.0000\n","Epoch [36/50], Step [3200/3815], Loss: 0.0000\n","Epoch [36/50], Step [3400/3815], Loss: 0.0023\n","Epoch [36/50], Step [3600/3815], Loss: 0.0003\n","Epoch [36/50], Step [3800/3815], Loss: 0.0005\n","Epoch [37/50], Step [200/3815], Loss: 0.0010\n","Epoch [37/50], Step [400/3815], Loss: 0.0009\n","Epoch [37/50], Step [600/3815], Loss: 0.0002\n","Epoch [37/50], Step [800/3815], Loss: 0.0000\n","Epoch [37/50], Step [1000/3815], Loss: 0.0003\n","Epoch [37/50], Step [1200/3815], Loss: 0.0006\n","Epoch [37/50], Step [1400/3815], Loss: 0.0001\n","Epoch [37/50], Step [1600/3815], Loss: 0.0002\n","Epoch [37/50], Step [1800/3815], Loss: 0.0002\n","Epoch [37/50], Step [2000/3815], Loss: 0.0004\n","Epoch [37/50], Step [2200/3815], Loss: 0.0035\n","Epoch [37/50], Step [2400/3815], Loss: 0.0002\n","Epoch [37/50], Step [2600/3815], Loss: 0.0001\n","Epoch [37/50], Step [2800/3815], Loss: 0.0002\n","Epoch [37/50], Step [3000/3815], Loss: 0.0006\n","Epoch [37/50], Step [3200/3815], Loss: 0.0001\n","Epoch [37/50], Step [3400/3815], Loss: 0.0002\n","Epoch [37/50], Step [3600/3815], Loss: 0.0001\n","Epoch [37/50], Step [3800/3815], Loss: 0.0004\n","Epoch [38/50], Step [200/3815], Loss: 0.0095\n","Epoch [38/50], Step [400/3815], Loss: 0.0000\n","Epoch [38/50], Step [600/3815], Loss: 0.0019\n","Epoch [38/50], Step [800/3815], Loss: 0.0002\n","Epoch [38/50], Step [1000/3815], Loss: 0.0013\n","Epoch [38/50], Step [1200/3815], Loss: 0.0016\n","Epoch [38/50], Step [1400/3815], Loss: 0.0002\n","Epoch [38/50], Step [1600/3815], Loss: 0.0003\n","Epoch [38/50], Step [1800/3815], Loss: 0.0004\n","Epoch [38/50], Step [2000/3815], Loss: 0.0000\n","Epoch [38/50], Step [2200/3815], Loss: 0.0006\n","Epoch [38/50], Step [2400/3815], Loss: 0.0009\n","Epoch [38/50], Step [2600/3815], Loss: 0.0002\n","Epoch [38/50], Step [2800/3815], Loss: 0.0002\n","Epoch [38/50], Step [3000/3815], Loss: 0.0000\n","Epoch [38/50], Step [3200/3815], Loss: 0.0001\n","Epoch [38/50], Step [3400/3815], Loss: 0.0000\n","Epoch [38/50], Step [3600/3815], Loss: 0.0006\n","Epoch [38/50], Step [3800/3815], Loss: 0.0007\n","Epoch [39/50], Step [200/3815], Loss: 0.0007\n","Epoch [39/50], Step [400/3815], Loss: 0.0007\n","Epoch [39/50], Step [600/3815], Loss: 0.0004\n","Epoch [39/50], Step [800/3815], Loss: 0.0001\n","Epoch [39/50], Step [1000/3815], Loss: 0.0001\n","Epoch [39/50], Step [1200/3815], Loss: 0.0300\n","Epoch [39/50], Step [1400/3815], Loss: 0.0006\n","Epoch [39/50], Step [1600/3815], Loss: 0.0001\n","Epoch [39/50], Step [1800/3815], Loss: 0.0012\n","Epoch [39/50], Step [2000/3815], Loss: 0.0002\n","Epoch [39/50], Step [2200/3815], Loss: 0.0072\n","Epoch [39/50], Step [2400/3815], Loss: 0.0000\n","Epoch [39/50], Step [2600/3815], Loss: 0.0113\n","Epoch [39/50], Step [2800/3815], Loss: 0.0629\n","Epoch [39/50], Step [3000/3815], Loss: 0.0000\n","Epoch [39/50], Step [3200/3815], Loss: 0.0029\n","Epoch [39/50], Step [3400/3815], Loss: 0.0004\n","Epoch [39/50], Step [3600/3815], Loss: 0.0001\n","Epoch [39/50], Step [3800/3815], Loss: 0.0001\n","Epoch [40/50], Step [200/3815], Loss: 0.0001\n","Epoch [40/50], Step [400/3815], Loss: 0.0001\n","Epoch [40/50], Step [600/3815], Loss: 0.1390\n","Epoch [40/50], Step [800/3815], Loss: 0.0001\n","Epoch [40/50], Step [1000/3815], Loss: 0.0000\n","Epoch [40/50], Step [1200/3815], Loss: 0.0007\n","Epoch [40/50], Step [1400/3815], Loss: 0.0001\n","Epoch [40/50], Step [1600/3815], Loss: 0.0368\n","Epoch [40/50], Step [1800/3815], Loss: 0.0001\n","Epoch [40/50], Step [2000/3815], Loss: 0.0002\n","Epoch [40/50], Step [2200/3815], Loss: 0.0055\n","Epoch [40/50], Step [2400/3815], Loss: 0.0028\n","Epoch [40/50], Step [2600/3815], Loss: 0.0001\n","Epoch [40/50], Step [2800/3815], Loss: 0.0005\n","Epoch [40/50], Step [3000/3815], Loss: 0.0000\n","Epoch [40/50], Step [3200/3815], Loss: 0.0003\n","Epoch [40/50], Step [3400/3815], Loss: 0.0003\n","Epoch [40/50], Step [3600/3815], Loss: 0.0002\n","Epoch [40/50], Step [3800/3815], Loss: 0.0000\n","Epoch [41/50], Step [200/3815], Loss: 0.0001\n","Epoch [41/50], Step [400/3815], Loss: 0.0003\n","Epoch [41/50], Step [600/3815], Loss: 0.0003\n","Epoch [41/50], Step [800/3815], Loss: 0.0000\n","Epoch [41/50], Step [1000/3815], Loss: 0.0001\n","Epoch [41/50], Step [1200/3815], Loss: 0.0001\n","Epoch [41/50], Step [1400/3815], Loss: 0.0002\n","Epoch [41/50], Step [1600/3815], Loss: 0.0182\n","Epoch [41/50], Step [1800/3815], Loss: 0.0001\n","Epoch [41/50], Step [2000/3815], Loss: 0.0006\n","Epoch [41/50], Step [2200/3815], Loss: 0.0002\n","Epoch [41/50], Step [2400/3815], Loss: 0.0005\n","Epoch [41/50], Step [2600/3815], Loss: 0.0001\n","Epoch [41/50], Step [2800/3815], Loss: 0.0011\n","Epoch [41/50], Step [3000/3815], Loss: 0.0003\n","Epoch [41/50], Step [3200/3815], Loss: 0.0029\n","Epoch [41/50], Step [3400/3815], Loss: 0.0002\n","Epoch [41/50], Step [3600/3815], Loss: 0.0001\n","Epoch [41/50], Step [3800/3815], Loss: 0.0001\n","Epoch [42/50], Step [200/3815], Loss: 0.0023\n","Epoch [42/50], Step [400/3815], Loss: 0.0000\n","Epoch [42/50], Step [600/3815], Loss: 0.0016\n","Epoch [42/50], Step [800/3815], Loss: 0.0002\n","Epoch [42/50], Step [1000/3815], Loss: 0.0868\n","Epoch [42/50], Step [1200/3815], Loss: 0.0440\n","Epoch [42/50], Step [1400/3815], Loss: 0.0001\n","Epoch [42/50], Step [1600/3815], Loss: 0.0001\n","Epoch [42/50], Step [1800/3815], Loss: 0.0003\n","Epoch [42/50], Step [2000/3815], Loss: 0.0001\n","Epoch [42/50], Step [2200/3815], Loss: 0.0012\n","Epoch [42/50], Step [2400/3815], Loss: 0.0000\n","Epoch [42/50], Step [2600/3815], Loss: 0.0003\n","Epoch [42/50], Step [2800/3815], Loss: 0.0002\n","Epoch [42/50], Step [3000/3815], Loss: 0.0000\n","Epoch [42/50], Step [3200/3815], Loss: 0.0004\n","Epoch [42/50], Step [3400/3815], Loss: 0.0002\n","Epoch [42/50], Step [3600/3815], Loss: 0.0003\n","Epoch [42/50], Step [3800/3815], Loss: 0.0006\n","Epoch [43/50], Step [200/3815], Loss: 0.0001\n","Epoch [43/50], Step [400/3815], Loss: 0.0000\n","Epoch [43/50], Step [600/3815], Loss: 0.0002\n","Epoch [43/50], Step [800/3815], Loss: 0.0000\n","Epoch [43/50], Step [1000/3815], Loss: 0.0003\n","Epoch [43/50], Step [1200/3815], Loss: 0.0003\n","Epoch [43/50], Step [1400/3815], Loss: 0.0001\n","Epoch [43/50], Step [1600/3815], Loss: 0.0000\n","Epoch [43/50], Step [1800/3815], Loss: 0.0007\n","Epoch [43/50], Step [2000/3815], Loss: 0.0000\n","Epoch [43/50], Step [2200/3815], Loss: 0.0062\n","Epoch [43/50], Step [2400/3815], Loss: 0.0002\n","Epoch [43/50], Step [2600/3815], Loss: 0.0002\n","Epoch [43/50], Step [2800/3815], Loss: 0.0005\n","Epoch [43/50], Step [3000/3815], Loss: 0.0001\n","Epoch [43/50], Step [3200/3815], Loss: 0.0015\n","Epoch [43/50], Step [3400/3815], Loss: 0.0002\n","Epoch [43/50], Step [3600/3815], Loss: 0.0003\n","Epoch [43/50], Step [3800/3815], Loss: 0.0028\n","Epoch [44/50], Step [200/3815], Loss: 0.0001\n","Epoch [44/50], Step [400/3815], Loss: 0.0003\n","Epoch [44/50], Step [600/3815], Loss: 0.0001\n","Epoch [44/50], Step [800/3815], Loss: 0.0002\n","Epoch [44/50], Step [1000/3815], Loss: 0.0001\n","Epoch [44/50], Step [1200/3815], Loss: 0.0194\n","Epoch [44/50], Step [1400/3815], Loss: 0.0002\n","Epoch [44/50], Step [1600/3815], Loss: 0.0116\n","Epoch [44/50], Step [1800/3815], Loss: 0.0002\n","Epoch [44/50], Step [2000/3815], Loss: 0.0000\n","Epoch [44/50], Step [2200/3815], Loss: 0.0005\n","Epoch [44/50], Step [2400/3815], Loss: 0.0002\n","Epoch [44/50], Step [2600/3815], Loss: 0.0001\n","Epoch [44/50], Step [2800/3815], Loss: 0.0001\n","Epoch [44/50], Step [3000/3815], Loss: 0.0003\n","Epoch [44/50], Step [3200/3815], Loss: 0.0002\n","Epoch [44/50], Step [3400/3815], Loss: 0.0000\n","Epoch [44/50], Step [3600/3815], Loss: 0.0500\n","Epoch [44/50], Step [3800/3815], Loss: 0.0009\n","Epoch [45/50], Step [200/3815], Loss: 0.0000\n","Epoch [45/50], Step [400/3815], Loss: 0.0009\n","Epoch [45/50], Step [600/3815], Loss: 0.0002\n","Epoch [45/50], Step [800/3815], Loss: 0.0000\n","Epoch [45/50], Step [1000/3815], Loss: 0.0000\n","Epoch [45/50], Step [1200/3815], Loss: 0.0001\n","Epoch [45/50], Step [1400/3815], Loss: 0.0005\n","Epoch [45/50], Step [1600/3815], Loss: 0.0000\n","Epoch [45/50], Step [1800/3815], Loss: 0.0000\n","Epoch [45/50], Step [2000/3815], Loss: 0.0001\n","Epoch [45/50], Step [2200/3815], Loss: 0.0001\n","Epoch [45/50], Step [2400/3815], Loss: 0.0001\n","Epoch [45/50], Step [2600/3815], Loss: 0.0006\n","Epoch [45/50], Step [2800/3815], Loss: 0.0001\n","Epoch [45/50], Step [3000/3815], Loss: 0.0001\n","Epoch [45/50], Step [3200/3815], Loss: 0.0001\n","Epoch [45/50], Step [3400/3815], Loss: 0.0455\n","Epoch [45/50], Step [3600/3815], Loss: 0.0001\n","Epoch [45/50], Step [3800/3815], Loss: 0.0001\n","Epoch [46/50], Step [200/3815], Loss: 0.0017\n","Epoch [46/50], Step [400/3815], Loss: 0.0017\n","Epoch [46/50], Step [600/3815], Loss: 0.0003\n","Epoch [46/50], Step [800/3815], Loss: 0.0002\n","Epoch [46/50], Step [1000/3815], Loss: 0.0001\n","Epoch [46/50], Step [1200/3815], Loss: 0.0000\n","Epoch [46/50], Step [1400/3815], Loss: 0.0001\n","Epoch [46/50], Step [1600/3815], Loss: 0.0007\n","Epoch [46/50], Step [1800/3815], Loss: 0.0003\n","Epoch [46/50], Step [2000/3815], Loss: 0.0003\n","Epoch [46/50], Step [2200/3815], Loss: 0.0001\n","Epoch [46/50], Step [2400/3815], Loss: 0.0000\n","Epoch [46/50], Step [2600/3815], Loss: 0.0000\n","Epoch [46/50], Step [2800/3815], Loss: 0.0001\n","Epoch [46/50], Step [3000/3815], Loss: 0.0003\n","Epoch [46/50], Step [3200/3815], Loss: 0.0002\n","Epoch [46/50], Step [3400/3815], Loss: 0.0000\n","Epoch [46/50], Step [3600/3815], Loss: 0.1080\n","Epoch [46/50], Step [3800/3815], Loss: 0.0003\n","Epoch [47/50], Step [200/3815], Loss: 0.0003\n","Epoch [47/50], Step [400/3815], Loss: 0.0001\n","Epoch [47/50], Step [600/3815], Loss: 0.0011\n","Epoch [47/50], Step [800/3815], Loss: 0.0002\n","Epoch [47/50], Step [1000/3815], Loss: 0.0001\n","Epoch [47/50], Step [1200/3815], Loss: 0.0001\n","Epoch [47/50], Step [1400/3815], Loss: 0.0047\n","Epoch [47/50], Step [1600/3815], Loss: 0.0000\n","Epoch [47/50], Step [1800/3815], Loss: 0.0002\n","Epoch [47/50], Step [2000/3815], Loss: 0.0001\n","Epoch [47/50], Step [2200/3815], Loss: 0.0000\n","Epoch [47/50], Step [2400/3815], Loss: 0.0001\n","Epoch [47/50], Step [2600/3815], Loss: 0.0001\n","Epoch [47/50], Step [2800/3815], Loss: 0.0006\n","Epoch [47/50], Step [3000/3815], Loss: 0.0002\n","Epoch [47/50], Step [3200/3815], Loss: 0.0001\n","Epoch [47/50], Step [3400/3815], Loss: 0.0001\n","Epoch [47/50], Step [3600/3815], Loss: 0.0003\n","Epoch [47/50], Step [3800/3815], Loss: 0.0001\n","Epoch [48/50], Step [200/3815], Loss: 0.0001\n","Epoch [48/50], Step [400/3815], Loss: 0.0007\n","Epoch [48/50], Step [600/3815], Loss: 0.0002\n","Epoch [48/50], Step [800/3815], Loss: 0.0000\n","Epoch [48/50], Step [1000/3815], Loss: 0.0000\n","Epoch [48/50], Step [1200/3815], Loss: 0.0002\n","Epoch [48/50], Step [1400/3815], Loss: 0.0001\n","Epoch [48/50], Step [1600/3815], Loss: 0.0118\n","Epoch [48/50], Step [1800/3815], Loss: 0.0001\n","Epoch [48/50], Step [2000/3815], Loss: 0.0000\n","Epoch [48/50], Step [2200/3815], Loss: 0.0002\n","Epoch [48/50], Step [2400/3815], Loss: 0.0001\n","Epoch [48/50], Step [2600/3815], Loss: 0.0003\n","Epoch [48/50], Step [2800/3815], Loss: 0.0002\n","Epoch [48/50], Step [3000/3815], Loss: 0.0003\n","Epoch [48/50], Step [3200/3815], Loss: 0.0003\n","Epoch [48/50], Step [3400/3815], Loss: 0.0054\n","Epoch [48/50], Step [3600/3815], Loss: 0.0001\n","Epoch [48/50], Step [3800/3815], Loss: 0.0001\n","Epoch [49/50], Step [200/3815], Loss: 0.0000\n","Epoch [49/50], Step [400/3815], Loss: 0.0002\n","Epoch [49/50], Step [600/3815], Loss: 0.0002\n","Epoch [49/50], Step [800/3815], Loss: 0.0001\n","Epoch [49/50], Step [1000/3815], Loss: 0.0000\n","Epoch [49/50], Step [1200/3815], Loss: 0.0002\n","Epoch [49/50], Step [1400/3815], Loss: 0.0001\n","Epoch [49/50], Step [1600/3815], Loss: 0.0002\n","Epoch [49/50], Step [1800/3815], Loss: 0.0001\n","Epoch [49/50], Step [2000/3815], Loss: 0.0002\n","Epoch [49/50], Step [2200/3815], Loss: 0.0002\n","Epoch [49/50], Step [2400/3815], Loss: 0.0000\n","Epoch [49/50], Step [2600/3815], Loss: 0.0001\n","Epoch [49/50], Step [2800/3815], Loss: 0.0000\n","Epoch [49/50], Step [3000/3815], Loss: 0.0603\n","Epoch [49/50], Step [3200/3815], Loss: 0.0002\n","Epoch [49/50], Step [3400/3815], Loss: 0.0343\n","Epoch [49/50], Step [3600/3815], Loss: 0.0002\n","Epoch [49/50], Step [3800/3815], Loss: 0.0004\n","Epoch [50/50], Step [200/3815], Loss: 0.0001\n","Epoch [50/50], Step [400/3815], Loss: 0.0000\n","Epoch [50/50], Step [600/3815], Loss: 0.0253\n","Epoch [50/50], Step [800/3815], Loss: 0.0003\n","Epoch [50/50], Step [1000/3815], Loss: 0.0005\n","Epoch [50/50], Step [1200/3815], Loss: 0.0000\n","Epoch [50/50], Step [1400/3815], Loss: 0.0001\n","Epoch [50/50], Step [1600/3815], Loss: 0.0001\n","Epoch [50/50], Step [1800/3815], Loss: 0.0000\n","Epoch [50/50], Step [2000/3815], Loss: 0.0001\n","Epoch [50/50], Step [2200/3815], Loss: 0.0001\n","Epoch [50/50], Step [2400/3815], Loss: 0.0003\n","Epoch [50/50], Step [2600/3815], Loss: 0.0001\n","Epoch [50/50], Step [2800/3815], Loss: 0.0007\n","Epoch [50/50], Step [3000/3815], Loss: 0.0001\n","Epoch [50/50], Step [3200/3815], Loss: 0.0000\n","Epoch [50/50], Step [3400/3815], Loss: 0.0016\n","Epoch [50/50], Step [3600/3815], Loss: 0.0002\n","Epoch [50/50], Step [3800/3815], Loss: 0.0003\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-19-f2e417b33a9e>:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  predictions = [torch.nn.functional.softmax(r) for r in results]\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9974\n","Precision: 0.7339\n","Recall: 0.6107\n","F1 Score: 0.6667\n","ROC: 0.9897963676169378\n","PR AUC: 0.7208\n","              precision    recall  f1-score   support\n","\n","           0     0.9983    0.9990    0.9987     30516\n","           1     0.7339    0.6107    0.6667       131\n","\n","    accuracy                         0.9974     30647\n","   macro avg     0.8661    0.8049    0.8327     30647\n","weighted avg     0.9972    0.9974    0.9973     30647\n","\n","Confusion Matrix\n","[[30487    29]\n"," [   51    80]]\n","Accuracy: 0.9974\n","Precision: 0.7339\n","Recall: 0.6107\n","F1 Score: 0.6667\n","ROC: 0.9898\n","PR AUC: 0.7208\n","Confusion Matrix\n","30487 \t 29\n","51 \t 80\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"best_lstm_model.pth\")"],"metadata":{"id":"R8MxsUEGbrMh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Best GRU Model"],"metadata":{"id":"rDyxEWZfbs_T"}},{"cell_type":"code","source":["best_config = {\n","    'vocab_size': 0,\n","    'emb_size': 4,\n","    'hidden_size': 128,\n","    'lstm_layers': 2,\n","    'bi_lstm': True,\n","    'number_hidder_layers': 0,\n","    'dropout_prob': 0.1,\n","    'reshape': False,\n","    'batch_size': 64,\n","    'epochs': 30,\n","    'learning_rate': 0.00050\n","}"],"metadata":{"id":"MHz8jYhnbv7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = best_config\n","debug=False\n","model_type=\"GRU\"\n","\n","model = trainer(config, train_x, train_y, num_epochs=config[\"epochs\"], lr=config[\"learning_rate\"], batch_size = config[\"batch_size\"], model_type = model_type)\n","stats = eval_matrices(model, test_x, test_y)\n","stats.print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4B51bsRkbyWS","executionInfo":{"status":"ok","timestamp":1689157236568,"user_tz":-360,"elapsed":550651,"user":{"displayName":"Md. Toufikuzzaman","userId":"17924224467448662776"}},"outputId":"667c6cc1-bde8-46b1-92ee-dd3915b7803f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Step [200/3815], Loss: 0.3177\n","Epoch [1/30], Step [400/3815], Loss: 0.1547\n","Epoch [1/30], Step [600/3815], Loss: 0.1019\n","Epoch [1/30], Step [800/3815], Loss: 0.2982\n","Epoch [1/30], Step [1000/3815], Loss: 0.1347\n","Epoch [1/30], Step [1200/3815], Loss: 0.0453\n","Epoch [1/30], Step [1400/3815], Loss: 0.1297\n","Epoch [1/30], Step [1600/3815], Loss: 0.0604\n","Epoch [1/30], Step [1800/3815], Loss: 0.0694\n","Epoch [1/30], Step [2000/3815], Loss: 0.0249\n","Epoch [1/30], Step [2200/3815], Loss: 0.0134\n","Epoch [1/30], Step [2400/3815], Loss: 0.0717\n","Epoch [1/30], Step [2600/3815], Loss: 0.0448\n","Epoch [1/30], Step [2800/3815], Loss: 0.0221\n","Epoch [1/30], Step [3000/3815], Loss: 0.0228\n","Epoch [1/30], Step [3200/3815], Loss: 0.0460\n","Epoch [1/30], Step [3400/3815], Loss: 0.0824\n","Epoch [1/30], Step [3600/3815], Loss: 0.0059\n","Epoch [1/30], Step [3800/3815], Loss: 0.0364\n","Epoch [2/30], Step [200/3815], Loss: 0.0387\n","Epoch [2/30], Step [400/3815], Loss: 0.0726\n","Epoch [2/30], Step [600/3815], Loss: 0.0085\n","Epoch [2/30], Step [800/3815], Loss: 0.0075\n","Epoch [2/30], Step [1000/3815], Loss: 0.0171\n","Epoch [2/30], Step [1200/3815], Loss: 0.0182\n","Epoch [2/30], Step [1400/3815], Loss: 0.0099\n","Epoch [2/30], Step [1600/3815], Loss: 0.0641\n","Epoch [2/30], Step [1800/3815], Loss: 0.0037\n","Epoch [2/30], Step [2000/3815], Loss: 0.0057\n","Epoch [2/30], Step [2200/3815], Loss: 0.0477\n","Epoch [2/30], Step [2400/3815], Loss: 0.0807\n","Epoch [2/30], Step [2600/3815], Loss: 0.0922\n","Epoch [2/30], Step [2800/3815], Loss: 0.0051\n","Epoch [2/30], Step [3000/3815], Loss: 0.0114\n","Epoch [2/30], Step [3200/3815], Loss: 0.0193\n","Epoch [2/30], Step [3400/3815], Loss: 0.0063\n","Epoch [2/30], Step [3600/3815], Loss: 0.0009\n","Epoch [2/30], Step [3800/3815], Loss: 0.0462\n","Epoch [3/30], Step [200/3815], Loss: 0.1240\n","Epoch [3/30], Step [400/3815], Loss: 0.0092\n","Epoch [3/30], Step [600/3815], Loss: 0.0433\n","Epoch [3/30], Step [800/3815], Loss: 0.0256\n","Epoch [3/30], Step [1000/3815], Loss: 0.0064\n","Epoch [3/30], Step [1200/3815], Loss: 0.0031\n","Epoch [3/30], Step [1400/3815], Loss: 0.0276\n","Epoch [3/30], Step [1600/3815], Loss: 0.0049\n","Epoch [3/30], Step [1800/3815], Loss: 0.0177\n","Epoch [3/30], Step [2000/3815], Loss: 0.0056\n","Epoch [3/30], Step [2200/3815], Loss: 0.0033\n","Epoch [3/30], Step [2400/3815], Loss: 0.1683\n","Epoch [3/30], Step [2600/3815], Loss: 0.0054\n","Epoch [3/30], Step [2800/3815], Loss: 0.0179\n","Epoch [3/30], Step [3000/3815], Loss: 0.0026\n","Epoch [3/30], Step [3200/3815], Loss: 0.0065\n","Epoch [3/30], Step [3400/3815], Loss: 0.0007\n","Epoch [3/30], Step [3600/3815], Loss: 0.0073\n","Epoch [3/30], Step [3800/3815], Loss: 0.0016\n","Epoch [4/30], Step [200/3815], Loss: 0.0520\n","Epoch [4/30], Step [400/3815], Loss: 0.0055\n","Epoch [4/30], Step [600/3815], Loss: 0.0007\n","Epoch [4/30], Step [800/3815], Loss: 0.0122\n","Epoch [4/30], Step [1000/3815], Loss: 0.1014\n","Epoch [4/30], Step [1200/3815], Loss: 0.0008\n","Epoch [4/30], Step [1400/3815], Loss: 0.0004\n","Epoch [4/30], Step [1600/3815], Loss: 0.0020\n","Epoch [4/30], Step [1800/3815], Loss: 0.0614\n","Epoch [4/30], Step [2000/3815], Loss: 0.0018\n","Epoch [4/30], Step [2200/3815], Loss: 0.0041\n","Epoch [4/30], Step [2400/3815], Loss: 0.0029\n","Epoch [4/30], Step [2600/3815], Loss: 0.0006\n","Epoch [4/30], Step [2800/3815], Loss: 0.0798\n","Epoch [4/30], Step [3000/3815], Loss: 0.0003\n","Epoch [4/30], Step [3200/3815], Loss: 0.0020\n","Epoch [4/30], Step [3400/3815], Loss: 0.0041\n","Epoch [4/30], Step [3600/3815], Loss: 0.0176\n","Epoch [4/30], Step [3800/3815], Loss: 0.0007\n","Epoch [5/30], Step [200/3815], Loss: 0.0097\n","Epoch [5/30], Step [400/3815], Loss: 0.0021\n","Epoch [5/30], Step [600/3815], Loss: 0.0173\n","Epoch [5/30], Step [800/3815], Loss: 0.0005\n","Epoch [5/30], Step [1000/3815], Loss: 0.0010\n","Epoch [5/30], Step [1200/3815], Loss: 0.0158\n","Epoch [5/30], Step [1400/3815], Loss: 0.0157\n","Epoch [5/30], Step [1600/3815], Loss: 0.0008\n","Epoch [5/30], Step [1800/3815], Loss: 0.0028\n","Epoch [5/30], Step [2000/3815], Loss: 0.0012\n","Epoch [5/30], Step [2200/3815], Loss: 0.0078\n","Epoch [5/30], Step [2400/3815], Loss: 0.0069\n","Epoch [5/30], Step [2600/3815], Loss: 0.0008\n","Epoch [5/30], Step [2800/3815], Loss: 0.0005\n","Epoch [5/30], Step [3000/3815], Loss: 0.0023\n","Epoch [5/30], Step [3200/3815], Loss: 0.0003\n","Epoch [5/30], Step [3400/3815], Loss: 0.0005\n","Epoch [5/30], Step [3600/3815], Loss: 0.0012\n","Epoch [5/30], Step [3800/3815], Loss: 0.0016\n","Epoch [6/30], Step [200/3815], Loss: 0.0011\n","Epoch [6/30], Step [400/3815], Loss: 0.0003\n","Epoch [6/30], Step [600/3815], Loss: 0.0008\n","Epoch [6/30], Step [800/3815], Loss: 0.0001\n","Epoch [6/30], Step [1000/3815], Loss: 0.0004\n","Epoch [6/30], Step [1200/3815], Loss: 0.0102\n","Epoch [6/30], Step [1400/3815], Loss: 0.0012\n","Epoch [6/30], Step [1600/3815], Loss: 0.0009\n","Epoch [6/30], Step [1800/3815], Loss: 0.0011\n","Epoch [6/30], Step [2000/3815], Loss: 0.0009\n","Epoch [6/30], Step [2200/3815], Loss: 0.0024\n","Epoch [6/30], Step [2400/3815], Loss: 0.0045\n","Epoch [6/30], Step [2600/3815], Loss: 0.0444\n","Epoch [6/30], Step [2800/3815], Loss: 0.0008\n","Epoch [6/30], Step [3000/3815], Loss: 0.0015\n","Epoch [6/30], Step [3200/3815], Loss: 0.0007\n","Epoch [6/30], Step [3400/3815], Loss: 0.0342\n","Epoch [6/30], Step [3600/3815], Loss: 0.0006\n","Epoch [6/30], Step [3800/3815], Loss: 0.0009\n","Epoch [7/30], Step [200/3815], Loss: 0.0020\n","Epoch [7/30], Step [400/3815], Loss: 0.0004\n","Epoch [7/30], Step [600/3815], Loss: 0.0003\n","Epoch [7/30], Step [800/3815], Loss: 0.0008\n","Epoch [7/30], Step [1000/3815], Loss: 0.0011\n","Epoch [7/30], Step [1200/3815], Loss: 0.0010\n","Epoch [7/30], Step [1400/3815], Loss: 0.0340\n","Epoch [7/30], Step [1600/3815], Loss: 0.0291\n","Epoch [7/30], Step [1800/3815], Loss: 0.0008\n","Epoch [7/30], Step [2000/3815], Loss: 0.0003\n","Epoch [7/30], Step [2200/3815], Loss: 0.0005\n","Epoch [7/30], Step [2400/3815], Loss: 0.0019\n","Epoch [7/30], Step [2600/3815], Loss: 0.0006\n","Epoch [7/30], Step [2800/3815], Loss: 0.0072\n","Epoch [7/30], Step [3000/3815], Loss: 0.0011\n","Epoch [7/30], Step [3200/3815], Loss: 0.0018\n","Epoch [7/30], Step [3400/3815], Loss: 0.0020\n","Epoch [7/30], Step [3600/3815], Loss: 0.0005\n","Epoch [7/30], Step [3800/3815], Loss: 0.0003\n","Epoch [8/30], Step [200/3815], Loss: 0.0005\n","Epoch [8/30], Step [400/3815], Loss: 0.0002\n","Epoch [8/30], Step [600/3815], Loss: 0.0013\n","Epoch [8/30], Step [800/3815], Loss: 0.0040\n","Epoch [8/30], Step [1000/3815], Loss: 0.0204\n","Epoch [8/30], Step [1200/3815], Loss: 0.0002\n","Epoch [8/30], Step [1400/3815], Loss: 0.0008\n","Epoch [8/30], Step [1600/3815], Loss: 0.0011\n","Epoch [8/30], Step [1800/3815], Loss: 0.0006\n","Epoch [8/30], Step [2000/3815], Loss: 0.0004\n","Epoch [8/30], Step [2200/3815], Loss: 0.0002\n","Epoch [8/30], Step [2400/3815], Loss: 0.0184\n","Epoch [8/30], Step [2600/3815], Loss: 0.0007\n","Epoch [8/30], Step [2800/3815], Loss: 0.0009\n","Epoch [8/30], Step [3000/3815], Loss: 0.0007\n","Epoch [8/30], Step [3200/3815], Loss: 0.0003\n","Epoch [8/30], Step [3400/3815], Loss: 0.0002\n","Epoch [8/30], Step [3600/3815], Loss: 0.0015\n","Epoch [8/30], Step [3800/3815], Loss: 0.0003\n","Epoch [9/30], Step [200/3815], Loss: 0.0003\n","Epoch [9/30], Step [400/3815], Loss: 0.0065\n","Epoch [9/30], Step [600/3815], Loss: 0.0004\n","Epoch [9/30], Step [800/3815], Loss: 0.0004\n","Epoch [9/30], Step [1000/3815], Loss: 0.0001\n","Epoch [9/30], Step [1200/3815], Loss: 0.0002\n","Epoch [9/30], Step [1400/3815], Loss: 0.0001\n","Epoch [9/30], Step [1600/3815], Loss: 0.0001\n","Epoch [9/30], Step [1800/3815], Loss: 0.0001\n","Epoch [9/30], Step [2000/3815], Loss: 0.0048\n","Epoch [9/30], Step [2200/3815], Loss: 0.0001\n","Epoch [9/30], Step [2400/3815], Loss: 0.0002\n","Epoch [9/30], Step [2600/3815], Loss: 0.0478\n","Epoch [9/30], Step [2800/3815], Loss: 0.0033\n","Epoch [9/30], Step [3000/3815], Loss: 0.0009\n","Epoch [9/30], Step [3200/3815], Loss: 0.0001\n","Epoch [9/30], Step [3400/3815], Loss: 0.0004\n","Epoch [9/30], Step [3600/3815], Loss: 0.0003\n","Epoch [9/30], Step [3800/3815], Loss: 0.0002\n","Epoch [10/30], Step [200/3815], Loss: 0.0002\n","Epoch [10/30], Step [400/3815], Loss: 0.0047\n","Epoch [10/30], Step [600/3815], Loss: 0.0017\n","Epoch [10/30], Step [800/3815], Loss: 0.0014\n","Epoch [10/30], Step [1000/3815], Loss: 0.0008\n","Epoch [10/30], Step [1200/3815], Loss: 0.0012\n","Epoch [10/30], Step [1400/3815], Loss: 0.0006\n","Epoch [10/30], Step [1600/3815], Loss: 0.0067\n","Epoch [10/30], Step [1800/3815], Loss: 0.0002\n","Epoch [10/30], Step [2000/3815], Loss: 0.0002\n","Epoch [10/30], Step [2200/3815], Loss: 0.0008\n","Epoch [10/30], Step [2400/3815], Loss: 0.0008\n","Epoch [10/30], Step [2600/3815], Loss: 0.0006\n","Epoch [10/30], Step [2800/3815], Loss: 0.0150\n","Epoch [10/30], Step [3000/3815], Loss: 0.0001\n","Epoch [10/30], Step [3200/3815], Loss: 0.0025\n","Epoch [10/30], Step [3400/3815], Loss: 0.0003\n","Epoch [10/30], Step [3600/3815], Loss: 0.0007\n","Epoch [10/30], Step [3800/3815], Loss: 0.0004\n","Epoch [11/30], Step [200/3815], Loss: 0.1004\n","Epoch [11/30], Step [400/3815], Loss: 0.0002\n","Epoch [11/30], Step [600/3815], Loss: 0.0001\n","Epoch [11/30], Step [800/3815], Loss: 0.0003\n","Epoch [11/30], Step [1000/3815], Loss: 0.0003\n","Epoch [11/30], Step [1200/3815], Loss: 0.0004\n","Epoch [11/30], Step [1400/3815], Loss: 0.0004\n","Epoch [11/30], Step [1600/3815], Loss: 0.0002\n","Epoch [11/30], Step [1800/3815], Loss: 0.0002\n","Epoch [11/30], Step [2000/3815], Loss: 0.0002\n","Epoch [11/30], Step [2200/3815], Loss: 0.0002\n","Epoch [11/30], Step [2400/3815], Loss: 0.0002\n","Epoch [11/30], Step [2600/3815], Loss: 0.0001\n","Epoch [11/30], Step [2800/3815], Loss: 0.0005\n","Epoch [11/30], Step [3000/3815], Loss: 0.0003\n","Epoch [11/30], Step [3200/3815], Loss: 0.0002\n","Epoch [11/30], Step [3400/3815], Loss: 0.0001\n","Epoch [11/30], Step [3600/3815], Loss: 0.0020\n","Epoch [11/30], Step [3800/3815], Loss: 0.0001\n","Epoch [12/30], Step [200/3815], Loss: 0.0001\n","Epoch [12/30], Step [400/3815], Loss: 0.0001\n","Epoch [12/30], Step [600/3815], Loss: 0.0010\n","Epoch [12/30], Step [800/3815], Loss: 0.0013\n","Epoch [12/30], Step [1000/3815], Loss: 0.0002\n","Epoch [12/30], Step [1200/3815], Loss: 0.0001\n","Epoch [12/30], Step [1400/3815], Loss: 0.0005\n","Epoch [12/30], Step [1600/3815], Loss: 0.0000\n","Epoch [12/30], Step [1800/3815], Loss: 0.0013\n","Epoch [12/30], Step [2000/3815], Loss: 0.0001\n","Epoch [12/30], Step [2200/3815], Loss: 0.0001\n","Epoch [12/30], Step [2400/3815], Loss: 0.0007\n","Epoch [12/30], Step [2600/3815], Loss: 0.0001\n","Epoch [12/30], Step [2800/3815], Loss: 0.0001\n","Epoch [12/30], Step [3000/3815], Loss: 0.0001\n","Epoch [12/30], Step [3200/3815], Loss: 0.0001\n","Epoch [12/30], Step [3400/3815], Loss: 0.0044\n","Epoch [12/30], Step [3600/3815], Loss: 0.0002\n","Epoch [12/30], Step [3800/3815], Loss: 0.0007\n","Epoch [13/30], Step [200/3815], Loss: 0.0001\n","Epoch [13/30], Step [400/3815], Loss: 0.0005\n","Epoch [13/30], Step [600/3815], Loss: 0.0001\n","Epoch [13/30], Step [800/3815], Loss: 0.0001\n","Epoch [13/30], Step [1000/3815], Loss: 0.0002\n","Epoch [13/30], Step [1200/3815], Loss: 0.0004\n","Epoch [13/30], Step [1400/3815], Loss: 0.0172\n","Epoch [13/30], Step [1600/3815], Loss: 0.0002\n","Epoch [13/30], Step [1800/3815], Loss: 0.0001\n","Epoch [13/30], Step [2000/3815], Loss: 0.0001\n","Epoch [13/30], Step [2200/3815], Loss: 0.0001\n","Epoch [13/30], Step [2400/3815], Loss: 0.0000\n","Epoch [13/30], Step [2600/3815], Loss: 0.0001\n","Epoch [13/30], Step [2800/3815], Loss: 0.0024\n","Epoch [13/30], Step [3000/3815], Loss: 0.0009\n","Epoch [13/30], Step [3200/3815], Loss: 0.0001\n","Epoch [13/30], Step [3400/3815], Loss: 0.0003\n","Epoch [13/30], Step [3600/3815], Loss: 0.0002\n","Epoch [13/30], Step [3800/3815], Loss: 0.0002\n","Epoch [14/30], Step [200/3815], Loss: 0.0001\n","Epoch [14/30], Step [400/3815], Loss: 0.0001\n","Epoch [14/30], Step [600/3815], Loss: 0.0000\n","Epoch [14/30], Step [800/3815], Loss: 0.0010\n","Epoch [14/30], Step [1000/3815], Loss: 0.0000\n","Epoch [14/30], Step [1200/3815], Loss: 0.0001\n","Epoch [14/30], Step [1400/3815], Loss: 0.0002\n","Epoch [14/30], Step [1600/3815], Loss: 0.0007\n","Epoch [14/30], Step [1800/3815], Loss: 0.0002\n","Epoch [14/30], Step [2000/3815], Loss: 0.0001\n","Epoch [14/30], Step [2200/3815], Loss: 0.0000\n","Epoch [14/30], Step [2400/3815], Loss: 0.0002\n","Epoch [14/30], Step [2600/3815], Loss: 0.0004\n","Epoch [14/30], Step [2800/3815], Loss: 0.0001\n","Epoch [14/30], Step [3000/3815], Loss: 0.0002\n","Epoch [14/30], Step [3200/3815], Loss: 0.0001\n","Epoch [14/30], Step [3400/3815], Loss: 0.0001\n","Epoch [14/30], Step [3600/3815], Loss: 0.0001\n","Epoch [14/30], Step [3800/3815], Loss: 0.0001\n","Epoch [15/30], Step [200/3815], Loss: 0.0002\n","Epoch [15/30], Step [400/3815], Loss: 0.0001\n","Epoch [15/30], Step [600/3815], Loss: 0.0006\n","Epoch [15/30], Step [800/3815], Loss: 0.0000\n","Epoch [15/30], Step [1000/3815], Loss: 0.0004\n","Epoch [15/30], Step [1200/3815], Loss: 0.0000\n","Epoch [15/30], Step [1400/3815], Loss: 0.0002\n","Epoch [15/30], Step [1600/3815], Loss: 0.0002\n","Epoch [15/30], Step [1800/3815], Loss: 0.0001\n","Epoch [15/30], Step [2000/3815], Loss: 0.0002\n","Epoch [15/30], Step [2200/3815], Loss: 0.0011\n","Epoch [15/30], Step [2400/3815], Loss: 0.0001\n","Epoch [15/30], Step [2600/3815], Loss: 0.0001\n","Epoch [15/30], Step [2800/3815], Loss: 0.0001\n","Epoch [15/30], Step [3000/3815], Loss: 0.0002\n","Epoch [15/30], Step [3200/3815], Loss: 0.0001\n","Epoch [15/30], Step [3400/3815], Loss: 0.0000\n","Epoch [15/30], Step [3600/3815], Loss: 0.0004\n","Epoch [15/30], Step [3800/3815], Loss: 0.0001\n","Epoch [16/30], Step [200/3815], Loss: 0.0020\n","Epoch [16/30], Step [400/3815], Loss: 0.0002\n","Epoch [16/30], Step [600/3815], Loss: 0.0206\n","Epoch [16/30], Step [800/3815], Loss: 0.0001\n","Epoch [16/30], Step [1000/3815], Loss: 0.0002\n","Epoch [16/30], Step [1200/3815], Loss: 0.0000\n","Epoch [16/30], Step [1400/3815], Loss: 0.0002\n","Epoch [16/30], Step [1600/3815], Loss: 0.0001\n","Epoch [16/30], Step [1800/3815], Loss: 0.0000\n","Epoch [16/30], Step [2000/3815], Loss: 0.0002\n","Epoch [16/30], Step [2200/3815], Loss: 0.0003\n","Epoch [16/30], Step [2400/3815], Loss: 0.0003\n","Epoch [16/30], Step [2600/3815], Loss: 0.0000\n","Epoch [16/30], Step [2800/3815], Loss: 0.0001\n","Epoch [16/30], Step [3000/3815], Loss: 0.0001\n","Epoch [16/30], Step [3200/3815], Loss: 0.0005\n","Epoch [16/30], Step [3400/3815], Loss: 0.0002\n","Epoch [16/30], Step [3600/3815], Loss: 0.0001\n","Epoch [16/30], Step [3800/3815], Loss: 0.0001\n","Epoch [17/30], Step [200/3815], Loss: 0.0002\n","Epoch [17/30], Step [400/3815], Loss: 0.0002\n","Epoch [17/30], Step [600/3815], Loss: 0.0000\n","Epoch [17/30], Step [800/3815], Loss: 0.0000\n","Epoch [17/30], Step [1000/3815], Loss: 0.0000\n","Epoch [17/30], Step [1200/3815], Loss: 0.0055\n","Epoch [17/30], Step [1400/3815], Loss: 0.0001\n","Epoch [17/30], Step [1600/3815], Loss: 0.0002\n","Epoch [17/30], Step [1800/3815], Loss: 0.0001\n","Epoch [17/30], Step [2000/3815], Loss: 0.0850\n","Epoch [17/30], Step [2200/3815], Loss: 0.0006\n","Epoch [17/30], Step [2400/3815], Loss: 0.0001\n","Epoch [17/30], Step [2600/3815], Loss: 0.0022\n","Epoch [17/30], Step [2800/3815], Loss: 0.0000\n","Epoch [17/30], Step [3000/3815], Loss: 0.1956\n","Epoch [17/30], Step [3200/3815], Loss: 0.0003\n","Epoch [17/30], Step [3400/3815], Loss: 0.0004\n","Epoch [17/30], Step [3600/3815], Loss: 0.0008\n","Epoch [17/30], Step [3800/3815], Loss: 0.0001\n","Epoch [18/30], Step [200/3815], Loss: 0.0001\n","Epoch [18/30], Step [400/3815], Loss: 0.0001\n","Epoch [18/30], Step [600/3815], Loss: 0.0001\n","Epoch [18/30], Step [800/3815], Loss: 0.0001\n","Epoch [18/30], Step [1000/3815], Loss: 0.0001\n","Epoch [18/30], Step [1200/3815], Loss: 0.0002\n","Epoch [18/30], Step [1400/3815], Loss: 0.0006\n","Epoch [18/30], Step [1600/3815], Loss: 0.0000\n","Epoch [18/30], Step [1800/3815], Loss: 0.0004\n","Epoch [18/30], Step [2000/3815], Loss: 0.0001\n","Epoch [18/30], Step [2200/3815], Loss: 0.0001\n","Epoch [18/30], Step [2400/3815], Loss: 0.0003\n","Epoch [18/30], Step [2600/3815], Loss: 0.0006\n","Epoch [18/30], Step [2800/3815], Loss: 0.0003\n","Epoch [18/30], Step [3000/3815], Loss: 0.0000\n","Epoch [18/30], Step [3200/3815], Loss: 0.0002\n","Epoch [18/30], Step [3400/3815], Loss: 0.0000\n","Epoch [18/30], Step [3600/3815], Loss: 0.0000\n","Epoch [18/30], Step [3800/3815], Loss: 0.0002\n","Epoch [19/30], Step [200/3815], Loss: 0.0000\n","Epoch [19/30], Step [400/3815], Loss: 0.0002\n","Epoch [19/30], Step [600/3815], Loss: 0.0001\n","Epoch [19/30], Step [800/3815], Loss: 0.0028\n","Epoch [19/30], Step [1000/3815], Loss: 0.0000\n","Epoch [19/30], Step [1200/3815], Loss: 0.0000\n","Epoch [19/30], Step [1400/3815], Loss: 0.0000\n","Epoch [19/30], Step [1600/3815], Loss: 0.0000\n","Epoch [19/30], Step [1800/3815], Loss: 0.0002\n","Epoch [19/30], Step [2000/3815], Loss: 0.0002\n","Epoch [19/30], Step [2200/3815], Loss: 0.1171\n","Epoch [19/30], Step [2400/3815], Loss: 0.0000\n","Epoch [19/30], Step [2600/3815], Loss: 0.0008\n","Epoch [19/30], Step [2800/3815], Loss: 0.0000\n","Epoch [19/30], Step [3000/3815], Loss: 0.0002\n","Epoch [19/30], Step [3200/3815], Loss: 0.0001\n","Epoch [19/30], Step [3400/3815], Loss: 0.0001\n","Epoch [19/30], Step [3600/3815], Loss: 0.0002\n","Epoch [19/30], Step [3800/3815], Loss: 0.0001\n","Epoch [20/30], Step [200/3815], Loss: 0.0001\n","Epoch [20/30], Step [400/3815], Loss: 0.0002\n","Epoch [20/30], Step [600/3815], Loss: 0.0001\n","Epoch [20/30], Step [800/3815], Loss: 0.0001\n","Epoch [20/30], Step [1000/3815], Loss: 0.0000\n","Epoch [20/30], Step [1200/3815], Loss: 0.0000\n","Epoch [20/30], Step [1400/3815], Loss: 0.0000\n","Epoch [20/30], Step [1600/3815], Loss: 0.0001\n","Epoch [20/30], Step [1800/3815], Loss: 0.0002\n","Epoch [20/30], Step [2000/3815], Loss: 0.0001\n","Epoch [20/30], Step [2200/3815], Loss: 0.0000\n","Epoch [20/30], Step [2400/3815], Loss: 0.0000\n","Epoch [20/30], Step [2600/3815], Loss: 0.0000\n","Epoch [20/30], Step [2800/3815], Loss: 0.0003\n","Epoch [20/30], Step [3000/3815], Loss: 0.0000\n","Epoch [20/30], Step [3200/3815], Loss: 0.0002\n","Epoch [20/30], Step [3400/3815], Loss: 0.0001\n","Epoch [20/30], Step [3600/3815], Loss: 0.0421\n","Epoch [20/30], Step [3800/3815], Loss: 0.0001\n","Epoch [21/30], Step [200/3815], Loss: 0.0015\n","Epoch [21/30], Step [400/3815], Loss: 0.0002\n","Epoch [21/30], Step [600/3815], Loss: 0.0001\n","Epoch [21/30], Step [800/3815], Loss: 0.0002\n","Epoch [21/30], Step [1000/3815], Loss: 0.0003\n","Epoch [21/30], Step [1200/3815], Loss: 0.0001\n","Epoch [21/30], Step [1400/3815], Loss: 0.0000\n","Epoch [21/30], Step [1600/3815], Loss: 0.0006\n","Epoch [21/30], Step [1800/3815], Loss: 0.0001\n","Epoch [21/30], Step [2000/3815], Loss: 0.0000\n","Epoch [21/30], Step [2200/3815], Loss: 0.0000\n","Epoch [21/30], Step [2400/3815], Loss: 0.0000\n","Epoch [21/30], Step [2600/3815], Loss: 0.0000\n","Epoch [21/30], Step [2800/3815], Loss: 0.0001\n","Epoch [21/30], Step [3000/3815], Loss: 0.0001\n","Epoch [21/30], Step [3200/3815], Loss: 0.0001\n","Epoch [21/30], Step [3400/3815], Loss: 0.0000\n","Epoch [21/30], Step [3600/3815], Loss: 0.0001\n","Epoch [21/30], Step [3800/3815], Loss: 0.0000\n","Epoch [22/30], Step [200/3815], Loss: 0.0001\n","Epoch [22/30], Step [400/3815], Loss: 0.0000\n","Epoch [22/30], Step [600/3815], Loss: 0.0001\n","Epoch [22/30], Step [800/3815], Loss: 0.0001\n","Epoch [22/30], Step [1000/3815], Loss: 0.0000\n","Epoch [22/30], Step [1200/3815], Loss: 0.0005\n","Epoch [22/30], Step [1400/3815], Loss: 0.0001\n","Epoch [22/30], Step [1600/3815], Loss: 0.0000\n","Epoch [22/30], Step [1800/3815], Loss: 0.0021\n","Epoch [22/30], Step [2000/3815], Loss: 0.0002\n","Epoch [22/30], Step [2200/3815], Loss: 0.0000\n","Epoch [22/30], Step [2400/3815], Loss: 0.0000\n","Epoch [22/30], Step [2600/3815], Loss: 0.0001\n","Epoch [22/30], Step [2800/3815], Loss: 0.0000\n","Epoch [22/30], Step [3000/3815], Loss: 0.0000\n","Epoch [22/30], Step [3200/3815], Loss: 0.0000\n","Epoch [22/30], Step [3400/3815], Loss: 0.0001\n","Epoch [22/30], Step [3600/3815], Loss: 0.0001\n","Epoch [22/30], Step [3800/3815], Loss: 0.0001\n","Epoch [23/30], Step [200/3815], Loss: 0.0001\n","Epoch [23/30], Step [400/3815], Loss: 0.0001\n","Epoch [23/30], Step [600/3815], Loss: 0.0003\n","Epoch [23/30], Step [800/3815], Loss: 0.0000\n","Epoch [23/30], Step [1000/3815], Loss: 0.0000\n","Epoch [23/30], Step [1200/3815], Loss: 0.0001\n","Epoch [23/30], Step [1400/3815], Loss: 0.0000\n","Epoch [23/30], Step [1600/3815], Loss: 0.0000\n","Epoch [23/30], Step [1800/3815], Loss: 0.0000\n","Epoch [23/30], Step [2000/3815], Loss: 0.0001\n","Epoch [23/30], Step [2200/3815], Loss: 0.0000\n","Epoch [23/30], Step [2400/3815], Loss: 0.0000\n","Epoch [23/30], Step [2600/3815], Loss: 0.0000\n","Epoch [23/30], Step [2800/3815], Loss: 0.0001\n","Epoch [23/30], Step [3000/3815], Loss: 0.0000\n","Epoch [23/30], Step [3200/3815], Loss: 0.0000\n","Epoch [23/30], Step [3400/3815], Loss: 0.0059\n","Epoch [23/30], Step [3600/3815], Loss: 0.0000\n","Epoch [23/30], Step [3800/3815], Loss: 0.0000\n","Epoch [24/30], Step [200/3815], Loss: 0.0000\n","Epoch [24/30], Step [400/3815], Loss: 0.0000\n","Epoch [24/30], Step [600/3815], Loss: 0.0005\n","Epoch [24/30], Step [800/3815], Loss: 0.0000\n","Epoch [24/30], Step [1000/3815], Loss: 0.0000\n","Epoch [24/30], Step [1200/3815], Loss: 0.0000\n","Epoch [24/30], Step [1400/3815], Loss: 0.0000\n","Epoch [24/30], Step [1600/3815], Loss: 0.0000\n","Epoch [24/30], Step [1800/3815], Loss: 0.0000\n","Epoch [24/30], Step [2000/3815], Loss: 0.0000\n","Epoch [24/30], Step [2200/3815], Loss: 0.0000\n","Epoch [24/30], Step [2400/3815], Loss: 0.0001\n","Epoch [24/30], Step [2600/3815], Loss: 0.0006\n","Epoch [24/30], Step [2800/3815], Loss: 0.0001\n","Epoch [24/30], Step [3000/3815], Loss: 0.0001\n","Epoch [24/30], Step [3200/3815], Loss: 0.0002\n","Epoch [24/30], Step [3400/3815], Loss: 0.0000\n","Epoch [24/30], Step [3600/3815], Loss: 0.0000\n","Epoch [24/30], Step [3800/3815], Loss: 0.0001\n","Epoch [25/30], Step [200/3815], Loss: 0.0000\n","Epoch [25/30], Step [400/3815], Loss: 0.0002\n","Epoch [25/30], Step [600/3815], Loss: 0.0000\n","Epoch [25/30], Step [800/3815], Loss: 0.0000\n","Epoch [25/30], Step [1000/3815], Loss: 0.0017\n","Epoch [25/30], Step [1200/3815], Loss: 0.0001\n","Epoch [25/30], Step [1400/3815], Loss: 0.0001\n","Epoch [25/30], Step [1600/3815], Loss: 0.0000\n","Epoch [25/30], Step [1800/3815], Loss: 0.0000\n","Epoch [25/30], Step [2000/3815], Loss: 0.0000\n","Epoch [25/30], Step [2200/3815], Loss: 0.0001\n","Epoch [25/30], Step [2400/3815], Loss: 0.0001\n","Epoch [25/30], Step [2600/3815], Loss: 0.0003\n","Epoch [25/30], Step [2800/3815], Loss: 0.0000\n","Epoch [25/30], Step [3000/3815], Loss: 0.0001\n","Epoch [25/30], Step [3200/3815], Loss: 0.0000\n","Epoch [25/30], Step [3400/3815], Loss: 0.0000\n","Epoch [25/30], Step [3600/3815], Loss: 0.0000\n","Epoch [25/30], Step [3800/3815], Loss: 0.0000\n","Epoch [26/30], Step [200/3815], Loss: 0.0000\n","Epoch [26/30], Step [400/3815], Loss: 0.0929\n","Epoch [26/30], Step [600/3815], Loss: 0.0001\n","Epoch [26/30], Step [800/3815], Loss: 0.0433\n","Epoch [26/30], Step [1000/3815], Loss: 0.0001\n","Epoch [26/30], Step [1200/3815], Loss: 0.0001\n","Epoch [26/30], Step [1400/3815], Loss: 0.0000\n","Epoch [26/30], Step [1600/3815], Loss: 0.0001\n","Epoch [26/30], Step [1800/3815], Loss: 0.0000\n","Epoch [26/30], Step [2000/3815], Loss: 0.0041\n","Epoch [26/30], Step [2200/3815], Loss: 0.0000\n","Epoch [26/30], Step [2400/3815], Loss: 0.0000\n","Epoch [26/30], Step [2600/3815], Loss: 0.0000\n","Epoch [26/30], Step [2800/3815], Loss: 0.0001\n","Epoch [26/30], Step [3000/3815], Loss: 0.0000\n","Epoch [26/30], Step [3200/3815], Loss: 0.0000\n","Epoch [26/30], Step [3400/3815], Loss: 0.0002\n","Epoch [26/30], Step [3600/3815], Loss: 0.0001\n","Epoch [26/30], Step [3800/3815], Loss: 0.0006\n","Epoch [27/30], Step [200/3815], Loss: 0.0000\n","Epoch [27/30], Step [400/3815], Loss: 0.0001\n","Epoch [27/30], Step [600/3815], Loss: 0.0001\n","Epoch [27/30], Step [800/3815], Loss: 0.0000\n","Epoch [27/30], Step [1000/3815], Loss: 0.0000\n","Epoch [27/30], Step [1200/3815], Loss: 0.0001\n","Epoch [27/30], Step [1400/3815], Loss: 0.0001\n","Epoch [27/30], Step [1600/3815], Loss: 0.0000\n","Epoch [27/30], Step [1800/3815], Loss: 0.0001\n","Epoch [27/30], Step [2000/3815], Loss: 0.0000\n","Epoch [27/30], Step [2200/3815], Loss: 0.0000\n","Epoch [27/30], Step [2400/3815], Loss: 0.0000\n","Epoch [27/30], Step [2600/3815], Loss: 0.0000\n","Epoch [27/30], Step [2800/3815], Loss: 0.0001\n","Epoch [27/30], Step [3000/3815], Loss: 0.0000\n","Epoch [27/30], Step [3200/3815], Loss: 0.0001\n","Epoch [27/30], Step [3400/3815], Loss: 0.0000\n","Epoch [27/30], Step [3600/3815], Loss: 0.0000\n","Epoch [27/30], Step [3800/3815], Loss: 0.0000\n","Epoch [28/30], Step [200/3815], Loss: 0.0000\n","Epoch [28/30], Step [400/3815], Loss: 0.0001\n","Epoch [28/30], Step [600/3815], Loss: 0.0000\n","Epoch [28/30], Step [800/3815], Loss: 0.0000\n","Epoch [28/30], Step [1000/3815], Loss: 0.0000\n","Epoch [28/30], Step [1200/3815], Loss: 0.0000\n","Epoch [28/30], Step [1400/3815], Loss: 0.0000\n","Epoch [28/30], Step [1600/3815], Loss: 0.0002\n","Epoch [28/30], Step [1800/3815], Loss: 0.0000\n","Epoch [28/30], Step [2000/3815], Loss: 0.0000\n","Epoch [28/30], Step [2200/3815], Loss: 0.0001\n","Epoch [28/30], Step [2400/3815], Loss: 0.0000\n","Epoch [28/30], Step [2600/3815], Loss: 0.0000\n","Epoch [28/30], Step [2800/3815], Loss: 0.0001\n","Epoch [28/30], Step [3000/3815], Loss: 0.0002\n","Epoch [28/30], Step [3200/3815], Loss: 0.0000\n","Epoch [28/30], Step [3400/3815], Loss: 0.0000\n","Epoch [28/30], Step [3600/3815], Loss: 0.0000\n","Epoch [28/30], Step [3800/3815], Loss: 0.0000\n","Epoch [29/30], Step [200/3815], Loss: 0.0001\n","Epoch [29/30], Step [400/3815], Loss: 0.0000\n","Epoch [29/30], Step [600/3815], Loss: 0.0000\n","Epoch [29/30], Step [800/3815], Loss: 0.0000\n","Epoch [29/30], Step [1000/3815], Loss: 0.0004\n","Epoch [29/30], Step [1200/3815], Loss: 0.0001\n","Epoch [29/30], Step [1400/3815], Loss: 0.0001\n","Epoch [29/30], Step [1600/3815], Loss: 0.0001\n","Epoch [29/30], Step [1800/3815], Loss: 0.0002\n","Epoch [29/30], Step [2000/3815], Loss: 0.0004\n","Epoch [29/30], Step [2200/3815], Loss: 0.0000\n","Epoch [29/30], Step [2400/3815], Loss: 0.0000\n","Epoch [29/30], Step [2600/3815], Loss: 0.0000\n","Epoch [29/30], Step [2800/3815], Loss: 0.0000\n","Epoch [29/30], Step [3000/3815], Loss: 0.0000\n","Epoch [29/30], Step [3200/3815], Loss: 0.0000\n","Epoch [29/30], Step [3400/3815], Loss: 0.0000\n","Epoch [29/30], Step [3600/3815], Loss: 0.0001\n","Epoch [29/30], Step [3800/3815], Loss: 0.0004\n","Epoch [30/30], Step [200/3815], Loss: 0.0001\n","Epoch [30/30], Step [400/3815], Loss: 0.0004\n","Epoch [30/30], Step [600/3815], Loss: 0.0000\n","Epoch [30/30], Step [800/3815], Loss: 0.0000\n","Epoch [30/30], Step [1000/3815], Loss: 0.0000\n","Epoch [30/30], Step [1200/3815], Loss: 0.0000\n","Epoch [30/30], Step [1400/3815], Loss: 0.0000\n","Epoch [30/30], Step [1600/3815], Loss: 0.0000\n","Epoch [30/30], Step [1800/3815], Loss: 0.0001\n","Epoch [30/30], Step [2000/3815], Loss: 0.0000\n","Epoch [30/30], Step [2200/3815], Loss: 0.0000\n","Epoch [30/30], Step [2400/3815], Loss: 0.0000\n","Epoch [30/30], Step [2600/3815], Loss: 0.0000\n","Epoch [30/30], Step [2800/3815], Loss: 0.0001\n","Epoch [30/30], Step [3000/3815], Loss: 0.0000\n","Epoch [30/30], Step [3200/3815], Loss: 0.0004\n","Epoch [30/30], Step [3400/3815], Loss: 0.0002\n","Epoch [30/30], Step [3600/3815], Loss: 0.0000\n","Epoch [30/30], Step [3800/3815], Loss: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-19-f2e417b33a9e>:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  predictions = [torch.nn.functional.softmax(r) for r in results]\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9972\n","Precision: 0.6891\n","Recall: 0.6260\n","F1 Score: 0.6560\n","ROC: 0.9907548186460062\n","PR AUC: 0.6859\n","              precision    recall  f1-score   support\n","\n","           0     0.9984    0.9988    0.9986     30516\n","           1     0.6891    0.6260    0.6560       131\n","\n","    accuracy                         0.9972     30647\n","   macro avg     0.8437    0.8124    0.8273     30647\n","weighted avg     0.9971    0.9972    0.9971     30647\n","\n","Confusion Matrix\n","[[30479    37]\n"," [   49    82]]\n","Accuracy: 0.9972\n","Precision: 0.6891\n","Recall: 0.6260\n","F1 Score: 0.6560\n","ROC: 0.9908\n","PR AUC: 0.6859\n","Confusion Matrix\n","30479 \t 37\n","49 \t 82\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"best_gru_model.pth\")"],"metadata":{"id":"Gcnf3Ngpbz6q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Best Model Interpretation"],"metadata":{"id":"9t4llx5Ypedp"}},{"cell_type":"code","source":[],"metadata":{"id":"_Gsk3Irfpl2Q"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["GkopnuflmA-K","VUcwPlCfFaGc"],"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}