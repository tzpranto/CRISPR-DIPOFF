{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# General"
      ],
      "metadata": {
        "id": "Yl-vOk86eptS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHXE4HPuSFEr",
        "outputId": "846855aa-1b85-4e30-9c3f-2f7579518adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8l67NdmsPR9",
        "outputId": "b4ef0109-c8f5-423d-fe72-ddb726ac00f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V98s2J5sAKQ",
        "outputId": "3bdaf2ef-681a-4068-db04-01e0b33e7bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01If0rTMSz9j"
      },
      "outputs": [],
      "source": [
        "root_path = '/gdrive/My Drive/Colab Data/CRISPR Off Target/'\n",
        "data_dir = root_path + '2018_DeepCRISPR/'\n",
        "data_path = data_dir + 'all_off_target.csv'\n",
        "resource_dir = data_dir + \"Resources/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5--9M4Oq9czH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "seed = 12345\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNep67GoNdka",
        "outputId": "12e82a03-2280-4f23-bac8-62d61f94e9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IxgtNXVXeDt",
        "outputId": "0ded3fb5-c0ed-4e9c-f086-96dc66d5d3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/.shortcut-targets-by-id/1-CPBoDSc88CelqHVwU-GHHjHASJq0kkO/CRISPR Off Target/2018_DeepCRISPR/Resources\n"
          ]
        }
      ],
      "source": [
        "cd $resource_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkopnuflmA-K"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NeTlGGOoa8j"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_Model_Generic(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(LSTM_Model_Generic,self).__init__()\n",
        "        # emb_size=256, hidden_size=128, hidden_layers=3, output=2\n",
        "\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.emb_size = config[\"emb_size\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.lstm_layers = config[\"lstm_layers\"]\n",
        "        self.bi_lstm = config[\"bi_lstm\"]\n",
        "        self.reshape = config[\"reshape\"]\n",
        "\n",
        "        self.number_hidden_layers = config[\"number_hidder_layers\"]\n",
        "        self.dropout_prob = config[\"dropout_prob\"]\n",
        "        self.hidden_layers = []\n",
        "\n",
        "        self.hidden_shape = self.hidden_size*2 if self.bi_lstm else self.hidden_size\n",
        "\n",
        "        self.embedding = None\n",
        "        if self.vocab_size > 0:\n",
        "            self.embedding = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0)\n",
        "\n",
        "        self.lstm= nn.LSTM(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n",
        "                            batch_first=True, bidirectional=self.bi_lstm)\n",
        "#         self.lstm= nn.GRU(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n",
        "#                             batch_first=True, bidirectional=self.bi_lstm)\n",
        "\n",
        "        start_size = self.hidden_shape\n",
        "\n",
        "        self.relu = nn.ReLU\n",
        "        # self.dropout = nn.Dropout(self.dropout_prob)\n",
        "\n",
        "        for i in range(self.number_hidden_layers):\n",
        "            self.hidden_layers.append(nn.Sequential(\n",
        "                nn.Linear(start_size, start_size // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(self.dropout_prob)))\n",
        "\n",
        "            start_size = start_size // 2\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList(self.hidden_layers)\n",
        "        self.output = nn.Linear(start_size,2)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        dir = 2 if self.bi_lstm else 1\n",
        "        h = torch.zeros((self.lstm_layers*dir, x.size(0), self.hidden_size)).to(device)\n",
        "        c = torch.zeros((self.lstm_layers*dir, x.size(0), self.hidden_size)).to(device)\n",
        "\n",
        "        if self.embedding is not None:\n",
        "            x = x.type(torch.LongTensor).to(device)\n",
        "            x = self.embedding(x)\n",
        "        elif self.reshape:\n",
        "            x = x.view(x.shape[0],x.shape[1],1)\n",
        "\n",
        "        x, (hidden, cell) = self.lstm(x, (h,c))\n",
        "\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # print(x.shape)\n",
        "        for i, layer in enumerate(self.hidden_layers):\n",
        "            x = layer(x)\n",
        "            # print(x.shape)\n",
        "        x = self.output(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8fWAzghsAKU"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN_Model_Generic(nn.Module):\n",
        "    def __init__(self, config, model_type):\n",
        "        super(RNN_Model_Generic,self).__init__()\n",
        "        # emb_size=256, hidden_size=128, hidden_layers=3, output=2\n",
        "\n",
        "        self.model_type = model_type\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.emb_size = config[\"emb_size\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.lstm_layers = config[\"lstm_layers\"]\n",
        "        self.bi_lstm = config[\"bi_lstm\"]\n",
        "        self.reshape = config[\"reshape\"]\n",
        "\n",
        "        self.number_hidden_layers = config[\"number_hidder_layers\"]\n",
        "        self.dropout_prob = config[\"dropout_prob\"]\n",
        "        self.hidden_layers = []\n",
        "\n",
        "        self.hidden_shape = self.hidden_size*2 if self.bi_lstm else self.hidden_size\n",
        "\n",
        "        self.embedding = None\n",
        "        if self.vocab_size > 0:\n",
        "            self.embedding = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0)\n",
        "\n",
        "\n",
        "        if model_type == \"LSTM\":\n",
        "            self.lstm = nn.LSTM(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n",
        "                            batch_first=True, bidirectional=self.bi_lstm)\n",
        "        elif model_type == \"GRU\":\n",
        "            self.lstm= nn.GRU(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n",
        "                           batch_first=True, bidirectional=self.bi_lstm)\n",
        "        else:\n",
        "            self.lstm= nn.RNN(self.emb_size, self.hidden_size, num_layers=self.lstm_layers,\n",
        "                           batch_first=True, bidirectional=self.bi_lstm)\n",
        "\n",
        "        start_size = self.hidden_shape\n",
        "\n",
        "        self.relu = nn.ReLU\n",
        "        # self.dropout = nn.Dropout(self.dropout_prob)\n",
        "\n",
        "        for i in range(self.number_hidden_layers):\n",
        "            self.hidden_layers.append(nn.Sequential(\n",
        "                nn.Linear(start_size, start_size // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(self.dropout_prob)))\n",
        "\n",
        "            start_size = start_size // 2\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList(self.hidden_layers)\n",
        "        self.output = nn.Linear(start_size,2)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # added for captum's prediction\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        dir = 2 if self.bi_lstm else 1\n",
        "        h = torch.zeros((self.lstm_layers*dir, x.size(0), self.hidden_size)).to(device)\n",
        "        c = torch.zeros((self.lstm_layers*dir, x.size(0), self.hidden_size)).to(device)\n",
        "\n",
        "        if self.embedding is not None:\n",
        "            x = x.type(torch.LongTensor).to(device)\n",
        "            x = self.embedding(x)\n",
        "        elif self.reshape:\n",
        "            x = x.view(x.shape[0],x.shape[1],1)\n",
        "\n",
        "        if self.model_type == \"LSTM\":\n",
        "            x, (hidden, cell) = self.lstm(x, (h,c))\n",
        "        else:\n",
        "            x, hidden = self.lstm(x, h)\n",
        "\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # print(x.shape)\n",
        "        for i, layer in enumerate(self.hidden_layers):\n",
        "            x = layer(x)\n",
        "            # print(x.shape)\n",
        "        x = self.output(x)\n",
        "        x  = softmax(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUcwPlCfFaGc"
      },
      "source": [
        "#Training and Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NETrJHBSCmOy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class TrainerDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs= inputs\n",
        "        self.targets = torch.from_numpy(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.inputs[idx]), self.targets[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNp_GjpLF6x4"
      },
      "outputs": [],
      "source": [
        "def trainer(config, train_x, train_y, num_epochs=100, batch_size=32, debug=False, lr=0.0001,model_type=\"LSTM\"):\n",
        "    train_pos_idx = np.where(train_y==1)\n",
        "    train_neg_idx = np.where(train_y==0)\n",
        "\n",
        "    train_xp = train_x[train_pos_idx]\n",
        "    train_xn = train_x[train_neg_idx]\n",
        "\n",
        "    train_yp = train_y[train_pos_idx]\n",
        "    train_yn = train_y[train_neg_idx]\n",
        "\n",
        "    train_dataset_pos = TrainerDataset(train_xp, train_yp)\n",
        "    train_dataloader_pos = DataLoader(train_dataset_pos, batch_size=batch_size//2, shuffle=True)\n",
        "    train_dataset_neg = TrainerDataset(train_xn, train_yn)\n",
        "    train_dataloader_neg = DataLoader(train_dataset_neg, batch_size=batch_size//2, shuffle=True)\n",
        "\n",
        "    seed = 12345\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    model = RNN_Model_Generic(config, model_type).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "    n_total_steps = len(train_dataloader_neg)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (train_features_neg, train_labels_neg) in enumerate(train_dataloader_neg):\n",
        "            train_features_pos, train_labels_pos = next(iter(train_dataloader_pos))\n",
        "            train_features = torch.cat((train_features_pos, train_features_neg),0)\n",
        "            train_labels = torch.cat((train_labels_pos, train_labels_neg),0)\n",
        "\n",
        "#             print(train_features.shape, train_labels.shape)\n",
        "\n",
        "            outputs = model(train_features.to(device))\n",
        "            loss = criterion(outputs, train_labels.to(device))\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "#             if (i+1) % 2000 == 0 and epoch % 10 == 0:\n",
        "            if (i+1) % 200 == 0:\n",
        "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "                if debug:\n",
        "                    return model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKq4TBTIWORv"
      },
      "outputs": [],
      "source": [
        "def tester(model, test_x, test_y):\n",
        "    test_dataset = TrainerDataset(test_x, test_y)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "    model.eval()\n",
        "    results = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for test_features, test_labels in test_dataloader:\n",
        "            outputs = model(test_features.to(device)).detach().to(\"cpu\")\n",
        "            results.extend(outputs)\n",
        "            true_labels.extend(test_labels)\n",
        "    return true_labels, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIEfwpqvUZwt"
      },
      "outputs": [],
      "source": [
        "class Stats:\n",
        "    def __init__(self):\n",
        "        self.acc = 0\n",
        "        self.pre = 0\n",
        "        self.re = 0\n",
        "        self.f1 = 0\n",
        "        self.roc = 0\n",
        "        self.prc = 0\n",
        "        self.tn = 0\n",
        "        self.fp = 0\n",
        "        self.fn = 0\n",
        "        self.tp = 0\n",
        "        self.pred_y = []\n",
        "    def print(self):\n",
        "        print('Accuracy: %.4f' %self.acc)\n",
        "        print('Precision: %.4f' %self.pre)\n",
        "        print('Recall: %.4f' %self.re)\n",
        "        print('F1 Score: %.4f' %self.f1)\n",
        "        print('ROC: %.4f' %self.roc)\n",
        "        print('PR AUC: %.4f' %self.prc)\n",
        "        print(\"Confusion Matrix\")\n",
        "        print(self.tn, \"\\t\", self.fp)\n",
        "        print(self.fn, \"\\t\", self.tp)\n",
        "\n",
        "    def formatted_print(self):\n",
        "        print(self.acc, end=\"\\t\")\n",
        "        print(self.pre, end=\"\\t\")\n",
        "        print(self.re, end=\"\\t\")\n",
        "        print(self.f1, end=\"\\t\")\n",
        "        print(self.roc, end=\"\\t\")\n",
        "        print(self.prc, end=\"\\t\")\n",
        "        print(self.tn, \"\\t\", self.fp, end=\"\\t\", sep=\"\")\n",
        "        print(self.fn, \"\\t\", self.tp, end=\"\\t\", sep=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NhnOw-xXDPK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def eval_matrices(model, test_x, test_y, debug = False):\n",
        "    true_y, results = tester(model, test_x, test_y)\n",
        "    # predictions = [torch.nn.functional.softmax(r) for r in results]\n",
        "    predictions = results\n",
        "    print(predictions[0])\n",
        "    pred_y = np.array([y[1].item() for y in predictions])\n",
        "    pred_y_list = []\n",
        "    test_y = np.array([y.item() for y in true_y])\n",
        "\n",
        "    for x in pred_y:\n",
        "        if(x>0.5):\n",
        "            pred_y_list.append(1)\n",
        "        else:\n",
        "            pred_y_list.append(0)\n",
        "\n",
        "    pred_y_list = np.array(pred_y_list)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(test_y, pred_y_list).ravel()\n",
        "    precision, recall, _ = precision_recall_curve(test_y, pred_y)\n",
        "    auc_score = auc(recall, precision)\n",
        "    acc = accuracy_score(test_y, pred_y_list)\n",
        "\n",
        "    pr = -1\n",
        "    re = -1\n",
        "    f1 = -1\n",
        "    try:\n",
        "        pr = tp / (tp+fp)\n",
        "        re = tp / (tp+fn)\n",
        "        f1 = 2*pr*re / (pr+re)\n",
        "    except:\n",
        "        f1 = -1\n",
        "\n",
        "    stats = Stats()\n",
        "    stats.acc = acc\n",
        "    stats.pre = pr\n",
        "    stats.re = re\n",
        "    stats.f1 = f1\n",
        "    stats.roc = roc_auc_score(test_y, pred_y)\n",
        "    stats.prc = auc_score\n",
        "    stats.tn = tn\n",
        "    stats.fp = fp\n",
        "    stats.fn = fn\n",
        "    stats.tp = tp\n",
        "\n",
        "    if debug:\n",
        "        print('Accuracy: %.4f' %acc)\n",
        "        print('Precision: %.4f' %pr)\n",
        "        print('Recall: %.4f' %re)\n",
        "        print('F1 Score: %.4f' %f1)\n",
        "        print('ROC:',roc_auc_score(test_y, pred_y))\n",
        "        print('PR AUC: %.4f' % auc_score)\n",
        "\n",
        "        print(classification_report(test_y, pred_y_list, digits=4))\n",
        "        print(\"Confusion Matrix\")\n",
        "        print(confusion_matrix(test_y, pred_y_list))\n",
        "\n",
        "    stats.pred_y = pred_y_list\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBNJ4uUgsAKW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pickle5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iptwcTgGswh2",
        "outputId": "73ceb010-d5ab-4bb3-e2c2-64027bfe7fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.10/dist-packages (0.0.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ-BGDRpsAKW",
        "outputId": "b33c0632-ac0d-41ed-a5fb-9cc6189981fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(153233, 23, 4)\n",
            "(153233,)\n",
            "(122586, 23, 4) (122586,)\n",
            "(30647, 23, 4) (30647,)\n"
          ]
        }
      ],
      "source": [
        "import pickle5 as pkl\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "in_file = data_dir + \"Encoded Data/all_encoded_data_new.pkl\"\n",
        "enc_dict = {}\n",
        "with open(in_file, \"rb\") as f:\n",
        "    enc_dict = pkl.load(f)\n",
        "\n",
        "data_x = enc_dict['enc_superposed']\n",
        "data_y = enc_dict['labels']\n",
        "\n",
        "data_x = np.array(data_x)\n",
        "data_y = np.array(data_y)\n",
        "\n",
        "print(data_x.shape)\n",
        "print(data_y.shape)\n",
        "\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y,\n",
        "                                                    stratify=data_y,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state=5)\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XImkyPIBsAKY"
      },
      "source": [
        "# Captum Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wyPcCzmaIux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfbcf58-d64b-4135-f7e7-7d0121c35867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynuQ-vBrsAKY",
        "outputId": "793918b1-843d-4cdb-c780-4359e95dcb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30647,) <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "from captum import attr\n",
        "torch.backends.cudnn.enabled=False\n",
        "# device = 'cpu'\n",
        "# print(stats.pred_y.shape, type(stats.pred_y))\n",
        "print(test_y.shape, type(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights = resource_dir + \"best_lstm_model.pth\""
      ],
      "metadata": {
        "id": "8uT-GvWjtL29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_config = {\n",
        "    'vocab_size': 0,\n",
        "    'emb_size': 4,\n",
        "    'hidden_size': 512,\n",
        "    'lstm_layers': 1,\n",
        "    'bi_lstm': True,\n",
        "    'number_hidder_layers': 2,\n",
        "    'dropout_prob': 0.4,\n",
        "    'reshape': False,\n",
        "    'batch_size': 64,\n",
        "    'epochs': 50,\n",
        "    'learning_rate': 0.00010\n",
        "}"
      ],
      "metadata": {
        "id": "NfFAbYeAtYXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wC2Iue3sAKY"
      },
      "outputs": [],
      "source": [
        "model = RNN_Model_Generic(best_config, \"LSTM\").to(device)\n",
        "model.load_state_dict(torch.load(model_weights))\n",
        "model.eval()\n",
        "test_yn = torch.from_numpy(test_y)\n",
        "interpreter = attr.IntegratedGradients(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats = eval_matrices(model, test_x, test_y)\n",
        "print(test_y[0], stats.pred_y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRckP7nWva6q",
        "outputId": "be7a3f54-8048-4db5-c6ab-1f403f122420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0000e+00, 6.6549e-13])\n",
            "0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Fogomwb_skQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbcOy2ObsAKY",
        "outputId": "8414a425-fd91-4519-87fa-3d3305d90f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 100\n",
            "100 200\n",
            "200 300\n",
            "300 400\n",
            "400 500\n",
            "500 600\n",
            "600 700\n",
            "700 800\n",
            "800 900\n",
            "900 1000\n",
            "1000 1100\n",
            "1100 1200\n",
            "1200 1300\n",
            "1300 1400\n",
            "1400 1500\n",
            "1500 1600\n",
            "1600 1700\n",
            "1700 1800\n",
            "1800 1900\n",
            "1900 2000\n",
            "2000 2100\n",
            "2100 2200\n",
            "2200 2300\n",
            "2300 2400\n",
            "2400 2500\n",
            "2500 2600\n",
            "2600 2700\n",
            "2700 2800\n",
            "2800 2900\n",
            "2900 3000\n",
            "3000 3100\n",
            "3100 3200\n",
            "3200 3300\n",
            "3300 3400\n",
            "3400 3500\n",
            "3500 3600\n",
            "3600 3700\n",
            "3700 3800\n",
            "3800 3900\n",
            "3900 4000\n",
            "4000 4100\n",
            "4100 4200\n",
            "4200 4300\n",
            "4300 4400\n",
            "4400 4500\n",
            "4500 4600\n",
            "4600 4700\n",
            "4700 4800\n",
            "4800 4900\n",
            "4900 5000\n",
            "5000 5100\n",
            "5100 5200\n",
            "5200 5300\n",
            "5300 5400\n",
            "5400 5500\n",
            "5500 5600\n",
            "5600 5700\n",
            "5700 5800\n",
            "5800 5900\n",
            "5900 6000\n",
            "6000 6100\n",
            "6100 6200\n",
            "6200 6300\n",
            "6300 6400\n",
            "6400 6500\n",
            "6500 6600\n",
            "6600 6700\n",
            "6700 6800\n",
            "6800 6900\n",
            "6900 7000\n",
            "7000 7100\n",
            "7100 7200\n",
            "7200 7300\n",
            "7300 7400\n",
            "7400 7500\n",
            "7500 7600\n",
            "7600 7700\n",
            "7700 7800\n",
            "7800 7900\n",
            "7900 8000\n",
            "8000 8100\n",
            "8100 8200\n",
            "8200 8300\n",
            "8300 8400\n",
            "8400 8500\n",
            "8500 8600\n",
            "8600 8700\n",
            "8700 8800\n",
            "8800 8900\n",
            "8900 9000\n",
            "9000 9100\n",
            "9100 9200\n",
            "9200 9300\n",
            "9300 9400\n",
            "9400 9500\n",
            "9500 9600\n",
            "9600 9700\n",
            "9700 9800\n",
            "9800 9900\n",
            "9900 10000\n",
            "10000 10100\n",
            "10100 10200\n",
            "10200 10300\n",
            "10300 10400\n",
            "10400 10500\n",
            "10500 10600\n",
            "10600 10700\n",
            "10700 10800\n",
            "10800 10900\n",
            "10900 11000\n",
            "11000 11100\n",
            "11100 11200\n",
            "11200 11300\n",
            "11300 11400\n",
            "11400 11500\n",
            "11500 11600\n",
            "11600 11700\n",
            "11700 11800\n",
            "11800 11900\n",
            "11900 12000\n",
            "12000 12100\n",
            "12100 12200\n",
            "12200 12300\n",
            "12300 12400\n",
            "12400 12500\n",
            "12500 12600\n",
            "12600 12700\n",
            "12700 12800\n",
            "12800 12900\n",
            "12900 13000\n",
            "13000 13100\n",
            "13100 13200\n",
            "13200 13300\n",
            "13300 13400\n",
            "13400 13500\n",
            "13500 13600\n",
            "13600 13700\n",
            "13700 13800\n",
            "13800 13900\n",
            "13900 14000\n",
            "14000 14100\n",
            "14100 14200\n",
            "14200 14300\n",
            "14300 14400\n",
            "14400 14500\n",
            "14500 14600\n",
            "14600 14700\n",
            "14700 14800\n",
            "14800 14900\n",
            "14900 15000\n",
            "15000 15100\n",
            "15100 15200\n",
            "15200 15300\n",
            "15300 15400\n",
            "15400 15500\n",
            "15500 15600\n",
            "15600 15700\n",
            "15700 15800\n",
            "15800 15900\n",
            "15900 16000\n",
            "16000 16100\n",
            "16100 16200\n",
            "16200 16300\n",
            "16300 16400\n",
            "16400 16500\n",
            "16500 16600\n",
            "16600 16700\n",
            "16700 16800\n",
            "16800 16900\n",
            "16900 17000\n",
            "17000 17100\n",
            "17100 17200\n",
            "17200 17300\n",
            "17300 17400\n",
            "17400 17500\n",
            "17500 17600\n",
            "17600 17700\n",
            "17700 17800\n",
            "17800 17900\n",
            "17900 18000\n",
            "18000 18100\n",
            "18100 18200\n",
            "18200 18300\n",
            "18300 18400\n",
            "18400 18500\n",
            "18500 18600\n",
            "18600 18700\n",
            "18700 18800\n",
            "18800 18900\n",
            "18900 19000\n",
            "19000 19100\n",
            "19100 19200\n",
            "19200 19300\n",
            "19300 19400\n",
            "19400 19500\n",
            "19500 19600\n",
            "19600 19700\n",
            "19700 19800\n",
            "19800 19900\n",
            "19900 20000\n",
            "20000 20100\n",
            "20100 20200\n",
            "20200 20300\n",
            "20300 20400\n",
            "20400 20500\n",
            "20500 20600\n",
            "20600 20700\n",
            "20700 20800\n",
            "20800 20900\n",
            "20900 21000\n",
            "21000 21100\n",
            "21100 21200\n",
            "21200 21300\n",
            "21300 21400\n",
            "21400 21500\n",
            "21500 21600\n",
            "21600 21700\n",
            "21700 21800\n",
            "21800 21900\n",
            "21900 22000\n",
            "22000 22100\n",
            "22100 22200\n",
            "22200 22300\n",
            "22300 22400\n",
            "22400 22500\n",
            "22500 22600\n",
            "22600 22700\n",
            "22700 22800\n",
            "22800 22900\n",
            "22900 23000\n",
            "23000 23100\n",
            "23100 23200\n",
            "23200 23300\n",
            "23300 23400\n",
            "23400 23500\n",
            "23500 23600\n",
            "23600 23700\n",
            "23700 23800\n",
            "23800 23900\n",
            "23900 24000\n",
            "24000 24100\n",
            "24100 24200\n",
            "24200 24300\n",
            "24300 24400\n",
            "24400 24500\n",
            "24500 24600\n",
            "24600 24700\n",
            "24700 24800\n",
            "24800 24900\n",
            "24900 25000\n",
            "25000 25100\n",
            "25100 25200\n",
            "25200 25300\n",
            "25300 25400\n",
            "25400 25500\n",
            "25500 25600\n",
            "25600 25700\n",
            "25700 25800\n",
            "25800 25900\n",
            "25900 26000\n",
            "26000 26100\n",
            "26100 26200\n",
            "26200 26300\n",
            "26300 26400\n",
            "26400 26500\n",
            "26500 26600\n",
            "26600 26700\n",
            "26700 26800\n",
            "26800 26900\n",
            "26900 27000\n",
            "27000 27100\n",
            "27100 27200\n",
            "27200 27300\n",
            "27300 27400\n",
            "27400 27500\n",
            "27500 27600\n",
            "27600 27700\n",
            "27700 27800\n",
            "27800 27900\n",
            "27900 28000\n",
            "28000 28100\n",
            "28100 28200\n",
            "28200 28300\n",
            "28300 28400\n",
            "28400 28500\n",
            "28500 28600\n",
            "28600 28700\n",
            "28700 28800\n",
            "28800 28900\n",
            "28900 29000\n",
            "29000 29100\n",
            "29100 29200\n",
            "29200 29300\n",
            "29300 29400\n",
            "29400 29500\n",
            "29500 29600\n",
            "29600 29700\n",
            "29700 29800\n",
            "29800 29900\n",
            "29900 30000\n",
            "30000 30100\n",
            "30100 30200\n",
            "30200 30300\n",
            "30300 30400\n",
            "30400 30500\n",
            "30500 30600\n",
            "30600 30647\n",
            "(30647, 23, 4)\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(resource_dir + \"feature_attribution_dict.pkl\"):\n",
        "    attribution_batch_size = 100\n",
        "    attributions = []\n",
        "    for i in range(0,test_x.shape[0], attribution_batch_size):\n",
        "        start = i\n",
        "        end = min(i+attribution_batch_size, test_x.shape[0])\n",
        "        # batch_attributions = interpreter.attribute(torch.Tensor(test_x[start:end]).to(device),\n",
        "        #                                      target = stats.pred_y[start:end].to(device))\n",
        "        batch_attributions = interpreter.attribute(torch.Tensor(test_x[start:end]).requires_grad_().to(device), target = 1)\n",
        "\n",
        "        # batch_attributions = interpreter.attribute(torch.Tensor(test_x[start:end]).requires_grad_().to(device), target = 0)\n",
        "        attributions.extend(batch_attributions.detach().to(\"cpu\").numpy())\n",
        "        torch.cuda.empty_cache()\n",
        "        print(start, end)\n",
        "        # break\n",
        "\n",
        "    attributions = np.array(attributions)\n",
        "    print(attributions.shape)\n",
        "\n",
        "    attribution_dict = {\n",
        "        \"overall_on_1\" : attributions,\n",
        "    }\n",
        "\n",
        "    with open(resource_dir + \"feature_attribution_dict.pkl\", \"wb\") as file:\n",
        "        pkl.dump(attribution_dict, file)\n",
        "else:\n",
        "    torch.cuda.empty_cache()\n",
        "    with open(resource_dir + \"feature_attribution_dict.pkl\", \"rb\") as file:\n",
        "        attribution_dict = pkl.load(file)\n",
        "        attributions = attribution_dict[\"overall_on_1\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n-2WPpFsAKZ",
        "outputId": "7b56710b-5c11-4345-ec86-fe4437ca848f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23, 4)\n",
            "(23,)\n",
            "[ 0.06940305  0.03091268  0.06772827  0.02346417  0.00197513 -0.04398288\n",
            " -0.06407119 -0.02852685 -0.02722828 -0.00378091 -0.00515271  0.04882731\n",
            "  0.06230796  0.16130585  0.06240389  0.06459423  0.06643197  0.02774311\n",
            "  0.20851873  0.04147655 -0.00404112 -0.01631606 -0.00524812]\n",
            "(23,)\n",
            "[ 0.20964384  0.09337706  0.20458488  0.07087756  0.00596623 -0.13285784\n",
            " -0.19353804 -0.08617024 -0.08224771 -0.01142089 -0.01556466  0.1474913\n",
            "  0.18821189  0.48725203  0.18850168  0.19511798  0.2006692   0.08380284\n",
            "  0.62986666  0.12528704 -0.01220691 -0.04928546 -0.01585285]\n"
          ]
        }
      ],
      "source": [
        "feat_attribution = attributions[0]\n",
        "print(feat_attribution.shape)\n",
        "feat_attribution = feat_attribution.sum(axis=1)\n",
        "print(feat_attribution.shape)\n",
        "print(feat_attribution)\n",
        "feat_attribution = feat_attribution / np.linalg.norm(feat_attribution)\n",
        "print(feat_attribution.shape)\n",
        "print(feat_attribution)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importantance_dict = {}\n",
        "# order=['A','T','C','G']\n",
        "order_dict = { 0 : 'A', 1:'T', 2:'C', 3:'G', 4:'R' }\n",
        "print(test_x.shape)\n",
        "\n",
        "for i in range(test_x.shape[0]):\n",
        "    ty = int(test_y[i])\n",
        "    py = int(stats.pred_y[i])\n",
        "\n",
        "    for j in range(test_x.shape[1]):\n",
        "        feature_name = 'Pos_' + str(j+1) + '_'\n",
        "        feat_attribution = attributions[i]\n",
        "        feat_attribution = feat_attribution.sum(axis=1)\n",
        "        feat_attribution = feat_attribution / np.linalg.norm(feat_attribution)\n",
        "\n",
        "        for k in range(test_x.shape[2]):\n",
        "            if test_x[i][j][k] > 0:\n",
        "                feature_name += order_dict[k]\n",
        "\n",
        "        if feature_name not in feature_importantance_dict:\n",
        "            feature_importantance_dict[feature_name] = {'tpc':0, 'tnc':0, 'fpc':0, 'fnc':0,\n",
        "                                                        'tps':0, 'tns':0, 'fps':0, 'fns':0,}\n",
        "\n",
        "        if ty == 0 and py == 0:\n",
        "            feature_importantance_dict[feature_name]['tnc'] += 1\n",
        "            feature_importantance_dict[feature_name]['tns'] += feat_attribution[j]\n",
        "\n",
        "        elif ty == 0 and py == 1:\n",
        "            feature_importantance_dict[feature_name]['fpc'] += 1\n",
        "            feature_importantance_dict[feature_name]['fps'] += feat_attribution[j]\n",
        "\n",
        "        elif ty == 1 and py == 0:\n",
        "            feature_importantance_dict[feature_name]['fnc'] += 1\n",
        "            feature_importantance_dict[feature_name]['fns'] += feat_attribution[j]\n",
        "\n",
        "        elif ty == 1 and py == 1:\n",
        "            feature_importantance_dict[feature_name]['tpc'] += 1\n",
        "            feature_importantance_dict[feature_name]['tps'] += feat_attribution[j]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seGkD32Af4BE",
        "outputId": "447a96be-1c65-434f-ead2-b51677dfcca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30647, 23, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uxPOOc-sAKd",
        "outputId": "9c5b3e77-9a4b-412e-9737-481540f648fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218\n"
          ]
        }
      ],
      "source": [
        "features = list(feature_importantance_dict.keys())\n",
        "tp = []\n",
        "tps = []\n",
        "tn = []\n",
        "tns = []\n",
        "fp = []\n",
        "fps = []\n",
        "fn = []\n",
        "fns = []\n",
        "print(len(features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUTheZgmsAKd"
      },
      "outputs": [],
      "source": [
        "for f in features:\n",
        "    tn.append(feature_importantance_dict[f]['tnc'])\n",
        "    tns.append(feature_importantance_dict[f]['tns'])\n",
        "\n",
        "    fp.append(feature_importantance_dict[f]['fpc'])\n",
        "    fps.append(feature_importantance_dict[f]['fps'])\n",
        "\n",
        "    fn.append(feature_importantance_dict[f]['fnc'])\n",
        "    fns.append(feature_importantance_dict[f]['fns'])\n",
        "\n",
        "    tp.append(feature_importantance_dict[f]['tpc'])\n",
        "    tps.append(feature_importantance_dict[f]['tps'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHGyKPyPsAKd",
        "outputId": "bfe0fd87-b29c-40b0-f4a8-3ed5684e373e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(218, 9)\n",
            "features      Pos_1_CG\n",
            "tnc               1953\n",
            "tns         182.117507\n",
            "fpc                  4\n",
            "fps          -0.106583\n",
            "fnc                  7\n",
            "fns           1.859818\n",
            "tpc                  8\n",
            "tps           0.059663\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "reformed_dict = {\n",
        "    \"features\" : features,\n",
        "    \"tnc\" : tn,\n",
        "    \"tns\" : tns,\n",
        "    \"fpc\" : fp,\n",
        "    \"fps\" : fps,\n",
        "    \"fnc\" : fn,\n",
        "    \"fns\" : fns,\n",
        "    \"tpc\" : tp,\n",
        "    \"tps\" : tps\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(reformed_dict)\n",
        "print(df.shape)\n",
        "print(df.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbzjCtOosAKd"
      },
      "outputs": [],
      "source": [
        "df.to_excel(resource_dir + 'feature_importance_neg.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJQ0HTspsAKd"
      },
      "source": [
        "# Layer Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_AImHiAsAKe",
        "outputId": "90b2da1c-86ea-491b-ec13-0b54fba0f2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN_Model_Generic(\n",
            "  (lstm): LSTM(4, 512, batch_first=True, bidirectional=True)\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (output): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igK-IlJAsAKe",
        "outputId": "9c22356c-cc8c-4529-bb02-a185fe7880bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU()\n",
            "ReLU()\n"
          ]
        }
      ],
      "source": [
        "print(model.hidden_layers[0][1])\n",
        "print(model.hidden_layers[1][1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "CGZCGqGDgTOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJcyRnGIsAKe",
        "outputId": "8183f951-3fcb-46ff-86e9-1ae70b52a4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 25\n",
            "25 50\n",
            "50 75\n",
            "75 100\n",
            "100 125\n",
            "125 150\n",
            "150 175\n",
            "175 200\n",
            "200 225\n",
            "225 250\n",
            "250 275\n",
            "275 300\n",
            "300 325\n",
            "325 350\n",
            "350 375\n",
            "375 400\n",
            "400 425\n",
            "425 450\n",
            "450 475\n",
            "475 500\n",
            "500 525\n",
            "525 550\n",
            "550 575\n",
            "575 600\n",
            "600 625\n",
            "625 650\n",
            "650 675\n",
            "675 700\n",
            "700 725\n",
            "725 750\n",
            "750 775\n",
            "775 800\n",
            "800 825\n",
            "825 850\n",
            "850 875\n",
            "875 900\n",
            "900 925\n",
            "925 950\n",
            "950 975\n",
            "975 1000\n",
            "1000 1025\n",
            "1025 1050\n",
            "1050 1075\n",
            "1075 1100\n",
            "1100 1125\n",
            "1125 1150\n",
            "1150 1175\n",
            "1175 1200\n",
            "1200 1225\n",
            "1225 1250\n",
            "1250 1275\n",
            "1275 1300\n",
            "1300 1325\n",
            "1325 1350\n",
            "1350 1375\n",
            "1375 1400\n",
            "1400 1425\n",
            "1425 1450\n",
            "1450 1475\n",
            "1475 1500\n",
            "1500 1525\n",
            "1525 1550\n",
            "1550 1575\n",
            "1575 1600\n",
            "1600 1625\n",
            "1625 1650\n",
            "1650 1675\n",
            "1675 1700\n",
            "1700 1725\n",
            "1725 1750\n",
            "1750 1775\n",
            "1775 1800\n",
            "1800 1825\n",
            "1825 1850\n",
            "1850 1875\n",
            "1875 1900\n",
            "1900 1925\n",
            "1925 1950\n",
            "1950 1975\n",
            "1975 2000\n",
            "2000 2025\n",
            "2025 2050\n",
            "2050 2075\n",
            "2075 2100\n",
            "2100 2125\n",
            "2125 2150\n",
            "2150 2175\n",
            "2175 2200\n",
            "2200 2225\n",
            "2225 2250\n",
            "2250 2275\n",
            "2275 2300\n",
            "2300 2325\n",
            "2325 2350\n",
            "2350 2375\n",
            "2375 2400\n",
            "2400 2425\n",
            "2425 2450\n",
            "2450 2475\n",
            "2475 2500\n",
            "2500 2525\n",
            "2525 2550\n",
            "2550 2575\n",
            "2575 2600\n",
            "2600 2625\n",
            "2625 2650\n",
            "2650 2675\n",
            "2675 2700\n",
            "2700 2725\n",
            "2725 2750\n",
            "2750 2775\n",
            "2775 2800\n",
            "2800 2825\n",
            "2825 2850\n",
            "2850 2875\n",
            "2875 2900\n",
            "2900 2925\n",
            "2925 2950\n",
            "2950 2975\n",
            "2975 3000\n",
            "3000 3025\n",
            "3025 3050\n",
            "3050 3075\n",
            "3075 3100\n",
            "3100 3125\n",
            "3125 3150\n",
            "3150 3175\n",
            "3175 3200\n",
            "3200 3225\n",
            "3225 3250\n",
            "3250 3275\n",
            "3275 3300\n",
            "3300 3325\n",
            "3325 3350\n",
            "3350 3375\n",
            "3375 3400\n",
            "3400 3425\n",
            "3425 3450\n",
            "3450 3475\n",
            "3475 3500\n",
            "3500 3525\n",
            "3525 3550\n",
            "3550 3575\n",
            "3575 3600\n",
            "3600 3625\n",
            "3625 3650\n",
            "3650 3675\n",
            "3675 3700\n",
            "3700 3725\n",
            "3725 3750\n",
            "3750 3775\n",
            "3775 3800\n",
            "3800 3825\n",
            "3825 3850\n",
            "3850 3875\n",
            "3875 3900\n",
            "3900 3925\n",
            "3925 3950\n",
            "3950 3975\n",
            "3975 4000\n",
            "4000 4025\n",
            "4025 4050\n",
            "4050 4075\n",
            "4075 4100\n",
            "4100 4125\n",
            "4125 4150\n",
            "4150 4175\n",
            "4175 4200\n",
            "4200 4225\n",
            "4225 4250\n",
            "4250 4275\n",
            "4275 4300\n",
            "4300 4325\n",
            "4325 4350\n",
            "4350 4375\n",
            "4375 4400\n",
            "4400 4425\n",
            "4425 4450\n",
            "4450 4475\n",
            "4475 4500\n",
            "4500 4525\n",
            "4525 4550\n",
            "4550 4575\n",
            "4575 4600\n",
            "4600 4625\n",
            "4625 4650\n",
            "4650 4675\n",
            "4675 4700\n",
            "4700 4725\n",
            "4725 4750\n",
            "4750 4775\n",
            "4775 4800\n",
            "4800 4825\n",
            "4825 4850\n",
            "4850 4875\n",
            "4875 4900\n",
            "4900 4925\n",
            "4925 4950\n",
            "4950 4975\n",
            "4975 5000\n",
            "5000 5025\n",
            "5025 5050\n",
            "5050 5075\n",
            "5075 5100\n",
            "5100 5125\n",
            "5125 5150\n",
            "5150 5175\n",
            "5175 5200\n",
            "5200 5225\n",
            "5225 5250\n",
            "5250 5275\n",
            "5275 5300\n",
            "5300 5325\n",
            "5325 5350\n",
            "5350 5375\n",
            "5375 5400\n",
            "5400 5425\n",
            "5425 5450\n",
            "5450 5475\n",
            "5475 5500\n",
            "5500 5525\n",
            "5525 5550\n",
            "5550 5575\n",
            "5575 5600\n",
            "5600 5625\n",
            "5625 5650\n",
            "5650 5675\n",
            "5675 5700\n",
            "5700 5725\n",
            "5725 5750\n",
            "5750 5775\n",
            "5775 5800\n",
            "5800 5825\n",
            "5825 5850\n",
            "5850 5875\n",
            "5875 5900\n",
            "5900 5925\n",
            "5925 5950\n",
            "5950 5975\n",
            "5975 6000\n",
            "6000 6025\n",
            "6025 6050\n",
            "6050 6075\n",
            "6075 6100\n",
            "6100 6125\n",
            "6125 6150\n",
            "6150 6175\n",
            "6175 6200\n",
            "6200 6225\n",
            "6225 6250\n",
            "6250 6275\n",
            "6275 6300\n",
            "6300 6325\n",
            "6325 6350\n",
            "6350 6375\n",
            "6375 6400\n",
            "6400 6425\n",
            "6425 6450\n",
            "6450 6475\n",
            "6475 6500\n",
            "6500 6525\n",
            "6525 6550\n",
            "6550 6575\n",
            "6575 6600\n",
            "6600 6625\n",
            "6625 6650\n",
            "6650 6675\n",
            "6675 6700\n",
            "6700 6725\n",
            "6725 6750\n",
            "6750 6775\n",
            "6775 6800\n",
            "6800 6825\n",
            "6825 6850\n",
            "6850 6875\n",
            "6875 6900\n",
            "6900 6925\n",
            "6925 6950\n",
            "6950 6975\n",
            "6975 7000\n",
            "7000 7025\n",
            "7025 7050\n",
            "7050 7075\n",
            "7075 7100\n",
            "7100 7125\n",
            "7125 7150\n",
            "7150 7175\n",
            "7175 7200\n",
            "7200 7225\n",
            "7225 7250\n",
            "7250 7275\n",
            "7275 7300\n",
            "7300 7325\n",
            "7325 7350\n",
            "7350 7375\n",
            "7375 7400\n",
            "7400 7425\n",
            "7425 7450\n",
            "7450 7475\n",
            "7475 7500\n",
            "7500 7525\n",
            "7525 7550\n",
            "7550 7575\n",
            "7575 7600\n",
            "7600 7625\n",
            "7625 7650\n",
            "7650 7675\n",
            "7675 7700\n",
            "7700 7725\n",
            "7725 7750\n",
            "7750 7775\n",
            "7775 7800\n",
            "7800 7825\n",
            "7825 7850\n",
            "7850 7875\n",
            "7875 7900\n",
            "7900 7925\n",
            "7925 7950\n",
            "7950 7975\n",
            "7975 8000\n",
            "8000 8025\n",
            "8025 8050\n",
            "8050 8075\n",
            "8075 8100\n",
            "8100 8125\n",
            "8125 8150\n",
            "8150 8175\n",
            "8175 8200\n",
            "8200 8225\n",
            "8225 8250\n",
            "8250 8275\n",
            "8275 8300\n",
            "8300 8325\n",
            "8325 8350\n",
            "8350 8375\n",
            "8375 8400\n",
            "8400 8425\n",
            "8425 8450\n",
            "8450 8475\n",
            "8475 8500\n",
            "8500 8525\n",
            "8525 8550\n",
            "8550 8575\n",
            "8575 8600\n",
            "8600 8625\n",
            "8625 8650\n",
            "8650 8675\n",
            "8675 8700\n",
            "8700 8725\n",
            "8725 8750\n",
            "8750 8775\n",
            "8775 8800\n",
            "8800 8825\n",
            "8825 8850\n",
            "8850 8875\n",
            "8875 8900\n",
            "8900 8925\n",
            "8925 8950\n",
            "8950 8975\n",
            "8975 9000\n",
            "9000 9025\n",
            "9025 9050\n",
            "9050 9075\n",
            "9075 9100\n",
            "9100 9125\n",
            "9125 9150\n",
            "9150 9175\n",
            "9175 9200\n",
            "9200 9225\n",
            "9225 9250\n",
            "9250 9275\n",
            "9275 9300\n",
            "9300 9325\n",
            "9325 9350\n",
            "9350 9375\n",
            "9375 9400\n",
            "9400 9425\n",
            "9425 9450\n",
            "9450 9475\n",
            "9475 9500\n",
            "9500 9525\n",
            "9525 9550\n",
            "9550 9575\n",
            "9575 9600\n",
            "9600 9625\n",
            "9625 9650\n",
            "9650 9675\n",
            "9675 9700\n",
            "9700 9725\n",
            "9725 9750\n",
            "9750 9775\n",
            "9775 9800\n",
            "9800 9825\n",
            "9825 9850\n",
            "9850 9875\n",
            "9875 9900\n",
            "9900 9925\n",
            "9925 9950\n",
            "9950 9975\n",
            "9975 10000\n",
            "10000 10025\n",
            "10025 10050\n",
            "10050 10075\n",
            "10075 10100\n",
            "10100 10125\n",
            "10125 10150\n",
            "10150 10175\n",
            "10175 10200\n",
            "10200 10225\n",
            "10225 10250\n",
            "10250 10275\n",
            "10275 10300\n",
            "10300 10325\n",
            "10325 10350\n",
            "10350 10375\n",
            "10375 10400\n",
            "10400 10425\n",
            "10425 10450\n",
            "10450 10475\n",
            "10475 10500\n",
            "10500 10525\n",
            "10525 10550\n",
            "10550 10575\n",
            "10575 10600\n",
            "10600 10625\n",
            "10625 10650\n",
            "10650 10675\n",
            "10675 10700\n",
            "10700 10725\n",
            "10725 10750\n",
            "10750 10775\n",
            "10775 10800\n",
            "10800 10825\n",
            "10825 10850\n",
            "10850 10875\n",
            "10875 10900\n",
            "10900 10925\n",
            "10925 10950\n",
            "10950 10975\n",
            "10975 11000\n",
            "11000 11025\n",
            "11025 11050\n",
            "11050 11075\n",
            "11075 11100\n",
            "11100 11125\n",
            "11125 11150\n",
            "11150 11175\n",
            "11175 11200\n",
            "11200 11225\n",
            "11225 11250\n",
            "11250 11275\n",
            "11275 11300\n",
            "11300 11325\n",
            "11325 11350\n",
            "11350 11375\n",
            "11375 11400\n",
            "11400 11425\n",
            "11425 11450\n",
            "11450 11475\n",
            "11475 11500\n",
            "11500 11525\n",
            "11525 11550\n",
            "11550 11575\n",
            "11575 11600\n",
            "11600 11625\n",
            "11625 11650\n",
            "11650 11675\n",
            "11675 11700\n",
            "11700 11725\n",
            "11725 11750\n",
            "11750 11775\n",
            "11775 11800\n",
            "11800 11825\n",
            "11825 11850\n",
            "11850 11875\n",
            "11875 11900\n",
            "11900 11925\n",
            "11925 11950\n",
            "11950 11975\n",
            "11975 12000\n",
            "12000 12025\n",
            "12025 12050\n",
            "12050 12075\n",
            "12075 12100\n",
            "12100 12125\n",
            "12125 12150\n",
            "12150 12175\n",
            "12175 12200\n",
            "12200 12225\n",
            "12225 12250\n",
            "12250 12275\n",
            "12275 12300\n",
            "12300 12325\n",
            "12325 12350\n",
            "12350 12375\n",
            "12375 12400\n",
            "12400 12425\n",
            "12425 12450\n",
            "12450 12475\n",
            "12475 12500\n",
            "12500 12525\n",
            "12525 12550\n",
            "12550 12575\n",
            "12575 12600\n",
            "12600 12625\n",
            "12625 12650\n",
            "12650 12675\n",
            "12675 12700\n",
            "12700 12725\n",
            "12725 12750\n",
            "12750 12775\n",
            "12775 12800\n",
            "12800 12825\n",
            "12825 12850\n",
            "12850 12875\n",
            "12875 12900\n",
            "12900 12925\n",
            "12925 12950\n",
            "12950 12975\n",
            "12975 13000\n",
            "13000 13025\n",
            "13025 13050\n",
            "13050 13075\n",
            "13075 13100\n",
            "13100 13125\n",
            "13125 13150\n",
            "13150 13175\n",
            "13175 13200\n",
            "13200 13225\n",
            "13225 13250\n",
            "13250 13275\n",
            "13275 13300\n",
            "13300 13325\n",
            "13325 13350\n",
            "13350 13375\n",
            "13375 13400\n",
            "13400 13425\n",
            "13425 13450\n",
            "13450 13475\n",
            "13475 13500\n",
            "13500 13525\n",
            "13525 13550\n",
            "13550 13575\n",
            "13575 13600\n",
            "13600 13625\n",
            "13625 13650\n",
            "13650 13675\n",
            "13675 13700\n",
            "13700 13725\n",
            "13725 13750\n",
            "13750 13775\n",
            "13775 13800\n",
            "13800 13825\n",
            "13825 13850\n",
            "13850 13875\n",
            "13875 13900\n",
            "13900 13925\n",
            "13925 13950\n",
            "13950 13975\n",
            "13975 14000\n",
            "14000 14025\n",
            "14025 14050\n",
            "14050 14075\n",
            "14075 14100\n",
            "14100 14125\n",
            "14125 14150\n",
            "14150 14175\n",
            "14175 14200\n",
            "14200 14225\n",
            "14225 14250\n",
            "14250 14275\n",
            "14275 14300\n",
            "14300 14325\n",
            "14325 14350\n",
            "14350 14375\n",
            "14375 14400\n",
            "14400 14425\n",
            "14425 14450\n",
            "14450 14475\n",
            "14475 14500\n",
            "14500 14525\n",
            "14525 14550\n",
            "14550 14575\n",
            "14575 14600\n",
            "14600 14625\n",
            "14625 14650\n",
            "14650 14675\n",
            "14675 14700\n",
            "14700 14725\n",
            "14725 14750\n",
            "14750 14775\n",
            "14775 14800\n",
            "14800 14825\n",
            "14825 14850\n",
            "14850 14875\n",
            "14875 14900\n",
            "14900 14925\n",
            "14925 14950\n",
            "14950 14975\n",
            "14975 15000\n",
            "15000 15025\n",
            "15025 15050\n",
            "15050 15075\n",
            "15075 15100\n",
            "15100 15125\n",
            "15125 15150\n",
            "15150 15175\n",
            "15175 15200\n",
            "15200 15225\n",
            "15225 15250\n",
            "15250 15275\n",
            "15275 15300\n",
            "15300 15325\n",
            "15325 15350\n",
            "15350 15375\n",
            "15375 15400\n",
            "15400 15425\n",
            "15425 15450\n",
            "15450 15475\n",
            "15475 15500\n",
            "15500 15525\n",
            "15525 15550\n",
            "15550 15575\n",
            "15575 15600\n",
            "15600 15625\n",
            "15625 15650\n",
            "15650 15675\n",
            "15675 15700\n",
            "15700 15725\n",
            "15725 15750\n",
            "15750 15775\n",
            "15775 15800\n",
            "15800 15825\n",
            "15825 15850\n",
            "15850 15875\n",
            "15875 15900\n",
            "15900 15925\n",
            "15925 15950\n",
            "15950 15975\n",
            "15975 16000\n",
            "16000 16025\n",
            "16025 16050\n",
            "16050 16075\n",
            "16075 16100\n",
            "16100 16125\n",
            "16125 16150\n",
            "16150 16175\n",
            "16175 16200\n",
            "16200 16225\n",
            "16225 16250\n",
            "16250 16275\n",
            "16275 16300\n",
            "16300 16325\n",
            "16325 16350\n",
            "16350 16375\n",
            "16375 16400\n",
            "16400 16425\n",
            "16425 16450\n",
            "16450 16475\n",
            "16475 16500\n",
            "16500 16525\n",
            "16525 16550\n",
            "16550 16575\n",
            "16575 16600\n",
            "16600 16625\n",
            "16625 16650\n",
            "16650 16675\n",
            "16675 16700\n",
            "16700 16725\n",
            "16725 16750\n",
            "16750 16775\n",
            "16775 16800\n",
            "16800 16825\n",
            "16825 16850\n",
            "16850 16875\n",
            "16875 16900\n",
            "16900 16925\n",
            "16925 16950\n",
            "16950 16975\n",
            "16975 17000\n",
            "17000 17025\n",
            "17025 17050\n",
            "17050 17075\n",
            "17075 17100\n",
            "17100 17125\n",
            "17125 17150\n",
            "17150 17175\n",
            "17175 17200\n",
            "17200 17225\n",
            "17225 17250\n",
            "17250 17275\n",
            "17275 17300\n",
            "17300 17325\n",
            "17325 17350\n",
            "17350 17375\n",
            "17375 17400\n",
            "17400 17425\n",
            "17425 17450\n",
            "17450 17475\n",
            "17475 17500\n",
            "17500 17525\n",
            "17525 17550\n",
            "17550 17575\n",
            "17575 17600\n",
            "17600 17625\n",
            "17625 17650\n",
            "17650 17675\n",
            "17675 17700\n",
            "17700 17725\n",
            "17725 17750\n",
            "17750 17775\n",
            "17775 17800\n",
            "17800 17825\n",
            "17825 17850\n",
            "17850 17875\n",
            "17875 17900\n",
            "17900 17925\n",
            "17925 17950\n",
            "17950 17975\n",
            "17975 18000\n",
            "18000 18025\n",
            "18025 18050\n",
            "18050 18075\n",
            "18075 18100\n",
            "18100 18125\n",
            "18125 18150\n",
            "18150 18175\n",
            "18175 18200\n",
            "18200 18225\n",
            "18225 18250\n",
            "18250 18275\n",
            "18275 18300\n",
            "18300 18325\n",
            "18325 18350\n",
            "18350 18375\n",
            "18375 18400\n",
            "18400 18425\n",
            "18425 18450\n",
            "18450 18475\n",
            "18475 18500\n",
            "18500 18525\n",
            "18525 18550\n",
            "18550 18575\n",
            "18575 18600\n",
            "18600 18625\n",
            "18625 18650\n",
            "18650 18675\n",
            "18675 18700\n",
            "18700 18725\n",
            "18725 18750\n",
            "18750 18775\n",
            "18775 18800\n",
            "18800 18825\n",
            "18825 18850\n",
            "18850 18875\n",
            "18875 18900\n",
            "18900 18925\n",
            "18925 18950\n",
            "18950 18975\n",
            "18975 19000\n",
            "19000 19025\n",
            "19025 19050\n",
            "19050 19075\n",
            "19075 19100\n",
            "19100 19125\n",
            "19125 19150\n",
            "19150 19175\n",
            "19175 19200\n",
            "19200 19225\n",
            "19225 19250\n",
            "19250 19275\n",
            "19275 19300\n",
            "19300 19325\n",
            "19325 19350\n",
            "19350 19375\n",
            "19375 19400\n",
            "19400 19425\n",
            "19425 19450\n",
            "19450 19475\n",
            "19475 19500\n",
            "19500 19525\n",
            "19525 19550\n",
            "19550 19575\n",
            "19575 19600\n",
            "19600 19625\n",
            "19625 19650\n",
            "19650 19675\n",
            "19675 19700\n",
            "19700 19725\n",
            "19725 19750\n",
            "19750 19775\n",
            "19775 19800\n",
            "19800 19825\n",
            "19825 19850\n",
            "19850 19875\n",
            "19875 19900\n",
            "19900 19925\n",
            "19925 19950\n",
            "19950 19975\n",
            "19975 20000\n",
            "20000 20025\n",
            "20025 20050\n",
            "20050 20075\n",
            "20075 20100\n",
            "20100 20125\n",
            "20125 20150\n",
            "20150 20175\n",
            "20175 20200\n",
            "20200 20225\n",
            "20225 20250\n",
            "20250 20275\n",
            "20275 20300\n",
            "20300 20325\n",
            "20325 20350\n",
            "20350 20375\n",
            "20375 20400\n",
            "20400 20425\n",
            "20425 20450\n",
            "20450 20475\n",
            "20475 20500\n",
            "20500 20525\n",
            "20525 20550\n",
            "20550 20575\n",
            "20575 20600\n",
            "20600 20625\n",
            "20625 20650\n",
            "20650 20675\n",
            "20675 20700\n",
            "20700 20725\n",
            "20725 20750\n",
            "20750 20775\n",
            "20775 20800\n",
            "20800 20825\n",
            "20825 20850\n",
            "20850 20875\n",
            "20875 20900\n",
            "20900 20925\n",
            "20925 20950\n",
            "20950 20975\n",
            "20975 21000\n",
            "21000 21025\n",
            "21025 21050\n",
            "21050 21075\n",
            "21075 21100\n",
            "21100 21125\n",
            "21125 21150\n",
            "21150 21175\n",
            "21175 21200\n",
            "21200 21225\n",
            "21225 21250\n",
            "21250 21275\n",
            "21275 21300\n",
            "21300 21325\n",
            "21325 21350\n",
            "21350 21375\n",
            "21375 21400\n",
            "21400 21425\n",
            "21425 21450\n",
            "21450 21475\n",
            "21475 21500\n",
            "21500 21525\n",
            "21525 21550\n",
            "21550 21575\n",
            "21575 21600\n",
            "21600 21625\n",
            "21625 21650\n",
            "21650 21675\n",
            "21675 21700\n",
            "21700 21725\n",
            "21725 21750\n",
            "21750 21775\n",
            "21775 21800\n",
            "21800 21825\n",
            "21825 21850\n",
            "21850 21875\n",
            "21875 21900\n",
            "21900 21925\n",
            "21925 21950\n",
            "21950 21975\n",
            "21975 22000\n",
            "22000 22025\n",
            "22025 22050\n",
            "22050 22075\n",
            "22075 22100\n",
            "22100 22125\n",
            "22125 22150\n",
            "22150 22175\n",
            "22175 22200\n",
            "22200 22225\n",
            "22225 22250\n",
            "22250 22275\n",
            "22275 22300\n",
            "22300 22325\n",
            "22325 22350\n",
            "22350 22375\n",
            "22375 22400\n",
            "22400 22425\n",
            "22425 22450\n",
            "22450 22475\n",
            "22475 22500\n",
            "22500 22525\n",
            "22525 22550\n",
            "22550 22575\n",
            "22575 22600\n",
            "22600 22625\n",
            "22625 22650\n",
            "22650 22675\n",
            "22675 22700\n",
            "22700 22725\n",
            "22725 22750\n",
            "22750 22775\n",
            "22775 22800\n",
            "22800 22825\n",
            "22825 22850\n",
            "22850 22875\n",
            "22875 22900\n",
            "22900 22925\n",
            "22925 22950\n",
            "22950 22975\n",
            "22975 23000\n",
            "23000 23025\n",
            "23025 23050\n",
            "23050 23075\n",
            "23075 23100\n",
            "23100 23125\n",
            "23125 23150\n",
            "23150 23175\n",
            "23175 23200\n",
            "23200 23225\n",
            "23225 23250\n",
            "23250 23275\n",
            "23275 23300\n",
            "23300 23325\n",
            "23325 23350\n",
            "23350 23375\n",
            "23375 23400\n",
            "23400 23425\n",
            "23425 23450\n",
            "23450 23475\n",
            "23475 23500\n",
            "23500 23525\n",
            "23525 23550\n",
            "23550 23575\n",
            "23575 23600\n",
            "23600 23625\n",
            "23625 23650\n",
            "23650 23675\n",
            "23675 23700\n",
            "23700 23725\n",
            "23725 23750\n",
            "23750 23775\n",
            "23775 23800\n",
            "23800 23825\n",
            "23825 23850\n",
            "23850 23875\n",
            "23875 23900\n",
            "23900 23925\n",
            "23925 23950\n",
            "23950 23975\n",
            "23975 24000\n",
            "24000 24025\n",
            "24025 24050\n",
            "24050 24075\n",
            "24075 24100\n",
            "24100 24125\n",
            "24125 24150\n",
            "24150 24175\n",
            "24175 24200\n",
            "24200 24225\n",
            "24225 24250\n",
            "24250 24275\n",
            "24275 24300\n",
            "24300 24325\n",
            "24325 24350\n",
            "24350 24375\n",
            "24375 24400\n",
            "24400 24425\n",
            "24425 24450\n",
            "24450 24475\n",
            "24475 24500\n",
            "24500 24525\n",
            "24525 24550\n",
            "24550 24575\n",
            "24575 24600\n",
            "24600 24625\n",
            "24625 24650\n",
            "24650 24675\n",
            "24675 24700\n",
            "24700 24725\n",
            "24725 24750\n",
            "24750 24775\n",
            "24775 24800\n",
            "24800 24825\n",
            "24825 24850\n",
            "24850 24875\n",
            "24875 24900\n",
            "24900 24925\n",
            "24925 24950\n",
            "24950 24975\n",
            "24975 25000\n",
            "25000 25025\n",
            "25025 25050\n",
            "25050 25075\n",
            "25075 25100\n",
            "25100 25125\n",
            "25125 25150\n",
            "25150 25175\n",
            "25175 25200\n",
            "25200 25225\n",
            "25225 25250\n",
            "25250 25275\n",
            "25275 25300\n",
            "25300 25325\n",
            "25325 25350\n",
            "25350 25375\n",
            "25375 25400\n",
            "25400 25425\n",
            "25425 25450\n",
            "25450 25475\n",
            "25475 25500\n",
            "25500 25525\n",
            "25525 25550\n",
            "25550 25575\n",
            "25575 25600\n",
            "25600 25625\n",
            "25625 25650\n",
            "25650 25675\n",
            "25675 25700\n",
            "25700 25725\n",
            "25725 25750\n",
            "25750 25775\n",
            "25775 25800\n",
            "25800 25825\n",
            "25825 25850\n",
            "25850 25875\n",
            "25875 25900\n",
            "25900 25925\n",
            "25925 25950\n",
            "25950 25975\n",
            "25975 26000\n",
            "26000 26025\n",
            "26025 26050\n",
            "26050 26075\n",
            "26075 26100\n",
            "26100 26125\n",
            "26125 26150\n",
            "26150 26175\n",
            "26175 26200\n",
            "26200 26225\n",
            "26225 26250\n",
            "26250 26275\n",
            "26275 26300\n",
            "26300 26325\n",
            "26325 26350\n",
            "26350 26375\n",
            "26375 26400\n",
            "26400 26425\n",
            "26425 26450\n",
            "26450 26475\n",
            "26475 26500\n",
            "26500 26525\n",
            "26525 26550\n",
            "26550 26575\n",
            "26575 26600\n",
            "26600 26625\n",
            "26625 26650\n",
            "26650 26675\n",
            "26675 26700\n",
            "26700 26725\n",
            "26725 26750\n",
            "26750 26775\n",
            "26775 26800\n",
            "26800 26825\n",
            "26825 26850\n",
            "26850 26875\n",
            "26875 26900\n",
            "26900 26925\n",
            "26925 26950\n",
            "26950 26975\n",
            "26975 27000\n",
            "27000 27025\n",
            "27025 27050\n",
            "27050 27075\n",
            "27075 27100\n",
            "27100 27125\n",
            "27125 27150\n",
            "27150 27175\n",
            "27175 27200\n",
            "27200 27225\n",
            "27225 27250\n",
            "27250 27275\n",
            "27275 27300\n",
            "27300 27325\n",
            "27325 27350\n",
            "27350 27375\n",
            "27375 27400\n",
            "27400 27425\n",
            "27425 27450\n",
            "27450 27475\n",
            "27475 27500\n",
            "27500 27525\n",
            "27525 27550\n",
            "27550 27575\n",
            "27575 27600\n",
            "27600 27625\n",
            "27625 27650\n",
            "27650 27675\n",
            "27675 27700\n",
            "27700 27725\n",
            "27725 27750\n",
            "27750 27775\n",
            "27775 27800\n",
            "27800 27825\n",
            "27825 27850\n",
            "27850 27875\n",
            "27875 27900\n",
            "27900 27925\n",
            "27925 27950\n",
            "27950 27975\n",
            "27975 28000\n",
            "28000 28025\n",
            "28025 28050\n",
            "28050 28075\n",
            "28075 28100\n",
            "28100 28125\n",
            "28125 28150\n",
            "28150 28175\n",
            "28175 28200\n",
            "28200 28225\n",
            "28225 28250\n",
            "28250 28275\n",
            "28275 28300\n",
            "28300 28325\n",
            "28325 28350\n",
            "28350 28375\n",
            "28375 28400\n",
            "28400 28425\n",
            "28425 28450\n",
            "28450 28475\n",
            "28475 28500\n",
            "28500 28525\n",
            "28525 28550\n",
            "28550 28575\n",
            "28575 28600\n",
            "28600 28625\n",
            "28625 28650\n",
            "28650 28675\n",
            "28675 28700\n",
            "28700 28725\n",
            "28725 28750\n",
            "28750 28775\n",
            "28775 28800\n",
            "28800 28825\n",
            "28825 28850\n",
            "28850 28875\n",
            "28875 28900\n",
            "28900 28925\n",
            "28925 28950\n",
            "28950 28975\n",
            "28975 29000\n",
            "29000 29025\n",
            "29025 29050\n",
            "29050 29075\n",
            "29075 29100\n",
            "29100 29125\n",
            "29125 29150\n",
            "29150 29175\n",
            "29175 29200\n",
            "29200 29225\n",
            "29225 29250\n",
            "29250 29275\n",
            "29275 29300\n",
            "29300 29325\n",
            "29325 29350\n",
            "29350 29375\n",
            "29375 29400\n",
            "29400 29425\n",
            "29425 29450\n",
            "29450 29475\n",
            "29475 29500\n",
            "29500 29525\n",
            "29525 29550\n",
            "29550 29575\n",
            "29575 29600\n",
            "29600 29625\n",
            "29625 29650\n",
            "29650 29675\n",
            "29675 29700\n",
            "29700 29725\n",
            "29725 29750\n",
            "29750 29775\n",
            "29775 29800\n",
            "29800 29825\n",
            "29825 29850\n",
            "29850 29875\n",
            "29875 29900\n",
            "29900 29925\n",
            "29925 29950\n",
            "29950 29975\n",
            "29975 30000\n",
            "30000 30025\n",
            "30025 30050\n",
            "30050 30075\n",
            "30075 30100\n",
            "30100 30125\n",
            "30125 30150\n",
            "30150 30175\n",
            "30175 30200\n",
            "30200 30225\n",
            "30225 30250\n",
            "30250 30275\n",
            "30275 30300\n",
            "30300 30325\n",
            "30325 30350\n",
            "30350 30375\n",
            "30375 30400\n",
            "30400 30425\n",
            "30425 30450\n",
            "30450 30475\n",
            "30475 30500\n",
            "30500 30525\n",
            "30525 30550\n",
            "30550 30575\n",
            "30575 30600\n",
            "30600 30625\n",
            "30625 30647\n",
            "(30647, 512)\n",
            "(30647, 256)\n",
            "(30647, 768)\n"
          ]
        }
      ],
      "source": [
        "interpreter_hl1 = attr.LayerConductance(model, model.hidden_layers[0][1])\n",
        "interpreter_hl2 = attr.LayerConductance(model, model.hidden_layers[1][1])\n",
        "attribution_batch_size = 25\n",
        "attributions_hl1 = []\n",
        "attributions_hl2 = []\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "if not os.path.exists(resource_dir + \"layer_attribution_dict.pkl\"):\n",
        "    for i in range(0,test_x.shape[0], attribution_batch_size):\n",
        "        start = i\n",
        "        end = min(i+attribution_batch_size, test_x.shape[0])\n",
        "\n",
        "        batch_attributions_hl1 = interpreter_hl1.attribute(torch.Tensor(test_x[start:end]).requires_grad_().to(device),\n",
        "                                            target = 1)\n",
        "        attributions_hl1.extend(batch_attributions_hl1.detach().to(\"cpu\").numpy())\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        batch_attributions_hl2 = interpreter_hl2.attribute(torch.Tensor(test_x[start:end]).requires_grad_().to(device),\n",
        "                                            target = 1)\n",
        "        attributions_hl2.extend(batch_attributions_hl2.detach().to(\"cpu\").numpy())\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(start, end)\n",
        "#         break\n",
        "\n",
        "    attributions_hl1 = np.array(attributions_hl1)\n",
        "    print(attributions_hl1.shape)\n",
        "\n",
        "    attributions_hl2 = np.array(attributions_hl2)\n",
        "    print(attributions_hl2.shape)\n",
        "\n",
        "    layer_attributions = {\n",
        "        \"hl1\" : attributions_hl1,\n",
        "        \"hl2\" : attributions_hl2\n",
        "    }\n",
        "\n",
        "    all_attributions = np.hstack((attributions_hl1, attributions_hl2))\n",
        "    print(all_attributions.shape)\n",
        "\n",
        "    with open(resource_dir + \"layer_attribution_dict.pkl\", \"wb\") as file:\n",
        "        pkl.dump(layer_attributions, file)\n",
        "\n",
        "else:\n",
        "    with open(resource_dir + \"layer_attribution_dict.pkl\", \"rb\") as file:\n",
        "        layer_attributions = pkl.load(file)\n",
        "        attributions_hl1 = layer_attributions[\"hl1\"]\n",
        "        print(attributions_hl1.shape)\n",
        "\n",
        "        attributions_hl2 = layer_attributions[\"hl2\"]\n",
        "        print(attributions_hl2.shape)\n",
        "\n",
        "        all_attributions = np.hstack((attributions_hl1, attributions_hl2))\n",
        "        print(all_attributions.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwWVlUTOsAKe",
        "outputId": "14711996-7b84-4108-acd1-4e9ceca9ce01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n",
            "HL_1_N_0\n",
            "HL_1_N_511\n",
            "HL_2_N_0\n",
            "HL_2_N_255\n"
          ]
        }
      ],
      "source": [
        "layer_importance_dict = []\n",
        "\n",
        "for j in range(all_attributions.shape[1]):\n",
        "        layer = 1 if j<attributions_hl1.shape[1] else 2\n",
        "        neuron = j if j<attributions_hl1.shape[1] else j - attributions_hl1.shape[1]\n",
        "\n",
        "        neuron_dict = {\n",
        "            \"Layer\": layer,\n",
        "            \"Neuron\" : neuron,\n",
        "            \"Neuron_Name\": \"HL_\" + str(layer) + \"_N_\" + str(neuron),\n",
        "            \"pos_score\": 0,\n",
        "            \"pos_count\": 0,\n",
        "            \"neg_score\": 0,\n",
        "            \"neg_count\": 0,\n",
        "            \"total_score\": 0,\n",
        "            \"total_count\": 0\n",
        "        }\n",
        "\n",
        "        layer_importance_dict.append(neuron_dict)\n",
        "\n",
        "print(len(layer_importance_dict))\n",
        "print(layer_importance_dict[0][\"Neuron_Name\"])\n",
        "print(layer_importance_dict[511][\"Neuron_Name\"])\n",
        "print(layer_importance_dict[512][\"Neuron_Name\"])\n",
        "print(layer_importance_dict[-1][\"Neuron_Name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HVFg5rxsAKf"
      },
      "outputs": [],
      "source": [
        "for i in range(test_x.shape[0]):\n",
        "    ty = int(test_y[i])\n",
        "    py = int(stats.pred_y[i])\n",
        "\n",
        "    for j in range(len(layer_importance_dict)):\n",
        "        layer_importance_dict[j][\"total_score\"] += all_attributions[i][j]\n",
        "        layer_importance_dict[j][\"total_count\"] += 1\n",
        "\n",
        "        if py > 0:\n",
        "            layer_importance_dict[j][\"pos_score\"] += all_attributions[i][j]\n",
        "            layer_importance_dict[j][\"pos_count\"] += 1\n",
        "        else:\n",
        "            layer_importance_dict[j][\"neg_score\"] += all_attributions[i][j]\n",
        "            layer_importance_dict[j][\"neg_count\"] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrMcijawsAKf",
        "outputId": "757e1c2a-7cdc-40c2-f74d-a1eee0ed3909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768, 9)\n",
            "Layer                  1\n",
            "Neuron                 0\n",
            "Neuron_Name     HL_1_N_0\n",
            "pos_score      -0.037365\n",
            "pos_count            109\n",
            "neg_score     -83.104712\n",
            "neg_count          30538\n",
            "total_score   -83.142076\n",
            "total_count        30647\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_dict(layer_importance_dict)\n",
        "print(df.shape)\n",
        "print(df.iloc[0])\n",
        "df.to_excel(resource_dir + \"Layer_Importance.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-1vw59csAKf"
      },
      "source": [
        "# Neuron Imoportance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6nIQjEXsAKf",
        "outputId": "cbd4c4ab-d237-414e-89da-48ffc9cbc58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 25\n",
            "25 50\n",
            "50 75\n",
            "75 100\n",
            "100 125\n",
            "125 150\n",
            "150 175\n",
            "175 200\n",
            "200 225\n",
            "225 250\n",
            "250 275\n",
            "275 300\n",
            "300 325\n",
            "325 350\n",
            "350 375\n",
            "375 400\n",
            "400 425\n",
            "425 450\n",
            "450 475\n",
            "475 500\n",
            "500 525\n",
            "525 550\n",
            "550 575\n",
            "575 600\n",
            "600 625\n",
            "625 650\n",
            "650 675\n",
            "675 700\n",
            "700 725\n",
            "725 750\n",
            "750 775\n",
            "775 800\n",
            "800 825\n",
            "825 850\n",
            "850 875\n",
            "875 900\n",
            "900 925\n",
            "925 950\n",
            "950 975\n",
            "975 1000\n",
            "1000 1025\n",
            "1025 1050\n",
            "1050 1075\n",
            "1075 1100\n",
            "1100 1125\n",
            "1125 1150\n",
            "1150 1175\n",
            "1175 1200\n",
            "1200 1225\n",
            "1225 1250\n",
            "1250 1275\n",
            "1275 1300\n",
            "1300 1325\n",
            "1325 1350\n",
            "1350 1375\n",
            "1375 1400\n",
            "1400 1425\n",
            "1425 1450\n",
            "1450 1475\n",
            "1475 1500\n",
            "1500 1525\n",
            "1525 1550\n",
            "1550 1575\n",
            "1575 1600\n",
            "1600 1625\n",
            "1625 1650\n",
            "1650 1675\n",
            "1675 1700\n",
            "1700 1725\n",
            "1725 1750\n",
            "1750 1775\n",
            "1775 1800\n",
            "1800 1825\n",
            "1825 1850\n",
            "1850 1875\n",
            "1875 1900\n",
            "1900 1925\n",
            "1925 1950\n",
            "1950 1975\n",
            "1975 2000\n",
            "2000 2025\n",
            "2025 2050\n",
            "2050 2075\n",
            "2075 2100\n",
            "2100 2125\n",
            "2125 2150\n",
            "2150 2175\n",
            "2175 2200\n",
            "2200 2225\n",
            "2225 2250\n",
            "2250 2275\n",
            "2275 2300\n",
            "2300 2325\n",
            "2325 2350\n",
            "2350 2375\n",
            "2375 2400\n",
            "2400 2425\n",
            "2425 2450\n",
            "2450 2475\n",
            "2475 2500\n",
            "2500 2525\n",
            "2525 2550\n",
            "2550 2575\n",
            "2575 2600\n",
            "2600 2625\n",
            "2625 2650\n",
            "2650 2675\n",
            "2675 2700\n",
            "2700 2725\n",
            "2725 2750\n",
            "2750 2775\n",
            "2775 2800\n",
            "2800 2825\n",
            "2825 2850\n",
            "2850 2875\n",
            "2875 2900\n",
            "2900 2925\n",
            "2925 2950\n",
            "2950 2975\n",
            "2975 3000\n",
            "3000 3025\n",
            "3025 3050\n",
            "3050 3075\n",
            "3075 3100\n",
            "3100 3125\n",
            "3125 3150\n",
            "3150 3175\n",
            "3175 3200\n",
            "3200 3225\n",
            "3225 3250\n",
            "3250 3275\n",
            "3275 3300\n",
            "3300 3325\n",
            "3325 3350\n",
            "3350 3375\n",
            "3375 3400\n",
            "3400 3425\n",
            "3425 3450\n",
            "3450 3475\n",
            "3475 3500\n",
            "3500 3525\n",
            "3525 3550\n",
            "3550 3575\n",
            "3575 3600\n",
            "3600 3625\n",
            "3625 3650\n",
            "3650 3675\n",
            "3675 3700\n",
            "3700 3725\n",
            "3725 3750\n",
            "3750 3775\n",
            "3775 3800\n",
            "3800 3825\n",
            "3825 3850\n",
            "3850 3875\n",
            "3875 3900\n",
            "3900 3925\n",
            "3925 3950\n",
            "3950 3975\n",
            "3975 4000\n",
            "4000 4025\n",
            "4025 4050\n",
            "4050 4075\n",
            "4075 4100\n",
            "4100 4125\n",
            "4125 4150\n",
            "4150 4175\n",
            "4175 4200\n",
            "4200 4225\n",
            "4225 4250\n",
            "4250 4275\n",
            "4275 4300\n",
            "4300 4325\n",
            "4325 4350\n",
            "4350 4375\n",
            "4375 4400\n",
            "4400 4425\n",
            "4425 4450\n",
            "4450 4475\n",
            "4475 4500\n",
            "4500 4525\n",
            "4525 4550\n",
            "4550 4575\n",
            "4575 4600\n",
            "4600 4625\n",
            "4625 4650\n",
            "4650 4675\n",
            "4675 4700\n",
            "4700 4725\n",
            "4725 4750\n",
            "4750 4775\n",
            "4775 4800\n",
            "4800 4825\n",
            "4825 4850\n",
            "4850 4875\n",
            "4875 4900\n",
            "4900 4925\n",
            "4925 4950\n",
            "4950 4975\n",
            "4975 5000\n",
            "5000 5025\n",
            "5025 5050\n",
            "5050 5075\n",
            "5075 5100\n",
            "5100 5125\n",
            "5125 5150\n",
            "5150 5175\n",
            "5175 5200\n",
            "5200 5225\n",
            "5225 5250\n",
            "5250 5275\n",
            "5275 5300\n",
            "5300 5325\n",
            "5325 5350\n",
            "5350 5375\n",
            "5375 5400\n",
            "5400 5425\n",
            "5425 5450\n",
            "5450 5475\n",
            "5475 5500\n",
            "5500 5525\n",
            "5525 5550\n",
            "5550 5575\n",
            "5575 5600\n",
            "5600 5625\n",
            "5625 5650\n",
            "5650 5675\n",
            "5675 5700\n",
            "5700 5725\n",
            "5725 5750\n",
            "5750 5775\n",
            "5775 5800\n",
            "5800 5825\n",
            "5825 5850\n",
            "5850 5875\n",
            "5875 5900\n",
            "5900 5925\n",
            "5925 5950\n",
            "5950 5975\n",
            "5975 6000\n",
            "6000 6025\n",
            "6025 6050\n",
            "6050 6075\n",
            "6075 6100\n",
            "6100 6125\n",
            "6125 6150\n",
            "6150 6175\n",
            "6175 6200\n",
            "6200 6225\n",
            "6225 6250\n",
            "6250 6275\n",
            "6275 6300\n",
            "6300 6325\n",
            "6325 6350\n",
            "6350 6375\n",
            "6375 6400\n",
            "6400 6425\n",
            "6425 6450\n",
            "6450 6475\n",
            "6475 6500\n",
            "6500 6525\n",
            "6525 6550\n",
            "6550 6575\n",
            "6575 6600\n",
            "6600 6625\n",
            "6625 6650\n",
            "6650 6675\n",
            "6675 6700\n",
            "6700 6725\n",
            "6725 6750\n",
            "6750 6775\n",
            "6775 6800\n",
            "6800 6825\n",
            "6825 6850\n",
            "6850 6875\n",
            "6875 6900\n",
            "6900 6925\n",
            "6925 6950\n",
            "6950 6975\n",
            "6975 7000\n",
            "7000 7025\n",
            "7025 7050\n",
            "7050 7075\n",
            "7075 7100\n",
            "7100 7125\n",
            "7125 7150\n",
            "7150 7175\n",
            "7175 7200\n",
            "7200 7225\n",
            "7225 7250\n",
            "7250 7275\n",
            "7275 7300\n",
            "7300 7325\n",
            "7325 7350\n",
            "7350 7375\n",
            "7375 7400\n",
            "7400 7425\n",
            "7425 7450\n",
            "7450 7475\n",
            "7475 7500\n",
            "7500 7525\n",
            "7525 7550\n",
            "7550 7575\n",
            "7575 7600\n",
            "7600 7625\n",
            "7625 7650\n",
            "7650 7675\n",
            "7675 7700\n",
            "7700 7725\n",
            "7725 7750\n",
            "7750 7775\n",
            "7775 7800\n",
            "7800 7825\n",
            "7825 7850\n",
            "7850 7875\n",
            "7875 7900\n",
            "7900 7925\n",
            "7925 7950\n",
            "7950 7975\n",
            "7975 8000\n",
            "8000 8025\n",
            "8025 8050\n",
            "8050 8075\n",
            "8075 8100\n",
            "8100 8125\n",
            "8125 8150\n",
            "8150 8175\n",
            "8175 8200\n",
            "8200 8225\n",
            "8225 8250\n",
            "8250 8275\n",
            "8275 8300\n",
            "8300 8325\n",
            "8325 8350\n",
            "8350 8375\n",
            "8375 8400\n",
            "8400 8425\n",
            "8425 8450\n",
            "8450 8475\n",
            "8475 8500\n",
            "8500 8525\n",
            "8525 8550\n",
            "8550 8575\n",
            "8575 8600\n",
            "8600 8625\n",
            "8625 8650\n",
            "8650 8675\n",
            "8675 8700\n",
            "8700 8725\n",
            "8725 8750\n",
            "8750 8775\n",
            "8775 8800\n",
            "8800 8825\n",
            "8825 8850\n",
            "8850 8875\n",
            "8875 8900\n",
            "8900 8925\n",
            "8925 8950\n",
            "8950 8975\n",
            "8975 9000\n",
            "9000 9025\n",
            "9025 9050\n",
            "9050 9075\n",
            "9075 9100\n",
            "9100 9125\n",
            "9125 9150\n",
            "9150 9175\n",
            "9175 9200\n",
            "9200 9225\n",
            "9225 9250\n",
            "9250 9275\n",
            "9275 9300\n",
            "9300 9325\n",
            "9325 9350\n",
            "9350 9375\n",
            "9375 9400\n",
            "9400 9425\n",
            "9425 9450\n",
            "9450 9475\n",
            "9475 9500\n",
            "9500 9525\n",
            "9525 9550\n",
            "9550 9575\n",
            "9575 9600\n",
            "9600 9625\n",
            "9625 9650\n",
            "9650 9675\n",
            "9675 9700\n",
            "9700 9725\n",
            "9725 9750\n",
            "9750 9775\n",
            "9775 9800\n",
            "9800 9825\n",
            "9825 9850\n",
            "9850 9875\n",
            "9875 9900\n",
            "9900 9925\n",
            "9925 9950\n",
            "9950 9975\n",
            "9975 10000\n",
            "10000 10025\n",
            "10025 10050\n",
            "10050 10075\n",
            "10075 10100\n",
            "10100 10125\n",
            "10125 10150\n",
            "10150 10175\n",
            "10175 10200\n",
            "10200 10225\n",
            "10225 10250\n",
            "10250 10275\n",
            "10275 10300\n",
            "10300 10325\n",
            "10325 10350\n",
            "10350 10375\n",
            "10375 10400\n",
            "10400 10425\n",
            "10425 10450\n",
            "10450 10475\n",
            "10475 10500\n",
            "10500 10525\n",
            "10525 10550\n",
            "10550 10575\n",
            "10575 10600\n",
            "10600 10625\n",
            "10625 10650\n",
            "10650 10675\n",
            "10675 10700\n",
            "10700 10725\n",
            "10725 10750\n",
            "10750 10775\n",
            "10775 10800\n",
            "10800 10825\n",
            "10825 10850\n",
            "10850 10875\n",
            "10875 10900\n",
            "10900 10925\n",
            "10925 10950\n",
            "10950 10975\n",
            "10975 11000\n",
            "11000 11025\n",
            "11025 11050\n",
            "11050 11075\n",
            "11075 11100\n",
            "11100 11125\n",
            "11125 11150\n",
            "11150 11175\n",
            "11175 11200\n",
            "11200 11225\n",
            "11225 11250\n",
            "11250 11275\n",
            "11275 11300\n",
            "11300 11325\n",
            "11325 11350\n",
            "11350 11375\n",
            "11375 11400\n",
            "11400 11425\n",
            "11425 11450\n",
            "11450 11475\n",
            "11475 11500\n",
            "11500 11525\n",
            "11525 11550\n",
            "11550 11575\n",
            "11575 11600\n",
            "11600 11625\n",
            "11625 11650\n",
            "11650 11675\n",
            "11675 11700\n",
            "11700 11725\n",
            "11725 11750\n",
            "11750 11775\n",
            "11775 11800\n",
            "11800 11825\n",
            "11825 11850\n",
            "11850 11875\n",
            "11875 11900\n",
            "11900 11925\n",
            "11925 11950\n",
            "11950 11975\n",
            "11975 12000\n",
            "12000 12025\n",
            "12025 12050\n",
            "12050 12075\n",
            "12075 12100\n",
            "12100 12125\n",
            "12125 12150\n",
            "12150 12175\n",
            "12175 12200\n",
            "12200 12225\n",
            "12225 12250\n",
            "12250 12275\n",
            "12275 12300\n",
            "12300 12325\n",
            "12325 12350\n",
            "12350 12375\n",
            "12375 12400\n",
            "12400 12425\n",
            "12425 12450\n",
            "12450 12475\n",
            "12475 12500\n",
            "12500 12525\n",
            "12525 12550\n",
            "12550 12575\n",
            "12575 12600\n",
            "12600 12625\n",
            "12625 12650\n",
            "12650 12675\n",
            "12675 12700\n",
            "12700 12725\n",
            "12725 12750\n",
            "12750 12775\n",
            "12775 12800\n",
            "12800 12825\n",
            "12825 12850\n",
            "12850 12875\n",
            "12875 12900\n",
            "12900 12925\n",
            "12925 12950\n",
            "12950 12975\n",
            "12975 13000\n",
            "13000 13025\n",
            "13025 13050\n",
            "13050 13075\n",
            "13075 13100\n",
            "13100 13125\n",
            "13125 13150\n",
            "13150 13175\n",
            "13175 13200\n",
            "13200 13225\n",
            "13225 13250\n",
            "13250 13275\n",
            "13275 13300\n",
            "13300 13325\n",
            "13325 13350\n",
            "13350 13375\n",
            "13375 13400\n",
            "13400 13425\n",
            "13425 13450\n",
            "13450 13475\n",
            "13475 13500\n",
            "13500 13525\n",
            "13525 13550\n",
            "13550 13575\n",
            "13575 13600\n",
            "13600 13625\n",
            "13625 13650\n",
            "13650 13675\n",
            "13675 13700\n",
            "13700 13725\n",
            "13725 13750\n",
            "13750 13775\n",
            "13775 13800\n",
            "13800 13825\n",
            "13825 13850\n",
            "13850 13875\n",
            "13875 13900\n",
            "13900 13925\n",
            "13925 13950\n",
            "13950 13975\n",
            "13975 14000\n",
            "14000 14025\n",
            "14025 14050\n",
            "14050 14075\n",
            "14075 14100\n",
            "14100 14125\n",
            "14125 14150\n",
            "14150 14175\n",
            "14175 14200\n",
            "14200 14225\n",
            "14225 14250\n",
            "14250 14275\n",
            "14275 14300\n",
            "14300 14325\n",
            "14325 14350\n",
            "14350 14375\n",
            "14375 14400\n",
            "14400 14425\n",
            "14425 14450\n",
            "14450 14475\n",
            "14475 14500\n",
            "14500 14525\n",
            "14525 14550\n",
            "14550 14575\n",
            "14575 14600\n",
            "14600 14625\n",
            "14625 14650\n",
            "14650 14675\n",
            "14675 14700\n",
            "14700 14725\n",
            "14725 14750\n",
            "14750 14775\n",
            "14775 14800\n",
            "14800 14825\n",
            "14825 14850\n",
            "14850 14875\n",
            "14875 14900\n",
            "14900 14925\n",
            "14925 14950\n",
            "14950 14975\n",
            "14975 15000\n",
            "15000 15025\n",
            "15025 15050\n",
            "15050 15075\n",
            "15075 15100\n",
            "15100 15125\n",
            "15125 15150\n",
            "15150 15175\n",
            "15175 15200\n",
            "15200 15225\n",
            "15225 15250\n",
            "15250 15275\n",
            "15275 15300\n",
            "15300 15325\n",
            "15325 15350\n",
            "15350 15375\n",
            "15375 15400\n",
            "15400 15425\n",
            "15425 15450\n",
            "15450 15475\n",
            "15475 15500\n",
            "15500 15525\n",
            "15525 15550\n",
            "15550 15575\n",
            "15575 15600\n",
            "15600 15625\n",
            "15625 15650\n",
            "15650 15675\n",
            "15675 15700\n",
            "15700 15725\n",
            "15725 15750\n",
            "15750 15775\n",
            "15775 15800\n",
            "15800 15825\n",
            "15825 15850\n",
            "15850 15875\n",
            "15875 15900\n",
            "15900 15925\n",
            "15925 15950\n",
            "15950 15975\n",
            "15975 16000\n",
            "16000 16025\n",
            "16025 16050\n",
            "16050 16075\n",
            "16075 16100\n",
            "16100 16125\n",
            "16125 16150\n",
            "16150 16175\n",
            "16175 16200\n",
            "16200 16225\n",
            "16225 16250\n",
            "16250 16275\n",
            "16275 16300\n",
            "16300 16325\n",
            "16325 16350\n",
            "16350 16375\n",
            "16375 16400\n",
            "16400 16425\n",
            "16425 16450\n",
            "16450 16475\n",
            "16475 16500\n",
            "16500 16525\n",
            "16525 16550\n",
            "16550 16575\n",
            "16575 16600\n",
            "16600 16625\n",
            "16625 16650\n",
            "16650 16675\n",
            "16675 16700\n",
            "16700 16725\n",
            "16725 16750\n",
            "16750 16775\n",
            "16775 16800\n",
            "16800 16825\n",
            "16825 16850\n",
            "16850 16875\n",
            "16875 16900\n",
            "16900 16925\n",
            "16925 16950\n",
            "16950 16975\n",
            "16975 17000\n",
            "17000 17025\n",
            "17025 17050\n",
            "17050 17075\n",
            "17075 17100\n",
            "17100 17125\n",
            "17125 17150\n",
            "17150 17175\n",
            "17175 17200\n",
            "17200 17225\n",
            "17225 17250\n",
            "17250 17275\n",
            "17275 17300\n",
            "17300 17325\n",
            "17325 17350\n",
            "17350 17375\n",
            "17375 17400\n",
            "17400 17425\n",
            "17425 17450\n",
            "17450 17475\n",
            "17475 17500\n",
            "17500 17525\n",
            "17525 17550\n",
            "17550 17575\n",
            "17575 17600\n",
            "17600 17625\n",
            "17625 17650\n",
            "17650 17675\n",
            "17675 17700\n",
            "17700 17725\n",
            "17725 17750\n",
            "17750 17775\n",
            "17775 17800\n",
            "17800 17825\n",
            "17825 17850\n",
            "17850 17875\n",
            "17875 17900\n",
            "17900 17925\n",
            "17925 17950\n",
            "17950 17975\n",
            "17975 18000\n",
            "18000 18025\n",
            "18025 18050\n",
            "18050 18075\n",
            "18075 18100\n",
            "18100 18125\n",
            "18125 18150\n",
            "18150 18175\n",
            "18175 18200\n",
            "18200 18225\n",
            "18225 18250\n",
            "18250 18275\n",
            "18275 18300\n",
            "18300 18325\n",
            "18325 18350\n",
            "18350 18375\n",
            "18375 18400\n",
            "18400 18425\n",
            "18425 18450\n",
            "18450 18475\n",
            "18475 18500\n",
            "18500 18525\n",
            "18525 18550\n",
            "18550 18575\n",
            "18575 18600\n",
            "18600 18625\n",
            "18625 18650\n",
            "18650 18675\n",
            "18675 18700\n",
            "18700 18725\n",
            "18725 18750\n",
            "18750 18775\n",
            "18775 18800\n",
            "18800 18825\n",
            "18825 18850\n",
            "18850 18875\n",
            "18875 18900\n",
            "18900 18925\n",
            "18925 18950\n",
            "18950 18975\n",
            "18975 19000\n",
            "19000 19025\n",
            "19025 19050\n",
            "19050 19075\n",
            "19075 19100\n",
            "19100 19125\n",
            "19125 19150\n",
            "19150 19175\n",
            "19175 19200\n",
            "19200 19225\n",
            "19225 19250\n",
            "19250 19275\n",
            "19275 19300\n",
            "19300 19325\n",
            "19325 19350\n",
            "19350 19375\n",
            "19375 19400\n",
            "19400 19425\n",
            "19425 19450\n",
            "19450 19475\n",
            "19475 19500\n",
            "19500 19525\n",
            "19525 19550\n",
            "19550 19575\n",
            "19575 19600\n",
            "19600 19625\n",
            "19625 19650\n",
            "19650 19675\n",
            "19675 19700\n",
            "19700 19725\n",
            "19725 19750\n",
            "19750 19775\n",
            "19775 19800\n",
            "19800 19825\n",
            "19825 19850\n",
            "19850 19875\n",
            "19875 19900\n",
            "19900 19925\n",
            "19925 19950\n",
            "19950 19975\n",
            "19975 20000\n",
            "20000 20025\n",
            "20025 20050\n",
            "20050 20075\n",
            "20075 20100\n",
            "20100 20125\n",
            "20125 20150\n",
            "20150 20175\n",
            "20175 20200\n",
            "20200 20225\n",
            "20225 20250\n",
            "20250 20275\n",
            "20275 20300\n",
            "20300 20325\n",
            "20325 20350\n",
            "20350 20375\n",
            "20375 20400\n",
            "20400 20425\n",
            "20425 20450\n",
            "20450 20475\n",
            "20475 20500\n",
            "20500 20525\n",
            "20525 20550\n",
            "20550 20575\n",
            "20575 20600\n",
            "20600 20625\n",
            "20625 20650\n",
            "20650 20675\n",
            "20675 20700\n",
            "20700 20725\n",
            "20725 20750\n",
            "20750 20775\n",
            "20775 20800\n",
            "20800 20825\n",
            "20825 20850\n",
            "20850 20875\n",
            "20875 20900\n",
            "20900 20925\n",
            "20925 20950\n",
            "20950 20975\n",
            "20975 21000\n",
            "21000 21025\n",
            "21025 21050\n",
            "21050 21075\n",
            "21075 21100\n",
            "21100 21125\n",
            "21125 21150\n",
            "21150 21175\n",
            "21175 21200\n",
            "21200 21225\n",
            "21225 21250\n",
            "21250 21275\n",
            "21275 21300\n",
            "21300 21325\n",
            "21325 21350\n",
            "21350 21375\n",
            "21375 21400\n",
            "21400 21425\n",
            "21425 21450\n",
            "21450 21475\n",
            "21475 21500\n",
            "21500 21525\n",
            "21525 21550\n",
            "21550 21575\n",
            "21575 21600\n",
            "21600 21625\n",
            "21625 21650\n",
            "21650 21675\n",
            "21675 21700\n",
            "21700 21725\n",
            "21725 21750\n",
            "21750 21775\n",
            "21775 21800\n",
            "21800 21825\n",
            "21825 21850\n",
            "21850 21875\n",
            "21875 21900\n",
            "21900 21925\n",
            "21925 21950\n",
            "21950 21975\n",
            "21975 22000\n",
            "22000 22025\n",
            "22025 22050\n",
            "22050 22075\n",
            "22075 22100\n",
            "22100 22125\n",
            "22125 22150\n",
            "22150 22175\n",
            "22175 22200\n",
            "22200 22225\n",
            "22225 22250\n",
            "22250 22275\n",
            "22275 22300\n",
            "22300 22325\n",
            "22325 22350\n",
            "22350 22375\n",
            "22375 22400\n",
            "22400 22425\n",
            "22425 22450\n",
            "22450 22475\n",
            "22475 22500\n",
            "22500 22525\n",
            "22525 22550\n",
            "22550 22575\n",
            "22575 22600\n",
            "22600 22625\n",
            "22625 22650\n",
            "22650 22675\n",
            "22675 22700\n",
            "22700 22725\n",
            "22725 22750\n",
            "22750 22775\n",
            "22775 22800\n",
            "22800 22825\n",
            "22825 22850\n",
            "22850 22875\n",
            "22875 22900\n",
            "22900 22925\n",
            "22925 22950\n",
            "22950 22975\n",
            "22975 23000\n",
            "23000 23025\n",
            "23025 23050\n",
            "23050 23075\n",
            "23075 23100\n",
            "23100 23125\n",
            "23125 23150\n",
            "23150 23175\n",
            "23175 23200\n",
            "23200 23225\n",
            "23225 23250\n",
            "23250 23275\n",
            "23275 23300\n",
            "23300 23325\n",
            "23325 23350\n",
            "23350 23375\n",
            "23375 23400\n",
            "23400 23425\n",
            "23425 23450\n",
            "23450 23475\n",
            "23475 23500\n",
            "23500 23525\n",
            "23525 23550\n",
            "23550 23575\n",
            "23575 23600\n",
            "23600 23625\n",
            "23625 23650\n",
            "23650 23675\n",
            "23675 23700\n",
            "23700 23725\n",
            "23725 23750\n",
            "23750 23775\n",
            "23775 23800\n",
            "23800 23825\n",
            "23825 23850\n",
            "23850 23875\n",
            "23875 23900\n",
            "23900 23925\n",
            "23925 23950\n",
            "23950 23975\n",
            "23975 24000\n",
            "24000 24025\n",
            "24025 24050\n",
            "24050 24075\n",
            "24075 24100\n",
            "24100 24125\n",
            "24125 24150\n",
            "24150 24175\n",
            "24175 24200\n",
            "24200 24225\n",
            "24225 24250\n",
            "24250 24275\n",
            "24275 24300\n",
            "24300 24325\n",
            "24325 24350\n",
            "24350 24375\n",
            "24375 24400\n",
            "24400 24425\n",
            "24425 24450\n",
            "24450 24475\n",
            "24475 24500\n",
            "24500 24525\n",
            "24525 24550\n",
            "24550 24575\n",
            "24575 24600\n",
            "24600 24625\n",
            "24625 24650\n",
            "24650 24675\n",
            "24675 24700\n",
            "24700 24725\n",
            "24725 24750\n",
            "24750 24775\n",
            "24775 24800\n",
            "24800 24825\n",
            "24825 24850\n",
            "24850 24875\n",
            "24875 24900\n",
            "24900 24925\n",
            "24925 24950\n",
            "24950 24975\n",
            "24975 25000\n",
            "25000 25025\n",
            "25025 25050\n",
            "25050 25075\n",
            "25075 25100\n",
            "25100 25125\n",
            "25125 25150\n",
            "25150 25175\n",
            "25175 25200\n",
            "25200 25225\n",
            "25225 25250\n",
            "25250 25275\n",
            "25275 25300\n",
            "25300 25325\n",
            "25325 25350\n",
            "25350 25375\n",
            "25375 25400\n",
            "25400 25425\n",
            "25425 25450\n",
            "25450 25475\n",
            "25475 25500\n",
            "25500 25525\n",
            "25525 25550\n",
            "25550 25575\n",
            "25575 25600\n",
            "25600 25625\n",
            "25625 25650\n",
            "25650 25675\n",
            "25675 25700\n",
            "25700 25725\n",
            "25725 25750\n",
            "25750 25775\n",
            "25775 25800\n",
            "25800 25825\n",
            "25825 25850\n",
            "25850 25875\n",
            "25875 25900\n",
            "25900 25925\n",
            "25925 25950\n",
            "25950 25975\n",
            "25975 26000\n",
            "26000 26025\n",
            "26025 26050\n",
            "26050 26075\n",
            "26075 26100\n",
            "26100 26125\n",
            "26125 26150\n",
            "26150 26175\n",
            "26175 26200\n",
            "26200 26225\n",
            "26225 26250\n",
            "26250 26275\n",
            "26275 26300\n",
            "26300 26325\n",
            "26325 26350\n",
            "26350 26375\n",
            "26375 26400\n",
            "26400 26425\n",
            "26425 26450\n",
            "26450 26475\n",
            "26475 26500\n",
            "26500 26525\n",
            "26525 26550\n",
            "26550 26575\n",
            "26575 26600\n",
            "26600 26625\n",
            "26625 26650\n",
            "26650 26675\n",
            "26675 26700\n",
            "26700 26725\n",
            "26725 26750\n",
            "26750 26775\n",
            "26775 26800\n",
            "26800 26825\n",
            "26825 26850\n",
            "26850 26875\n",
            "26875 26900\n",
            "26900 26925\n",
            "26925 26950\n",
            "26950 26975\n",
            "26975 27000\n",
            "27000 27025\n",
            "27025 27050\n",
            "27050 27075\n",
            "27075 27100\n",
            "27100 27125\n",
            "27125 27150\n",
            "27150 27175\n",
            "27175 27200\n",
            "27200 27225\n",
            "27225 27250\n",
            "27250 27275\n",
            "27275 27300\n",
            "27300 27325\n",
            "27325 27350\n",
            "27350 27375\n",
            "27375 27400\n",
            "27400 27425\n",
            "27425 27450\n",
            "27450 27475\n",
            "27475 27500\n",
            "27500 27525\n",
            "27525 27550\n",
            "27550 27575\n",
            "27575 27600\n",
            "27600 27625\n",
            "27625 27650\n",
            "27650 27675\n",
            "27675 27700\n",
            "27700 27725\n",
            "27725 27750\n",
            "27750 27775\n",
            "27775 27800\n",
            "27800 27825\n",
            "27825 27850\n",
            "27850 27875\n",
            "27875 27900\n",
            "27900 27925\n",
            "27925 27950\n",
            "27950 27975\n",
            "27975 28000\n",
            "28000 28025\n",
            "28025 28050\n",
            "28050 28075\n",
            "28075 28100\n",
            "28100 28125\n",
            "28125 28150\n",
            "28150 28175\n",
            "28175 28200\n",
            "28200 28225\n",
            "28225 28250\n",
            "28250 28275\n",
            "28275 28300\n",
            "28300 28325\n",
            "28325 28350\n",
            "28350 28375\n",
            "28375 28400\n",
            "28400 28425\n",
            "28425 28450\n",
            "28450 28475\n",
            "28475 28500\n",
            "28500 28525\n",
            "28525 28550\n",
            "28550 28575\n",
            "28575 28600\n",
            "28600 28625\n",
            "28625 28650\n",
            "28650 28675\n",
            "28675 28700\n",
            "28700 28725\n",
            "28725 28750\n",
            "28750 28775\n",
            "28775 28800\n",
            "28800 28825\n",
            "28825 28850\n",
            "28850 28875\n",
            "28875 28900\n",
            "28900 28925\n",
            "28925 28950\n",
            "28950 28975\n",
            "28975 29000\n",
            "29000 29025\n",
            "29025 29050\n",
            "29050 29075\n",
            "29075 29100\n",
            "29100 29125\n",
            "29125 29150\n",
            "29150 29175\n",
            "29175 29200\n",
            "29200 29225\n",
            "29225 29250\n",
            "29250 29275\n",
            "29275 29300\n",
            "29300 29325\n",
            "29325 29350\n",
            "29350 29375\n",
            "29375 29400\n",
            "29400 29425\n",
            "29425 29450\n",
            "29450 29475\n",
            "29475 29500\n",
            "29500 29525\n",
            "29525 29550\n",
            "29550 29575\n",
            "29575 29600\n",
            "29600 29625\n",
            "29625 29650\n",
            "29650 29675\n",
            "29675 29700\n",
            "29700 29725\n",
            "29725 29750\n",
            "29750 29775\n",
            "29775 29800\n",
            "29800 29825\n",
            "29825 29850\n",
            "29850 29875\n",
            "29875 29900\n",
            "29900 29925\n",
            "29925 29950\n",
            "29950 29975\n",
            "29975 30000\n",
            "30000 30025\n",
            "30025 30050\n",
            "30050 30075\n",
            "30075 30100\n",
            "30100 30125\n",
            "30125 30150\n",
            "30150 30175\n",
            "30175 30200\n",
            "30200 30225\n",
            "30225 30250\n",
            "30250 30275\n",
            "30275 30300\n",
            "30300 30325\n",
            "30325 30350\n",
            "30350 30375\n",
            "30375 30400\n",
            "30400 30425\n",
            "30425 30450\n",
            "30450 30475\n",
            "30475 30500\n",
            "30500 30525\n",
            "30525 30550\n",
            "30550 30575\n",
            "30575 30600\n",
            "30600 30625\n",
            "30625 30647\n",
            "(30647, 23, 4)\n",
            "(30647, 23, 4)\n"
          ]
        }
      ],
      "source": [
        "interpreter_n1 = attr.NeuronConductance(model, model.hidden_layers[0][1])\n",
        "interpreter_n2 = attr.NeuronConductance(model, model.hidden_layers[0][1])\n",
        "\n",
        "attribution_batch_size = 25\n",
        "attributions_n1 = []\n",
        "attributions_n2 = []\n",
        "\n",
        "pos_neuron = 310\n",
        "neg_neuron = 215\n",
        "\n",
        "if not os.path.exists(resource_dir + \"neuron_attribution_dict.pkl\"):\n",
        "    torch.cuda.empty_cache()\n",
        "    for i in range(0,test_x.shape[0], attribution_batch_size):\n",
        "        start = i\n",
        "        end = min(i+attribution_batch_size, test_x.shape[0])\n",
        "\n",
        "        batch_attributions_n1 = interpreter_n1.attribute(torch.Tensor(test_x[start:end]).requires_grad_().to(device),\n",
        "                                                neuron_selector = pos_neuron,\n",
        "                                                target = 1)\n",
        "        attributions_n1.extend(batch_attributions_n1.detach().to(\"cpu\").numpy())\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        batch_attributions_n2 = interpreter_n2.attribute(torch.Tensor(test_x[start:end]).requires_grad_().to(device),\n",
        "                                                neuron_selector = neg_neuron,\n",
        "                                                target = 1)\n",
        "        attributions_n2.extend(batch_attributions_n2.detach().to(\"cpu\").numpy())\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(start, end)\n",
        "\n",
        "    attributions_n1 = np.array(attributions_n1)\n",
        "    print(attributions_n1.shape)\n",
        "\n",
        "    attributions_n2 = np.array(attributions_n2)\n",
        "    print(attributions_n2.shape)\n",
        "\n",
        "    neuron_attributions = {\n",
        "        \"n1\" : attributions_n1,\n",
        "        \"n2\" : attributions_n2\n",
        "    }\n",
        "    #         break\n",
        "    with open(resource_dir + \"neuron_attribution_dict.pkl\", \"wb\") as file:\n",
        "        pkl.dump(layer_attributions, file)\n",
        "\n",
        "else:\n",
        "    with open(resource_dir + \"neuron_attribution_dict.pkl\", \"rb\") as file:\n",
        "        neuron_attributions = pkl.load(file)\n",
        "        attributions_n1 = neuron_attributions[\"n1\"]\n",
        "        attributions_n1 = neuron_attributions[\"n2\"]\n",
        "        print(attributions_n1.shape)\n",
        "        print(attributions_n2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5qgAL-3sAKg",
        "outputId": "69295474-6950-4210-d39d-4ea4faffc985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30647, 23, 4)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "neuron_importantance_dict = {}\n",
        "order_dict = { 0 : 'A', 1:'T', 2:'C', 3:'G', 4:'R' }\n",
        "print(test_x.shape)\n",
        "\n",
        "for i in range(test_x.shape[0]):\n",
        "    ty = int(test_y[i])\n",
        "    py = int(stats.pred_y[i])\n",
        "\n",
        "    n1_attribution = attributions_n1[i]\n",
        "    n1_attribution = n1_attribution.sum(axis=1)\n",
        "    n1_attribution = n1_attribution / np.linalg.norm(n1_attribution)\n",
        "\n",
        "    n2_attribution = attributions_n2[i]\n",
        "    n2_attribution = n2_attribution.sum(axis=1)\n",
        "    n2_attribution = n2_attribution / np.linalg.norm(n2_attribution)\n",
        "\n",
        "\n",
        "\n",
        "    for j in range(test_x.shape[1]):\n",
        "        feature_name = 'Pos_' + str(j+1) + '_'\n",
        "\n",
        "        for k in range(test_x.shape[2]):\n",
        "            if test_x[i][j][k] > 0:\n",
        "                feature_name += order_dict[k]\n",
        "\n",
        "        if feature_name not in neuron_importantance_dict:\n",
        "            neuron_importantance_dict[feature_name] = {'n1_tot_c': 0, 'n1_tot_s': 0,\n",
        "                                                       'n2_tot_c': 0, 'n2_tot_s': 0}\n",
        "\n",
        "        # if py < 0.5:\n",
        "        neuron_importantance_dict[feature_name]['n2_tot_c'] += 1\n",
        "        neuron_importantance_dict[feature_name]['n2_tot_s'] += 0 if math.isnan(n2_attribution[j]) else n2_attribution[j]\n",
        "        # else:\n",
        "        neuron_importantance_dict[feature_name]['n1_tot_c'] += 1\n",
        "        neuron_importantance_dict[feature_name]['n1_tot_s'] += 0 if math.isnan(n1_attribution[j]) else n1_attribution[j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdcwI9AssAKg",
        "outputId": "04464857-d2bf-4234-9a0c-6e4939b2daa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218\n",
            "(218, 5)\n",
            "features      Pos_1_CG\n",
            "n1_tot_c          1972\n",
            "n1_tot_s   -153.062445\n",
            "n2_tot_c          1972\n",
            "n2_tot_s    -170.42968\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "features = list(neuron_importantance_dict.keys())\n",
        "n1_tot_c = []\n",
        "n1_tot_s = []\n",
        "\n",
        "n2_tot_c = []\n",
        "n2_tot_s = []\n",
        "\n",
        "\n",
        "print(len(features))\n",
        "\n",
        "for f in features:\n",
        "    n1_tot_c.append(neuron_importantance_dict[f]['n1_tot_c'])\n",
        "    n1_tot_s.append(neuron_importantance_dict[f]['n1_tot_s'])\n",
        "\n",
        "    n2_tot_c.append(neuron_importantance_dict[f]['n2_tot_c'])\n",
        "    n2_tot_s.append(neuron_importantance_dict[f]['n2_tot_s'])\n",
        "\n",
        "\n",
        "reformed_dict = {\n",
        "    \"features\" : features,\n",
        "    \"n1_tot_c\" : n1_tot_c,\n",
        "    \"n1_tot_s\" : n1_tot_s,\n",
        "    \"n2_tot_c\" : n2_tot_c,\n",
        "    \"n2_tot_s\" : n2_tot_s\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(reformed_dict)\n",
        "print(df.shape)\n",
        "print(df.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNeVDOSMsAKg"
      },
      "outputs": [],
      "source": [
        "df.to_excel(resource_dir + \"Neuron_Importance.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9mBeipasAKg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Nd75ulcVZ3Bg",
        "GkopnuflmA-K",
        "VUcwPlCfFaGc",
        "UsPmiHZ_81qF",
        "MUgAshgTS5BH"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}